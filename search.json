[
  {
    "objectID": "posts/word-cloud/word-cloud.html",
    "href": "posts/word-cloud/word-cloud.html",
    "title": "Creating WordClouds",
    "section": "",
    "text": "Text analysis is hard, but one simple visual tool for extracting insights from dense text are word clouds. Word clouds help identify patterns in the preponderance of certain words in a body of text or within many bodies of text. The more frequently a word appears, the larger the word appears in the word cloud.\nBut manually stripping words from paragraphs of text one by one is time consuming and mind-numbing. In the immortal words of Raymond Hettinger: “There must be a better way!”"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#generating-some-text",
    "href": "posts/word-cloud/word-cloud.html#generating-some-text",
    "title": "Creating WordClouds",
    "section": "Generating Some Text",
    "text": "Generating Some Text\nTo start, let’s create a bunch of text to generate a word cloud from. Using the Faker package, we can create a single column dataframe where each row contains a paragraph of fake/non-sensical text.\nFrom there, we’ll explore a simple way to parse out the frequency of individual words.\n\nfrom faker import Faker\nimport pandas as pd\n\nfake = Faker()\n\ndata = []\n\n# Loop over a range of 5000 records and generate a random paragraph in a key:value pair.\nfor _ in range(5000):\n    data.append(dict(text=fake.paragraph(variable_nb_sentences=True)))\n\n# Convert that list of dictionaries into a pandas dataframe\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\ntext\n\n\n\n\n0\nForget trip paper and Mr around lot.\n\n\n1\nHis include painting success amount child. Who...\n\n\n2\nEvery keep them against build ask. Practice fo...\n\n\n3\nWorker run particular fish officer son. Reason...\n\n\n4\nRest environment must word. Allow brother bene...\n\n\n...\n...\n\n\n4995\nProduct activity paper power factor. Challenge...\n\n\n4996\nChoice consider deep quality right wrong. Rece...\n\n\n4997\nKid car hotel field. So employee degree air de...\n\n\n4998\nStation authority benefit despite whether. Mov...\n\n\n4999\nDaughter ability player.\n\n\n\n\n5000 rows × 1 columns\n\n\n\nThis gives us a dataframe of 5000 paragraphs to work with. Not something I’d want to do manually!"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#splitting-paragraphs-into-words",
    "href": "posts/word-cloud/word-cloud.html#splitting-paragraphs-into-words",
    "title": "Creating WordClouds",
    "section": "Splitting Paragraphs into Words",
    "text": "Splitting Paragraphs into Words\nAdmittedly, there may be a more performant way to do this in pandas using vectorization instead of iterating row by row (sort of a no-no in dataframes), but I’m going to keep this simple and concise.\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided (using) a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing.\n\nThe general approach here is to create a list to hold our extracted words (word_list), then loop over each paragraph in the dataframe, splitting each word from the paragraph into an item in a list of its own. Then before looping to the next row, extending the original list*.\n* Python extend vs append\n\nword_list = []\nfor index, row in df.iterrows():\n    # Strip out periods and make all words lowercase\n    words = row.text.lower().replace(\".\",\"\").split(\" \")\n    word_list.extend(words)\n\n# Show the first 10 records as example\nprint(word_list[:10]) \n\n['forget', 'trip', 'paper', 'and', 'mr', 'around', 'lot', 'his', 'include', 'painting']\n\n\nThen we’ll convert that list into a dataframe and name the column word for easier reference later.\n\ndf2 = pd.DataFrame({\"word\": word_list})\ndf2\n\n\n\n\n\n\n\n\nword\n\n\n\n\n0\nforget\n\n\n1\ntrip\n\n\n2\npaper\n\n\n3\nand\n\n\n4\nmr\n\n\n...\n...\n\n\n69266\ncare\n\n\n69267\ntalk\n\n\n69268\ndaughter\n\n\n69269\nability\n\n\n69270\nplayer\n\n\n\n\n69271 rows × 1 columns"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#data-source",
    "href": "posts/word-cloud/word-cloud.html#data-source",
    "title": "Creating WordClouds",
    "section": "Data Source",
    "text": "Data Source\nAt this point we could load this dataframe into a database (perhaps using SQLSorcery or similar tool) and move on to creating a word cloud in a data viz tool like Tableau.\nBut it would be neat to also do this in python while we’re at it!"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#word-cloud",
    "href": "posts/word-cloud/word-cloud.html#word-cloud",
    "title": "Creating WordClouds",
    "section": "Word cloud",
    "text": "Word cloud\nAfter installing the wordcloud package, we can load our dataframe in and begin visualizing. We’ll want to remove some common words that won’t give us much insight (such as pronouns, conjunctions, and similar filler). For a full list of what we’ll filter out, see below.\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nprint(STOPWORDS)\n\n{'over', \"he'd\", 'we', 'some', 'k', 'further', 'yours', 'has', \"she's\", 'otherwise', \"she'll\", \"he's\", \"i'll\", 'so', 'being', 'to', 'are', \"weren't\", 'each', 'by', 'why', 'that', 'were', 'all', 'through', 'than', 'such', 'the', 'about', 'their', 'between', 'most', 'ours', 'then', 'same', 'or', 'like', 'above', \"couldn't\", \"won't\", 'would', 'who', 'they', 'was', 'where', \"hasn't\", 'since', 'other', 'them', 'when', \"wouldn't\", 'herself', 'itself', \"shouldn't\", 'else', 'myself', 'under', 'only', \"they're\", 'if', \"it's\", \"when's\", 'she', 'there', 'here', 'r', 'how', 'these', 'against', \"here's\", 'no', 'theirs', 'doing', 'ever', 'at', 'her', 'more', \"you're\", 'until', \"what's\", 'me', 'shall', \"we'd\", 'cannot', \"i'd\", 'www', 'he', 'can', 'very', 'it', \"they've\", 'once', 'been', 'and', \"isn't\", 'therefore', 'i', 'could', 'is', 'him', 'hence', \"mustn't\", \"we'll\", \"he'll\", \"i'm\", 'up', 'with', 'be', 'too', 'while', 'whom', 'your', 'again', 'com', \"let's\", 'have', 'in', 'few', 'own', \"we're\", 'below', \"don't\", \"you'll\", 'his', \"they'll\", 'however', \"shan't\", \"we've\", \"can't\", 'during', 'having', 'not', 'yourself', 'you', 'yourselves', \"how's\", 'ought', 'any', \"that's\", \"you've\", 'ourselves', 'on', 'because', 'should', 'also', 'for', \"she'd\", 'into', 'this', \"where's\", \"i've\", \"why's\", 'out', 'himself', 'am', 'do', \"aren't\", 'down', 'what', 'its', 'those', \"doesn't\", \"wasn't\", 'of', 'an', \"haven't\", \"there's\", 'get', \"didn't\", 'themselves', 'but', 'my', 'both', \"they'd\", 'did', 'our', \"hadn't\", 'from', \"who's\", 'off', 'nor', 'before', 'had', 'a', 'after', 'which', 'as', \"you'd\", 'just', 'hers', 'does', 'http'}\n\n\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n\n# Create a word cloud with resolution of 1200 x 800 with a black background\ntext = \" \".join(df2.word.values)\nwordcloud = WordCloud(\n    width=1200,\n    height=800,\n    background_color='black',\n    stopwords=STOPWORDS).generate(text)\n\nAt this point we could spit this out to an image for presentation.\nwordcloud.to_file(\"wordcloud.jpg\")\nBut we could also use a plotting library like matplotlib to build a visualization with our wordcloud.\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=(8, 6), facecolor='k', edgecolor='k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#conclusion",
    "href": "posts/word-cloud/word-cloud.html#conclusion",
    "title": "Creating WordClouds",
    "section": "Conclusion",
    "text": "Conclusion\nAnd there we have it. A simple, repeatable method for extracting word frequency from paragraph text for use in generating word clouds.\nIt really only required about 5 lines of code to extract the data:\n\nword_list = []\nfor index, row in df.iterrows():\n    words = row.text.lower().replace(\".\",\"\").split(\" \")\n    word_list.extend(words)\n\ndf = pd.DataFrame({\"word\": word_list})\n\nAnd only another 10 or so to actually create a word cloud visualization as well!"
  },
  {
    "objectID": "posts/manager-readme/index.html",
    "href": "posts/manager-readme/index.html",
    "title": "My Manager README",
    "section": "",
    "text": "When I transitioned from being primarily a software engineer who was an individual contributor on a team to managing a team of engineers, I didn’t have a clear sense of who I was going to be as a manager. I knew the sort of manager I wanted to be, but the shift from peer to leader isn’t always clear cut.\nAfter close to two years in the role, I learned a lot about who I am and who I still want to be as a leader, but had no clear way to communicate that to my team.\nOne of the engineers on our team left the organization after five years in the role, and while hiring their replacement I took the opportunity to get a fresh start and reset.\nAbout six months prior I had come across a few posts about the practice of creating a manager readme as a way to reflect on who you are as a leader as well as provide new hires a sort of guidebook or operating manual for how to work with you as their manager.\nI wanted to start my new engineer off with not only a clear understanding of how I saw my role but also what I expected out of them.\nTo do this, I looked over many examples of great manager readmes from top tech companies to draw inspiration from.\nUltimately, I wanted this document to be both explanatory and inspirational for my team as well as aspirational for myself. A sort of contract I could hold myself to. So that my team could say, “You said you were going to do X.” And I could make sure I was holding myself up to that same standard.\nSo, I drafted a welcome letter and appended my Manager README in the true python spirit of: &gt; explicit is better than implicit\nBeing a programmer, I also couldn’t help seeing this as yet another program to write. A set of instructions for myself to operate by. So, of course, I had to check it into source control and track changes. Like any other codebase, I knew it would inevitably evolve and grow. And I wanted to be able to git blame myself if it went off the rails. =)\nWithout further ado, here’s what I’ve come up with, so far:"
  },
  {
    "objectID": "posts/manager-readme/index.html#my-role",
    "href": "posts/manager-readme/index.html#my-role",
    "title": "My Manager README",
    "section": "My Role",
    "text": "My Role\n\nI strive to be a servant leader: I believe managers work for their teams, not the other way around\nI am an advocate for you and the team with the rest of the organization\nI will set the context for your work, not tell you how to do it\nI want to support your growth and provide you with opportunities to learn and apply your skills\nI am also an individual contributor on this team and hold myself accountable to the same expectations I set for the team"
  },
  {
    "objectID": "posts/manager-readme/index.html#my-availability",
    "href": "posts/manager-readme/index.html#my-availability",
    "title": "My Manager README",
    "section": "My Availability",
    "text": "My Availability\n\nWhile I usually have a lot on my plate, very little is more important to me than talking to you if you need to talk. If you want to talk, let’s talk.\nIf you have questions, have a roadblock in your work, or just need a thought partner I’d love to hear about it as soon as possible. You don’t have to wait until our scheduled one-on-one meetings. Come by my desk, stop me in the hall, catch me on slack, call me, or just put some time on my calendar anywhere that’s open.\nIf my schedule is blocked when you need me, let me know so I can make sure we find time to connect\nMy commute is hell. I tend to work from home frequently and work non-standard hours. My regular schedule is M-W-F in office from 9:30/10-6ish and T-TH at home 8ish-5ish. If you need to schedule something in person, early morning, let me know and I will take extras steps to be there on time."
  },
  {
    "objectID": "posts/manager-readme/index.html#my-assumptions-about-you",
    "href": "posts/manager-readme/index.html#my-assumptions-about-you",
    "title": "My Manager README",
    "section": "My Assumptions About You",
    "text": "My Assumptions About You\n\nYou are agile and adaptable. We wear many hats and our roles and work evolve as the organization’s needs change. I will expect you to change gears as needed and work on things you may have no prior experience in. Innovation means trying new things and not always knowing how to do it right away.\nYou are a directly responsible individual. I expect you to take ownership for your work and your mistakes. I will create a safe space for productive failure as well as give you credit for your successes.\nYou are good at your job. I wouldn’t have hired you if I thought otherwise. If it feels like I’m questioning you it’s because I am trying to gather context or trying to be a sounding board and rubber duck for your ideas. However, you know best how to do your work. I’ll ask questions and vet your ideas, but won’t override your decisions. I will expect you to own those decisions though.\nYou feel safe debating with me. I believe in productive disagreement. I find that ideas improve by being examined from all angles. If it sounds like I’m disagreeing I’m most likely just playing devil’s advocate and want you to articulate and refine your position. This does rely on us being able to have a safe debate. Sometimes the right answer is one neither of us started with."
  },
  {
    "objectID": "posts/manager-readme/index.html#values",
    "href": "posts/manager-readme/index.html#values",
    "title": "My Manager README",
    "section": "Values",
    "text": "Values\n\nI value your transparency. I want you to keep me in the loop and let me know about issues as they arise. Be transparent with me. I will do the same for you.\nI value your learning. I know that it takes time to learn new things and that novel projects will require additional time to experiment and iterate.\nI value your time. I don’t want you to waste your time on busy work. I want you to provide value and feel valued. I want you to have pride in your work and feel inspired and purposeful in the work you do here. That doesn’t mean we won’t ever have a slog or work we have to power through. However, if you feel like you are wasting your time on menial work, let’s find a way to automate it!\nI value your input. I put processes in place to make us more effective. I don’t believe in just going through the motions or process for the sake of process. If you think something is getting in the way of your work, let me know. I want to improve our processes so they work effectively for all of us."
  },
  {
    "objectID": "posts/manager-readme/index.html#focus",
    "href": "posts/manager-readme/index.html#focus",
    "title": "My Manager README",
    "section": "Focus",
    "text": "Focus\n\nThe quality of our development work increases with the amount of concentration we give it.\nAttention is a valuable resource. Be prepared to defend it. Deep work is vital to quality.\nWe work in an open space. Feel free to use headphones or move to a quiet space if you need to concentrate. I find I do some of my best coding when I’m not at the office.\nYou are not paid to maintain your inbox or attend meetings. Block off time on your calendar if you need to. It’s okay to say no to an immediate request for your attention or to ask to circle back later.\nTroubleshooting a problem with a teammate, though, is a valuable exercise. There are always things to learn from unplanned collaboration. Be open to help others when they need it and they will return the favor. Just be judicious with how much of your time you dole out for interruptions."
  },
  {
    "objectID": "posts/manager-readme/index.html#feedback",
    "href": "posts/manager-readme/index.html#feedback",
    "title": "My Manager README",
    "section": "Feedback",
    "text": "Feedback\n\nRadical Candor makes a lot of sense to me. I want to give you feedback because I care about you and want to help you grow.\nI prefer to offer and receive direct feedback in a timely manner. The tighter the feedback loop, the better.\nI expect people to call BS when they see it and not be afraid to speak up.\nFeedback is best when it is actionable and explicit about what improvement looks like."
  },
  {
    "objectID": "posts/manager-readme/index.html#worklife-balance",
    "href": "posts/manager-readme/index.html#worklife-balance",
    "title": "My Manager README",
    "section": "Work/Life Balance",
    "text": "Work/Life Balance\n\nI want you to work smarter, not harder. Don’t burn yourself out. There is always more work to do. You don’t have to solve everything today.\nI believe in working to live, not living to work. We are mission driven and everyone wants to give 110%, but we have to be mindful of the stress this imbalance can cause.\nUnless there’s an emergency, I don’t expect to communicate with you outside of business hours. If you get an email from me late at night or first thing in the morning, I don’t expect you to respond immediately. That’s me working when I need to. Pick it up when you get a chance during your preferred schedule. If I call your cell or text you directly, it’s likely urgent. Please respond accordingly.\nMost people work somewhere between 7am (earliest) to 7pm (latest). Figure out what works for you and let me know. I’m flexible about your schedule and know that things come up that may require coming in late or leaving early.\nI support working from home. It’s good for mental health, supports deep work and productivity, and is definitely a quality of life issue for me. I also recognize the value of in person meetings, face-to-face collaboration, and informal discussion. We are not full-time remote."
  },
  {
    "objectID": "posts/manager-readme/index.html#one-on-ones",
    "href": "posts/manager-readme/index.html#one-on-ones",
    "title": "My Manager README",
    "section": "One on Ones",
    "text": "One on Ones\n\nI think 1:1s are important and want us to check in regularly. I will strive to prioritize our 1:1s ahead of other commitments and be there on-time and committed to listening. I will give you my full attention.\nThese meetings are for you and I expect you to set the agenda. It’s not just a time for status updates (although I am happy to chat about your project status). If I have things I want to ask you, I will, but this is your time.\nI think hard discussions are frequently facilitated by taking a walk. That doesn’t mean if I ask you to go for a walk that something is wrong though. I like stretching my legs and walking meetings can get the mind moving in different directions. I am also open to taking a meeting over the ping-pong table or grabbing some coffee.\nFor remote 1:1s, I have a difficult time focusing and connecting unless I can see your face over video. If you’re doing the whole work in your pajamas thing, fine by me. You don’t have to dress up.\nI expect to check in at least once per sprint, usually at the mid way point. But the length, frequency, and medium are up to you. I will carve out at least 60 minutes for these but if you need less time or want to run over, let me know."
  },
  {
    "objectID": "posts/manager-readme/index.html#relationships",
    "href": "posts/manager-readme/index.html#relationships",
    "title": "My Manager README",
    "section": "Relationships",
    "text": "Relationships\n\nI think strong teams come from great working relationships. I encourage you to connect with your peers as well as other teammates across Aspire.\nA former Aspire teammate once coined the term frolleague(friend-colleague) and I find it very fitting. I consider a lot of current and former teammates to be friends. I hang out with folks at Aspire on my personal time and am usually up for happy hour. There is no expectation that anyone here is your new best friend, but I do think that the presence of someone you work with you consider a friend to be a strong indicator of healthy working relationships.\nI’m happy to make introductions for you or provide networking suggestions to help you do that.\nI think it is important to get out to school sites and see the work of the organization in action. I will make efforts to provide you opportunities to do so as soon as possible."
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "Data files available here on the TEA website.\nThe STAAR data files contain results for 60+ student demographic groups and each variable is repeated in a separate column for both demographic and test subject. As you will see below, this results in data files that are several thousand columns wide and are a different schema for every grade.\nThese files are designed for use in SAS and SPSS, and the website itself indicates that the number of variables in these files is too great to import into Microsoft Access or some versions of Microsoft Excel without significant truncation.\nHowever, my goal was to pivot this data into a standardized format and load into BigQuery for cross analysis with other state assessment data. What follows is my approach in Python, but I am sure there are other potentially simpler methods than mine, I hope this demonstration helps other approach this dataset for analysis and possibly encourages others to improve upon this method.\nThe original files are linked below for easy access.\nCampus level data for Grades 3-8\n\nGrade 3: test data | variable list\nGrade 4: test data | variable list\nGrade 5: test data | variable list\nGrade 6: test data | variable list\nGrade 7: test data | variable list\nGrade 8: test data | variable list\n\n\n\nBefore determining a solution and final schema, I wanted to understand the structure of these files. This can be accomplished by looping over the linked dat files and getting their shapes.\n\nimport pandas as pd\nfrom pathlib import Path\nimport xlrd\nimport os\n\n# Display options for pandas data frames\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\npd.set_option('max_seq_item', None)\n\n\nbase_url = \"https://tea.texas.gov/sites/default/files/\"\ndat_files = [\"cfy21e3.dat\", \"cfy21e4.dat\", \"cfy21e5.dat\", \"cfy21e6.dat\", \"cfy21e7.dat\", \"cfy21e8.dat\"]\n\nfor dat_file in dat_files:\n    url = f\"{base_url}{dat_file}\"\n    content = Path(url).stem\n    df = pd.read_csv(url)\n    print(content, df.shape)\n\ncfy21e3 (4643, 2093)\ncfy21e4 (4622, 3075)\ncfy21e5 (4415, 3197)\ncfy21e6 (2700, 2093)\ncfy21e7 (2337, 3075)\ncfy21e8 (2370, 4301)\n\n\nEach file is named with this convention (as far as I can discern): - c: Campus level data - fy21: Fiscal year 2021 - e#: English grade # (English, because grades 3-5 also include Spanish results)\nThe numbers in the accompanying parenthesis represent the count of rows and columns, respectively.\n\n\n\nWe can see from this quick file inspection that combining these datasets into a single unified structure will require more than just concatenation. The number of columns is jagged/inconsistent. Reviewing the variable files shows that each grade has differences in the variables reported. Some grades have Reading and Mathematics, while other include a Writing test as well. There are additional differences, but that is one of the most common.\nThey are so wide, because each variable such as # Tested is repeated for each subject and each student group (60+ in total). We can also see that the naming of these columns is encoded/abbreviated, such as r_all_d for # Tested -- Reading -- All Students or r_eth2_d for # Tested -- Reading -- Two or More Races Students.\n\n\n\nThere is something of a pattern to this naming convention you may have noticed: - r: reading - all: all students - d: # tested\nThis becomes more obvious with subsequent examples such as r_all_unsatgl_nm for # Did Not Meet Grade Level Performance -- Reading -- All Students.\n\n\n\nMy initial thought was to split these variable names using the the _ as a delimiter, but this was met with limited success, again due to inconsistent usage.\nNote: You will see the following warning in places WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero. This is due to an encoding issue with the TEA provided files and can be ignored as it does not impact our ability to parse the files.\n\ngrade3_vars = 'https://tea.texas.gov/sites/default/files/fy21_varlist_g03.xls'\n\ndf = pd.read_excel(grade3_vars)\ndf.head(20)\n\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n\n\n1\nyear\nCharacter\nTest Administration Year\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n\n\n3\ndistrict\nCharacter\nDistrict Number\n\n\n4\ndname\nCharacter\nDistrict Name\n\n\n5\ncname\nCharacter\nCampus Name\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n\n\n\n\n\n\n\nIt appears at first glance, that the pattern is subject, student demographic group, variable type. However, we can see this doesn’t remain consistent.\n\ndf.tail(15)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n2078\nm_spen_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Special Ed Students\n\n\n2079\nm_spev_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Special Ed Students\n\n\n2080\nm_spev_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Special Ed Students\n\n\n2081\nm_gify_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Gifted/Talented Students\n\n\n2082\nm_gify_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Gifted/Talented Students\n\n\n2083\nm_gifn_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Gifted/Talented Students\n\n\n2084\nm_gifn_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Gifted/Talented Students\n\n\n2085\nm_gifv_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Gifted/Talented Students\n\n\n2086\nm_gifv_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Gifted/Talented Students\n\n\n2087\nm_atry_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n\n\n2088\nm_atry_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n\n\n2089\nm_atrn_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n\n\n2090\nm_atrn_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n\n\n2091\nm_atrv_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n\n\n2092\nm_atrv_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n\n\n\n\n\n\n\nIn these latter cases, there is a 4th variable reporting category that comes before the subject and student group.\n\ndf[195:215]\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n195\nr_atrv_unsatgl_nm\nNumeric\n# Did Not Meet Grade Level Performance -- Reading -- No Info At-Risk Students\n\n\n196\nr_all_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- All Students\n\n\n197\nr_sexm_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Male Students\n\n\n198\nr_sexf_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Female Students\n\n\n199\nr_sexv_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- No Sex Info Students\n\n\n200\nr_ethh_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Hispanic/Latino Students\n\n\n201\nr_ethi_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- American Indian or Alaska Native Students\n\n\n202\nr_etha_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Asian Students\n\n\n203\nr_ethb_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Black or African American Students\n\n\n204\nr_ethp_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Native Hawaiian or Other Pacific Islander Students\n\n\n205\nr_ethw_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- White Students\n\n\n206\nr_eth2_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Two or More Races Students\n\n\n207\nr_ethv_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- No Ethnicity Info Students\n\n\n208\nr_ecoy_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Econ Disadv Students Codes: 1 2 9\n\n\n209\nr_econ_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Not Econ Disadv Students\n\n\n210\nr_eco1_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Free Meals Students\n\n\n211\nr_eco2_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Reduced-Price Meals Students\n\n\n212\nr_eco9_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Other Econ Disadvantaged Students\n\n\n213\nr_ecov_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- No Info Econ Students\n\n\n214\nr_ti1y_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Title-I Participant Students Codes: 6 7 9\n\n\n\n\n\n\n\nThere is another pattern with 4 parts for the proficiency levels that is subject, student group, and then a two part string for the variable type.\nThese differences would make it challenging to simply split on the _ character. Even distinguishing between the three part and four part variables isn’t sufficient, because there are two types of the four part variables.\n\n\n\nHowever, we can simply ignore the variables. We have a description column that also has it’s own delimiter: --.\n\ndescriptions = df.Description.values.tolist()\n\nfor description in descriptions[:20]:\n    parts = description.split('--')\n    print(parts)\n\nprint('\\n...\\n')\n\nfor description in descriptions[-5:]:\n    parts = description.split('--')\n    print(parts)\n\n['Campus Number']\n['Test Administration Year']\n['Education Service Center (Region) Number']\n['District Number']\n['District Name']\n['Campus Name']\n['Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)']\n['# Answer Documents Submitted ', ' Reading ', ' All Students']\n['# Absent - Not Tested ', ' Reading ', ' All Students']\n['# Other - Not Tested ', ' Reading ', ' All Students']\n['# Answer Documents Submitted ', ' Mathematics ', ' All Students']\n['# Absent - Not Tested ', ' Mathematics ', ' All Students']\n['# Other - Not Tested ', ' Mathematics ', ' All Students']\n['# Tested ', ' Reading ', ' All Students']\n['# Tested ', ' Reading ', ' Male Students']\n['# Tested ', ' Reading ', ' Female Students']\n['# Tested ', ' Reading ', ' No Sex Info Students']\n['# Tested ', ' Reading ', ' Hispanic/Latino Students']\n['# Tested ', ' Reading ', ' American Indian or Alaska Native Students']\n['# Tested ', ' Reading ', ' Asian Students']\n\n...\n\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' At-Risk Students']\n['# Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' Not At-Risk Students']\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' Not At-Risk Students']\n['# Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' No Info At-Risk Students']\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' No Info At-Risk Students']\n\n\nThis mostly works. We have three different types of descriptions/variables: - 1 part variables like ['Campus Number'] - 3 part variables like ['# Tested ', ' Reading ', ' All Students'] - 4 part variables like ['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' At-Risk Students']\nThat means we can use the list length to determine the order of the variable values: variable name, test subject, and student group. The fourth part reporting category is basically a modifier for the variable name so we can combine those.\nBy examining a single variable name, we can see that # Tested, for example, isn’t actually 122 different variables but rather a single variable with 122 permutations of test subject and student group: 61 student groups * 2 test subjects\n\ndf[df.Description.str.contains('# Tested')]\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n\n\n...\n...\n...\n...\n\n\n130\nm_gifn_d\nNumeric\n# Tested -- Mathematics -- Not Gifted/Talented Students\n\n\n131\nm_gifv_d\nNumeric\n# Tested -- Mathematics -- No Info Gifted/Talented Students\n\n\n132\nm_atry_d\nNumeric\n# Tested -- Mathematics -- At-Risk Students\n\n\n133\nm_atrn_d\nNumeric\n# Tested -- Mathematics -- Not At-Risk Students\n\n\n134\nm_atrv_d\nNumeric\n# Tested -- Mathematics -- No Info At-Risk Students\n\n\n\n\n122 rows × 3 columns\n\n\n\n\n\n\nIn addition to pulling out the test subject and student group from the variable description, it will be useful to rename the columns into a common naming convention that is more human readable than the current encodings.\nExample:\nIf we take the initial variable example r_all_d for # Tested -- Reading -- All Students and remove the subject and student group we would be left with simply d as the variable/column name. But a better description would be # Tested or even better (to eliminate special characters that would be problematic in the database) n_tested.\n\n\nTo start let’s add a new column to the dataframe that has the array of descriptors. We can also remove any leading or trailing whitespace from both the variable and description to prevent mismatch issues later.\n\ndf[\"Variable\"] = df[\"Variable\"].str.strip()\ndf[\"Description\"] = df[\"Description\"].str.strip()\ndf[\"desc_list\"] = df[\"Description\"].apply(lambda x: x.split(\"--\"))\ndf\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\n\n\n...\n...\n...\n...\n...\n\n\n2088\nm_atry_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n[% Avg Items Correct, Reporting Category 4 , Mathematics , At-Risk Students]\n\n\n2089\nm_atrn_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n[# Avg Items Correct, Reporting Category 4 , Mathematics , Not At-Risk Students]\n\n\n2090\nm_atrn_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n[% Avg Items Correct, Reporting Category 4 , Mathematics , Not At-Risk Students]\n\n\n2091\nm_atrv_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n[# Avg Items Correct, Reporting Category 4 , Mathematics , No Info At-Risk Students]\n\n\n2092\nm_atrv_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n[% Avg Items Correct, Reporting Category 4 , Mathematics , No Info At-Risk Students]\n\n\n\n\n2093 rows × 4 columns\n\n\n\n\n\n\nIn most cases the test subject is the 2nd part of the variable description with two exceptions: if the variable is one of the campus/grade variables that only has a single part or for the four part variables that include reporting category.\n\ndef get_subject(desc_list):\n    \"\"\"\n    Return the 2nd part of 3 part descriptors, the 3rd part of 4 part descriptors, or None.\n\n    Note: Else None is not necessary here, since functions return None by default in Python\n    \"\"\"\n    length = len(desc_list)\n    if length == 4:\n        return desc_list[2]\n    elif length &gt; 1:\n        return desc_list[1]\n\n\ndf[\"subject\"] = df[\"desc_list\"].apply(lambda x: get_subject(x))\ndf.head(20)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\nsubject\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\nNone\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\nNone\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\nNone\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\nNone\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\nNone\n\n\n5\ncname\nCharacter\nCampus Name\n[Campus Name]\nNone\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n[Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\nNone\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n[# Answer Documents Submitted , Reading , All Students]\nReading\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n[# Absent - Not Tested , Reading , All Students]\nReading\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n[# Other - Not Tested , Reading , All Students]\nReading\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n[# Answer Documents Submitted , Mathematics , All Students]\nMathematics\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n[# Absent - Not Tested , Mathematics , All Students]\nMathematics\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n[# Other - Not Tested , Mathematics , All Students]\nMathematics\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n[# Tested , Reading , All Students]\nReading\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n[# Tested , Reading , Male Students]\nReading\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n[# Tested , Reading , Female Students]\nReading\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n[# Tested , Reading , No Sex Info Students]\nReading\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n[# Tested , Reading , Hispanic/Latino Students]\nReading\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n[# Tested , Reading , American Indian or Alaska Native Students]\nReading\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n[# Tested , Reading , Asian Students]\nReading\n\n\n\n\n\n\n\n\n\n\n\nIn all cases (except the single descriptor variables) the student group is included at the end of the description.\n\ndef get_student_group(desc_list):\n    \"\"\"\n    Unless the description has only one part, return the final part.\n    \"\"\"\n    if len(desc_list) &gt; 1:\n        return desc_list[-1]\n\n\ndf[\"student_group\"] = df[\"desc_list\"].apply(lambda x: get_student_group(x))\ndf.head(20)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\nsubject\nstudent_group\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\nNone\nNone\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\nNone\nNone\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\nNone\nNone\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\nNone\nNone\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\nNone\nNone\n\n\n5\ncname\nCharacter\nCampus Name\n[Campus Name]\nNone\nNone\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n[Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\nNone\nNone\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n[# Answer Documents Submitted , Reading , All Students]\nReading\nAll Students\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n[# Absent - Not Tested , Reading , All Students]\nReading\nAll Students\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n[# Other - Not Tested , Reading , All Students]\nReading\nAll Students\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n[# Answer Documents Submitted , Mathematics , All Students]\nMathematics\nAll Students\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n[# Absent - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n[# Other - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n[# Tested , Reading , All Students]\nReading\nAll Students\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n[# Tested , Reading , Male Students]\nReading\nMale Students\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n[# Tested , Reading , Female Students]\nReading\nFemale Students\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n[# Tested , Reading , No Sex Info Students]\nReading\nNo Sex Info Students\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n[# Tested , Reading , Hispanic/Latino Students]\nReading\nHispanic/Latino Students\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n[# Tested , Reading , American Indian or Alaska Native Students]\nReading\nAmerican Indian or Alaska Native Students\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n[# Tested , Reading , Asian Students]\nReading\nAsian Students\n\n\n\n\n\n\n\n\n\n\nThere are a few problematic characters like # and % in the names of the variables that should be replaced as well as replacing spaces and abbreviating some longer words in the names. This also uses some regex parsing to remove any text contained in parenthesis.\n\nimport re\n\ndef rename_variable(desc_list):\n    \"\"\"\n    Replace special characters and append reporting category when present.\n    \"\"\"\n    variable_name = desc_list[0].lower().strip()\n    if len(desc_list) == 4:\n        variable_name += desc_list[1].lower().strip()\n    replacements = {\n        '-': '',\n        '#': 'n',\n        '%': 'pct',\n        'average': 'avg',\n        'reporting category': 'rpt cat',\n        ' ': '_',\n        '__': '_',\n    }\n    for old, new in replacements.items():\n        variable_name = re.sub(r\"\\([^()]*\\)\", \"\", variable_name).strip()\n        variable_name = variable_name.replace(old, new)\n    return variable_name\n\n\ndf[\"name\"] = df[\"desc_list\"].apply(lambda x: rename_variable(x))\ndf.head(20)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\nsubject\nstudent_group\nname\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\nNone\nNone\ncampus_number\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\nNone\nNone\ntest_administration_year\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\nNone\nNone\neducation_service_center_number\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\nNone\nNone\ndistrict_number\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\nNone\nNone\ndistrict_name\n\n\n5\ncname\nCharacter\nCampus Name\n[Campus Name]\nNone\nNone\ncampus_name\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n[Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\nNone\nNone\ntested_grade\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n[# Answer Documents Submitted , Reading , All Students]\nReading\nAll Students\nn_answer_documents_submitted\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n[# Absent - Not Tested , Reading , All Students]\nReading\nAll Students\nn_absent_not_tested\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n[# Other - Not Tested , Reading , All Students]\nReading\nAll Students\nn_other_not_tested\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n[# Answer Documents Submitted , Mathematics , All Students]\nMathematics\nAll Students\nn_answer_documents_submitted\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n[# Absent - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\nn_absent_not_tested\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n[# Other - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\nn_other_not_tested\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n[# Tested , Reading , All Students]\nReading\nAll Students\nn_tested\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n[# Tested , Reading , Male Students]\nReading\nMale Students\nn_tested\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n[# Tested , Reading , Female Students]\nReading\nFemale Students\nn_tested\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n[# Tested , Reading , No Sex Info Students]\nReading\nNo Sex Info Students\nn_tested\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n[# Tested , Reading , Hispanic/Latino Students]\nReading\nHispanic/Latino Students\nn_tested\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n[# Tested , Reading , American Indian or Alaska Native Students]\nReading\nAmerican Indian or Alaska Native Students\nn_tested\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n[# Tested , Reading , Asian Students]\nReading\nAsian Students\nn_tested\n\n\n\n\n\n\n\n\n\n\nBecause we’re going to iterate over each variable list (there are six files: one for each grade between 3 and 8), we can simplify this by running all these change operations at once. We also want to remove unneeded columns and rename the original columns to standardize the naming conventions and make for an easy join condition later.\n\ndef add_descriptors(df):\n    df[\"Variable\"] = df[\"Variable\"].str.strip()\n    df[\"Description\"] = df[\"Description\"].str.strip()\n    df[\"desc_list\"] = df[\"Description\"].apply(lambda x: x.split('--'))\n    df[\"name\"] = df[\"desc_list\"].apply(lambda x: rename_variable(x))\n    df[\"subject\"] = df[\"desc_list\"].apply(lambda x: get_subject(x))\n    df[\"student_group\"] = df[\"desc_list\"].apply(lambda x: get_student_group(x))\n    df.drop(columns=[\"Description\", \"desc_list\"], inplace=True)\n    df.rename(columns={'Variable': 'variable', 'Format_Type': 'format_type'}, inplace=True)\n    return df\n\n\n\n\nThat means it would be possible to pivot this data long for each distinct variable after extracting the subject and student group from the variable description. This information, though, is split across two different files: the test data in the .dat files and the variable descriptions in the .xls files.\nMy plan then is to: 1. Match the corresponding dat file to its variable list 2. Pivot the test data long so that each variable and value is a single row 3. Extract the test subject and student group data points from the variable description 4. Join the two datasets together using the variable name as a unique identifier 5. Once more pivot the combined data such that each distinct variable becomes its own column 6. Combine those standardized data structures for each grade into a single dataset 7. Write that new dataset out to a compressed csv file for importing into BigQuery\n\nbase_url = \"https://tea.texas.gov/sites/default/files/\"\ndat_files = [\"cfy21e3.dat\", \"cfy21e4.dat\", \"cfy21e5.dat\", \"cfy21e6.dat\", \"cfy21e7.dat\", \"cfy21e8.dat\"]\nvar_files = [\"fy21_varlist_g03.xls\", \"fy21_varlist_g04.xls\", \"fy21_varlist_g05.xls\", \"fy21_varlist_g06.xls\", \"fy21_varlist_g07.xls\", \"fy21_varlist_g08.xls\"]\n\n# 1. Match the dat file with its corresponding var file\nall_files = zip(dat_files, var_files)\n\n# Create a list to concatenate the resulting grade level data frames later\nall_grades = []\n\nfor dat_file, var_file in all_files:\n    test_data = pd.read_csv(f\"{base_url}{dat_file}\", dtype=\"string\")\n    variables = pd.read_excel(f\"{base_url}{var_file}\", dtype=\"string\")\n\n    print(\"Parsing:\", dat_file, \"rows/cols:\", test_data.shape)\n\n\n    # 2. melt the dataframe (pivot wide to long) but keep all the site and grade identifiers out of the pivot\n    identifiers = variables[variables.Format_Type == 'Character'].Variable.tolist()\n    id_vars = [identifier.upper() for identifier in identifiers]\n    test_data = pd.melt(test_data, id_vars=id_vars)\n    print(\"Pivoted:\", dat_file, \"rows/cols:\", test_data.shape)\n\n    # 3. extract the subject and student groups\n    variables = add_descriptors(variables)\n\n    # 4. join the two dataset together using the variable name as the unique identifier\n    combined = test_data.merge(variables, how='left', on='variable')\n    print(\"Joined:\", dat_file, \"+\", var_file, \"rows/cols:\", combined.shape)\n\n    # 5. pivot back wide but with only the distinct variables\n    cols = combined.columns.tolist()\n    combined[cols] = combined[cols].apply(lambda x: x.str.strip())\n    combined.columns = combined.columns.str.lower()\n    columns = ['campus','year','region','district','dname','cname','grade','format_type','subject','student_group','name']\n    combined = combined.set_index(columns)['value'].unstack().reset_index()\n    print(\"Combined:\", dat_file, \"+\", var_file, \"rows/cols:\", combined.shape)\n\n    # 6. append data for each grade into a single data structure and concatenate\n    all_grades.append(combined)\n\ndf = pd.concat(all_grades)\nprint(\"Final rows/cols:\", df.shape)\n\n# 7. write to compressed csv\ndf.to_csv(\"TX_STAAR_3_8_2021.csv.gz\", index=False, compression=\"gzip\")\n\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e3.dat rows/cols: (4643, 2093)\nPivoted: cfy21e3.dat rows/cols: (9685298, 9)\nJoined: cfy21e3.dat + fy21_varlist_g03.xls rows/cols: (9685298, 13)\nCombined: cfy21e3.dat + fy21_varlist_g03.xls rows/cols: (571089, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e4.dat rows/cols: (4622, 3075)\nPivoted: cfy21e4.dat rows/cols: (14180296, 9)\nJoined: cfy21e4.dat + fy21_varlist_g04.xls rows/cols: (14180296, 13)\nCombined: cfy21e4.dat + fy21_varlist_g04.xls rows/cols: (850448, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e5.dat rows/cols: (4415, 3197)\nPivoted: cfy21e5.dat rows/cols: (14083850, 9)\nJoined: cfy21e5.dat + fy21_varlist_g05.xls rows/cols: (14083850, 13)\nCombined: cfy21e5.dat + fy21_varlist_g05.xls rows/cols: (812360, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e6.dat rows/cols: (2700, 2093)\nPivoted: cfy21e6.dat rows/cols: (5632200, 9)\nJoined: cfy21e6.dat + fy21_varlist_g06.xls rows/cols: (5632200, 13)\nCombined: cfy21e6.dat + fy21_varlist_g06.xls rows/cols: (332100, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e7.dat rows/cols: (2337, 3075)\nPivoted: cfy21e7.dat rows/cols: (7169916, 9)\nJoined: cfy21e7.dat + fy21_varlist_g07.xls rows/cols: (7169916, 13)\nCombined: cfy21e7.dat + fy21_varlist_g07.xls rows/cols: (430008, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e8.dat rows/cols: (2370, 4301)\nPivoted: cfy21e8.dat rows/cols: (10176780, 9)\nJoined: cfy21e8.dat + fy21_varlist_g08.xls rows/cols: (10176780, 13)\nCombined: cfy21e8.dat + fy21_varlist_g08.xls rows/cols: (580650, 35)\nFinal rows/cols: (3576655, 35)\n\n\n\ndf\n\n\n\n\n\n\n\nname\ncampus\nyear\nregion\ndistrict\ndname\ncname\ngrade\nformat_type\nsubject\nstudent_group\nNaN\navg_scale_score\nn_absent_not_tested\nn_answer_documents_submitted\nn_approaches_grade_level_performance\nn_avg_items_correctrpt_cat_1\nn_avg_items_correctrpt_cat_2\nn_avg_items_correctrpt_cat_3\nn_avg_items_correctrpt_cat_4\nn_did_not_meet_grade_level_performance\nn_masters_grade_level_performance\nn_meets_grade_level_performance\nn_other_not_tested\nn_tested\npct_absent_not_tested\npct_answer_documents_submitted\npct_approaches_grade_level_performance\npct_avg_items_correctrpt_cat_1\npct_avg_items_correctrpt_cat_2\npct_avg_items_correctrpt_cat_3\npct_avg_items_correctrpt_cat_4\npct_did_not_meet_grade_level_performance\npct_masters_grade_level_performance\npct_meets_grade_level_performance\npct_other_not_tested\n\n\n\n\n0\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\n&lt;NA&gt;\nNaN\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAll Students\nNaN\n1607\n0\n26\n22\n6.3\n10.7\n5\n3.3\n4\n16\n19\n0\n26\n0\n100\n85\n79\n83\n71\n84\n15\n62\n73\n0\n\n\n2\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAmerican Indian or Alaska Native Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n3\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAsian Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n4\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAt-Risk Students\nNaN\n1465\nNaN\nNaN\n5\n5.1\n9.3\n3.3\n2.7\n2\n2\n3\nNaN\n7\nNaN\nNaN\n71\n64\n71\n47\n68\n29\n29\n43\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n580645\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTitle-I Participant Students Codes: 6 7 9\nNaN\n3425\nNaN\nNaN\n12\n6.9\n5.4\n4.2\n3.2\n23\n0\n4\nNaN\n35\nNaN\nNaN\n34\n41\n54\n42\n46\n66\n0\n11\nNaN\n\n\n580646\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTransitional Bilingual/Early Exit Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n580647\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTransitional Bilingual/Late Exit Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n580648\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTwo or More Races Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n580649\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nWhite Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n1\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n\n\n3576655 rows × 35 columns\n\n\n\n\n\n\nThe resulting dataset/file ends up being ~3.5 million rows in length and a consistent 35 columns wide. You can download the reshaped dataset here: TX STAAR 2021 - Grades 3-8\nWe can now run simpler queries on the reshaped data such as:\nSELECT *\nFROM TX_STAAR_2021\nWHERE subject = 'Reading'\nAND student_group = 'All Students'\nAND grade IN ('6', '7', '8')\nA similar query using the original datasets would’ve required unioning three different datasets and a much more complicated set of column matching to filter down to the right subject and student group.\nIn our production environment, the approach is similar but we initially load the raw dat and var files into Google Cloud Storage and as well as the resulting csv. That is then converted to avro for better query performance and an external table is generated in BigQuery using their API."
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#getting-the-lay-of-the-land",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#getting-the-lay-of-the-land",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "Before determining a solution and final schema, I wanted to understand the structure of these files. This can be accomplished by looping over the linked dat files and getting their shapes.\n\nimport pandas as pd\nfrom pathlib import Path\nimport xlrd\nimport os\n\n# Display options for pandas data frames\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\npd.set_option('max_seq_item', None)\n\n\nbase_url = \"https://tea.texas.gov/sites/default/files/\"\ndat_files = [\"cfy21e3.dat\", \"cfy21e4.dat\", \"cfy21e5.dat\", \"cfy21e6.dat\", \"cfy21e7.dat\", \"cfy21e8.dat\"]\n\nfor dat_file in dat_files:\n    url = f\"{base_url}{dat_file}\"\n    content = Path(url).stem\n    df = pd.read_csv(url)\n    print(content, df.shape)\n\ncfy21e3 (4643, 2093)\ncfy21e4 (4622, 3075)\ncfy21e5 (4415, 3197)\ncfy21e6 (2700, 2093)\ncfy21e7 (2337, 3075)\ncfy21e8 (2370, 4301)\n\n\nEach file is named with this convention (as far as I can discern): - c: Campus level data - fy21: Fiscal year 2021 - e#: English grade # (English, because grades 3-5 also include Spanish results)\nThe numbers in the accompanying parenthesis represent the count of rows and columns, respectively."
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#inconsistent-data-shape",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#inconsistent-data-shape",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "We can see from this quick file inspection that combining these datasets into a single unified structure will require more than just concatenation. The number of columns is jagged/inconsistent. Reviewing the variable files shows that each grade has differences in the variables reported. Some grades have Reading and Mathematics, while other include a Writing test as well. There are additional differences, but that is one of the most common.\nThey are so wide, because each variable such as # Tested is repeated for each subject and each student group (60+ in total). We can also see that the naming of these columns is encoded/abbreviated, such as r_all_d for # Tested -- Reading -- All Students or r_eth2_d for # Tested -- Reading -- Two or More Races Students."
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#finding-a-pattern",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#finding-a-pattern",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "There is something of a pattern to this naming convention you may have noticed: - r: reading - all: all students - d: # tested\nThis becomes more obvious with subsequent examples such as r_all_unsatgl_nm for # Did Not Meet Grade Level Performance -- Reading -- All Students."
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#a-first-attempt",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#a-first-attempt",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "My initial thought was to split these variable names using the the _ as a delimiter, but this was met with limited success, again due to inconsistent usage.\nNote: You will see the following warning in places WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero. This is due to an encoding issue with the TEA provided files and can be ignored as it does not impact our ability to parse the files.\n\ngrade3_vars = 'https://tea.texas.gov/sites/default/files/fy21_varlist_g03.xls'\n\ndf = pd.read_excel(grade3_vars)\ndf.head(20)\n\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n\n\n1\nyear\nCharacter\nTest Administration Year\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n\n\n3\ndistrict\nCharacter\nDistrict Number\n\n\n4\ndname\nCharacter\nDistrict Name\n\n\n5\ncname\nCharacter\nCampus Name\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n\n\n\n\n\n\n\nIt appears at first glance, that the pattern is subject, student demographic group, variable type. However, we can see this doesn’t remain consistent.\n\ndf.tail(15)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n2078\nm_spen_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Special Ed Students\n\n\n2079\nm_spev_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Special Ed Students\n\n\n2080\nm_spev_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Special Ed Students\n\n\n2081\nm_gify_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Gifted/Talented Students\n\n\n2082\nm_gify_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Gifted/Talented Students\n\n\n2083\nm_gifn_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Gifted/Talented Students\n\n\n2084\nm_gifn_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Gifted/Talented Students\n\n\n2085\nm_gifv_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Gifted/Talented Students\n\n\n2086\nm_gifv_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Gifted/Talented Students\n\n\n2087\nm_atry_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n\n\n2088\nm_atry_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n\n\n2089\nm_atrn_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n\n\n2090\nm_atrn_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n\n\n2091\nm_atrv_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n\n\n2092\nm_atrv_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n\n\n\n\n\n\n\nIn these latter cases, there is a 4th variable reporting category that comes before the subject and student group.\n\ndf[195:215]\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n195\nr_atrv_unsatgl_nm\nNumeric\n# Did Not Meet Grade Level Performance -- Reading -- No Info At-Risk Students\n\n\n196\nr_all_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- All Students\n\n\n197\nr_sexm_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Male Students\n\n\n198\nr_sexf_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Female Students\n\n\n199\nr_sexv_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- No Sex Info Students\n\n\n200\nr_ethh_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Hispanic/Latino Students\n\n\n201\nr_ethi_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- American Indian or Alaska Native Students\n\n\n202\nr_etha_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Asian Students\n\n\n203\nr_ethb_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Black or African American Students\n\n\n204\nr_ethp_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Native Hawaiian or Other Pacific Islander Students\n\n\n205\nr_ethw_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- White Students\n\n\n206\nr_eth2_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Two or More Races Students\n\n\n207\nr_ethv_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- No Ethnicity Info Students\n\n\n208\nr_ecoy_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Econ Disadv Students Codes: 1 2 9\n\n\n209\nr_econ_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Not Econ Disadv Students\n\n\n210\nr_eco1_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Free Meals Students\n\n\n211\nr_eco2_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Reduced-Price Meals Students\n\n\n212\nr_eco9_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Other Econ Disadvantaged Students\n\n\n213\nr_ecov_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- No Info Econ Students\n\n\n214\nr_ti1y_approgl_nm\nNumeric\n# Approaches Grade Level Performance -- Reading -- Title-I Participant Students Codes: 6 7 9\n\n\n\n\n\n\n\nThere is another pattern with 4 parts for the proficiency levels that is subject, student group, and then a two part string for the variable type.\nThese differences would make it challenging to simply split on the _ character. Even distinguishing between the three part and four part variables isn’t sufficient, because there are two types of the four part variables."
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#a-different-solution",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#a-different-solution",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "However, we can simply ignore the variables. We have a description column that also has it’s own delimiter: --.\n\ndescriptions = df.Description.values.tolist()\n\nfor description in descriptions[:20]:\n    parts = description.split('--')\n    print(parts)\n\nprint('\\n...\\n')\n\nfor description in descriptions[-5:]:\n    parts = description.split('--')\n    print(parts)\n\n['Campus Number']\n['Test Administration Year']\n['Education Service Center (Region) Number']\n['District Number']\n['District Name']\n['Campus Name']\n['Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)']\n['# Answer Documents Submitted ', ' Reading ', ' All Students']\n['# Absent - Not Tested ', ' Reading ', ' All Students']\n['# Other - Not Tested ', ' Reading ', ' All Students']\n['# Answer Documents Submitted ', ' Mathematics ', ' All Students']\n['# Absent - Not Tested ', ' Mathematics ', ' All Students']\n['# Other - Not Tested ', ' Mathematics ', ' All Students']\n['# Tested ', ' Reading ', ' All Students']\n['# Tested ', ' Reading ', ' Male Students']\n['# Tested ', ' Reading ', ' Female Students']\n['# Tested ', ' Reading ', ' No Sex Info Students']\n['# Tested ', ' Reading ', ' Hispanic/Latino Students']\n['# Tested ', ' Reading ', ' American Indian or Alaska Native Students']\n['# Tested ', ' Reading ', ' Asian Students']\n\n...\n\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' At-Risk Students']\n['# Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' Not At-Risk Students']\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' Not At-Risk Students']\n['# Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' No Info At-Risk Students']\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' No Info At-Risk Students']\n\n\nThis mostly works. We have three different types of descriptions/variables: - 1 part variables like ['Campus Number'] - 3 part variables like ['# Tested ', ' Reading ', ' All Students'] - 4 part variables like ['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' At-Risk Students']\nThat means we can use the list length to determine the order of the variable values: variable name, test subject, and student group. The fourth part reporting category is basically a modifier for the variable name so we can combine those.\nBy examining a single variable name, we can see that # Tested, for example, isn’t actually 122 different variables but rather a single variable with 122 permutations of test subject and student group: 61 student groups * 2 test subjects\n\ndf[df.Description.str.contains('# Tested')]\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\n\n\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n\n\n...\n...\n...\n...\n\n\n130\nm_gifn_d\nNumeric\n# Tested -- Mathematics -- Not Gifted/Talented Students\n\n\n131\nm_gifv_d\nNumeric\n# Tested -- Mathematics -- No Info Gifted/Talented Students\n\n\n132\nm_atry_d\nNumeric\n# Tested -- Mathematics -- At-Risk Students\n\n\n133\nm_atrn_d\nNumeric\n# Tested -- Mathematics -- Not At-Risk Students\n\n\n134\nm_atrv_d\nNumeric\n# Tested -- Mathematics -- No Info At-Risk Students\n\n\n\n\n122 rows × 3 columns"
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#extracting-the-variable-descriptors",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#extracting-the-variable-descriptors",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "In addition to pulling out the test subject and student group from the variable description, it will be useful to rename the columns into a common naming convention that is more human readable than the current encodings.\nExample:\nIf we take the initial variable example r_all_d for # Tested -- Reading -- All Students and remove the subject and student group we would be left with simply d as the variable/column name. But a better description would be # Tested or even better (to eliminate special characters that would be problematic in the database) n_tested.\n\n\nTo start let’s add a new column to the dataframe that has the array of descriptors. We can also remove any leading or trailing whitespace from both the variable and description to prevent mismatch issues later.\n\ndf[\"Variable\"] = df[\"Variable\"].str.strip()\ndf[\"Description\"] = df[\"Description\"].str.strip()\ndf[\"desc_list\"] = df[\"Description\"].apply(lambda x: x.split(\"--\"))\ndf\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\n\n\n...\n...\n...\n...\n...\n\n\n2088\nm_atry_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n[% Avg Items Correct, Reporting Category 4 , Mathematics , At-Risk Students]\n\n\n2089\nm_atrn_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n[# Avg Items Correct, Reporting Category 4 , Mathematics , Not At-Risk Students]\n\n\n2090\nm_atrn_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n[% Avg Items Correct, Reporting Category 4 , Mathematics , Not At-Risk Students]\n\n\n2091\nm_atrv_avg_cat4\nNumeric\n# Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n[# Avg Items Correct, Reporting Category 4 , Mathematics , No Info At-Risk Students]\n\n\n2092\nm_atrv_pct_cat4\nNumeric\n% Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n[% Avg Items Correct, Reporting Category 4 , Mathematics , No Info At-Risk Students]\n\n\n\n\n2093 rows × 4 columns\n\n\n\n\n\n\nIn most cases the test subject is the 2nd part of the variable description with two exceptions: if the variable is one of the campus/grade variables that only has a single part or for the four part variables that include reporting category.\n\ndef get_subject(desc_list):\n    \"\"\"\n    Return the 2nd part of 3 part descriptors, the 3rd part of 4 part descriptors, or None.\n\n    Note: Else None is not necessary here, since functions return None by default in Python\n    \"\"\"\n    length = len(desc_list)\n    if length == 4:\n        return desc_list[2]\n    elif length &gt; 1:\n        return desc_list[1]\n\n\ndf[\"subject\"] = df[\"desc_list\"].apply(lambda x: get_subject(x))\ndf.head(20)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\nsubject\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\nNone\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\nNone\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\nNone\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\nNone\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\nNone\n\n\n5\ncname\nCharacter\nCampus Name\n[Campus Name]\nNone\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n[Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\nNone\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n[# Answer Documents Submitted , Reading , All Students]\nReading\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n[# Absent - Not Tested , Reading , All Students]\nReading\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n[# Other - Not Tested , Reading , All Students]\nReading\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n[# Answer Documents Submitted , Mathematics , All Students]\nMathematics\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n[# Absent - Not Tested , Mathematics , All Students]\nMathematics\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n[# Other - Not Tested , Mathematics , All Students]\nMathematics\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n[# Tested , Reading , All Students]\nReading\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n[# Tested , Reading , Male Students]\nReading\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n[# Tested , Reading , Female Students]\nReading\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n[# Tested , Reading , No Sex Info Students]\nReading\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n[# Tested , Reading , Hispanic/Latino Students]\nReading\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n[# Tested , Reading , American Indian or Alaska Native Students]\nReading\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n[# Tested , Reading , Asian Students]\nReading"
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#extracting-student-group",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#extracting-student-group",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "In all cases (except the single descriptor variables) the student group is included at the end of the description.\n\ndef get_student_group(desc_list):\n    \"\"\"\n    Unless the description has only one part, return the final part.\n    \"\"\"\n    if len(desc_list) &gt; 1:\n        return desc_list[-1]\n\n\ndf[\"student_group\"] = df[\"desc_list\"].apply(lambda x: get_student_group(x))\ndf.head(20)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\nsubject\nstudent_group\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\nNone\nNone\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\nNone\nNone\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\nNone\nNone\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\nNone\nNone\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\nNone\nNone\n\n\n5\ncname\nCharacter\nCampus Name\n[Campus Name]\nNone\nNone\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n[Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\nNone\nNone\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n[# Answer Documents Submitted , Reading , All Students]\nReading\nAll Students\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n[# Absent - Not Tested , Reading , All Students]\nReading\nAll Students\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n[# Other - Not Tested , Reading , All Students]\nReading\nAll Students\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n[# Answer Documents Submitted , Mathematics , All Students]\nMathematics\nAll Students\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n[# Absent - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n[# Other - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n[# Tested , Reading , All Students]\nReading\nAll Students\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n[# Tested , Reading , Male Students]\nReading\nMale Students\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n[# Tested , Reading , Female Students]\nReading\nFemale Students\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n[# Tested , Reading , No Sex Info Students]\nReading\nNo Sex Info Students\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n[# Tested , Reading , Hispanic/Latino Students]\nReading\nHispanic/Latino Students\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n[# Tested , Reading , American Indian or Alaska Native Students]\nReading\nAmerican Indian or Alaska Native Students\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n[# Tested , Reading , Asian Students]\nReading\nAsian Students"
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#creating-a-new-variable-name",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#creating-a-new-variable-name",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "There are a few problematic characters like # and % in the names of the variables that should be replaced as well as replacing spaces and abbreviating some longer words in the names. This also uses some regex parsing to remove any text contained in parenthesis.\n\nimport re\n\ndef rename_variable(desc_list):\n    \"\"\"\n    Replace special characters and append reporting category when present.\n    \"\"\"\n    variable_name = desc_list[0].lower().strip()\n    if len(desc_list) == 4:\n        variable_name += desc_list[1].lower().strip()\n    replacements = {\n        '-': '',\n        '#': 'n',\n        '%': 'pct',\n        'average': 'avg',\n        'reporting category': 'rpt cat',\n        ' ': '_',\n        '__': '_',\n    }\n    for old, new in replacements.items():\n        variable_name = re.sub(r\"\\([^()]*\\)\", \"\", variable_name).strip()\n        variable_name = variable_name.replace(old, new)\n    return variable_name\n\n\ndf[\"name\"] = df[\"desc_list\"].apply(lambda x: rename_variable(x))\ndf.head(20)\n\n\n\n\n\n\n\n\nVariable\nFormat_Type\nDescription\ndesc_list\nsubject\nstudent_group\nname\n\n\n\n\n0\ncampus\nCharacter\nCampus Number\n[Campus Number]\nNone\nNone\ncampus_number\n\n\n1\nyear\nCharacter\nTest Administration Year\n[Test Administration Year]\nNone\nNone\ntest_administration_year\n\n\n2\nregion\nCharacter\nEducation Service Center (Region) Number\n[Education Service Center (Region) Number]\nNone\nNone\neducation_service_center_number\n\n\n3\ndistrict\nCharacter\nDistrict Number\n[District Number]\nNone\nNone\ndistrict_number\n\n\n4\ndname\nCharacter\nDistrict Name\n[District Name]\nNone\nNone\ndistrict_name\n\n\n5\ncname\nCharacter\nCampus Name\n[Campus Name]\nNone\nNone\ncampus_name\n\n\n6\ngrade\nCharacter\nTested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n[Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\nNone\nNone\ntested_grade\n\n\n7\nr_all_docs_n\nNumeric\n# Answer Documents Submitted -- Reading -- All Students\n[# Answer Documents Submitted , Reading , All Students]\nReading\nAll Students\nn_answer_documents_submitted\n\n\n8\nr_all_abs_n\nNumeric\n# Absent - Not Tested -- Reading -- All Students\n[# Absent - Not Tested , Reading , All Students]\nReading\nAll Students\nn_absent_not_tested\n\n\n9\nr_all_oth_n\nNumeric\n# Other - Not Tested -- Reading -- All Students\n[# Other - Not Tested , Reading , All Students]\nReading\nAll Students\nn_other_not_tested\n\n\n10\nm_all_docs_n\nNumeric\n# Answer Documents Submitted -- Mathematics -- All Students\n[# Answer Documents Submitted , Mathematics , All Students]\nMathematics\nAll Students\nn_answer_documents_submitted\n\n\n11\nm_all_abs_n\nNumeric\n# Absent - Not Tested -- Mathematics -- All Students\n[# Absent - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\nn_absent_not_tested\n\n\n12\nm_all_oth_n\nNumeric\n# Other - Not Tested -- Mathematics -- All Students\n[# Other - Not Tested , Mathematics , All Students]\nMathematics\nAll Students\nn_other_not_tested\n\n\n13\nr_all_d\nNumeric\n# Tested -- Reading -- All Students\n[# Tested , Reading , All Students]\nReading\nAll Students\nn_tested\n\n\n14\nr_sexm_d\nNumeric\n# Tested -- Reading -- Male Students\n[# Tested , Reading , Male Students]\nReading\nMale Students\nn_tested\n\n\n15\nr_sexf_d\nNumeric\n# Tested -- Reading -- Female Students\n[# Tested , Reading , Female Students]\nReading\nFemale Students\nn_tested\n\n\n16\nr_sexv_d\nNumeric\n# Tested -- Reading -- No Sex Info Students\n[# Tested , Reading , No Sex Info Students]\nReading\nNo Sex Info Students\nn_tested\n\n\n17\nr_ethh_d\nNumeric\n# Tested -- Reading -- Hispanic/Latino Students\n[# Tested , Reading , Hispanic/Latino Students]\nReading\nHispanic/Latino Students\nn_tested\n\n\n18\nr_ethi_d\nNumeric\n# Tested -- Reading -- American Indian or Alaska Native Students\n[# Tested , Reading , American Indian or Alaska Native Students]\nReading\nAmerican Indian or Alaska Native Students\nn_tested\n\n\n19\nr_etha_d\nNumeric\n# Tested -- Reading -- Asian Students\n[# Tested , Reading , Asian Students]\nReading\nAsian Students\nn_tested"
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#bringing-it-all-together",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#bringing-it-all-together",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "Because we’re going to iterate over each variable list (there are six files: one for each grade between 3 and 8), we can simplify this by running all these change operations at once. We also want to remove unneeded columns and rename the original columns to standardize the naming conventions and make for an easy join condition later.\n\ndef add_descriptors(df):\n    df[\"Variable\"] = df[\"Variable\"].str.strip()\n    df[\"Description\"] = df[\"Description\"].str.strip()\n    df[\"desc_list\"] = df[\"Description\"].apply(lambda x: x.split('--'))\n    df[\"name\"] = df[\"desc_list\"].apply(lambda x: rename_variable(x))\n    df[\"subject\"] = df[\"desc_list\"].apply(lambda x: get_subject(x))\n    df[\"student_group\"] = df[\"desc_list\"].apply(lambda x: get_student_group(x))\n    df.drop(columns=[\"Description\", \"desc_list\"], inplace=True)\n    df.rename(columns={'Variable': 'variable', 'Format_Type': 'format_type'}, inplace=True)\n    return df"
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#a-path-forward",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#a-path-forward",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "That means it would be possible to pivot this data long for each distinct variable after extracting the subject and student group from the variable description. This information, though, is split across two different files: the test data in the .dat files and the variable descriptions in the .xls files.\nMy plan then is to: 1. Match the corresponding dat file to its variable list 2. Pivot the test data long so that each variable and value is a single row 3. Extract the test subject and student group data points from the variable description 4. Join the two datasets together using the variable name as a unique identifier 5. Once more pivot the combined data such that each distinct variable becomes its own column 6. Combine those standardized data structures for each grade into a single dataset 7. Write that new dataset out to a compressed csv file for importing into BigQuery\n\nbase_url = \"https://tea.texas.gov/sites/default/files/\"\ndat_files = [\"cfy21e3.dat\", \"cfy21e4.dat\", \"cfy21e5.dat\", \"cfy21e6.dat\", \"cfy21e7.dat\", \"cfy21e8.dat\"]\nvar_files = [\"fy21_varlist_g03.xls\", \"fy21_varlist_g04.xls\", \"fy21_varlist_g05.xls\", \"fy21_varlist_g06.xls\", \"fy21_varlist_g07.xls\", \"fy21_varlist_g08.xls\"]\n\n# 1. Match the dat file with its corresponding var file\nall_files = zip(dat_files, var_files)\n\n# Create a list to concatenate the resulting grade level data frames later\nall_grades = []\n\nfor dat_file, var_file in all_files:\n    test_data = pd.read_csv(f\"{base_url}{dat_file}\", dtype=\"string\")\n    variables = pd.read_excel(f\"{base_url}{var_file}\", dtype=\"string\")\n\n    print(\"Parsing:\", dat_file, \"rows/cols:\", test_data.shape)\n\n\n    # 2. melt the dataframe (pivot wide to long) but keep all the site and grade identifiers out of the pivot\n    identifiers = variables[variables.Format_Type == 'Character'].Variable.tolist()\n    id_vars = [identifier.upper() for identifier in identifiers]\n    test_data = pd.melt(test_data, id_vars=id_vars)\n    print(\"Pivoted:\", dat_file, \"rows/cols:\", test_data.shape)\n\n    # 3. extract the subject and student groups\n    variables = add_descriptors(variables)\n\n    # 4. join the two dataset together using the variable name as the unique identifier\n    combined = test_data.merge(variables, how='left', on='variable')\n    print(\"Joined:\", dat_file, \"+\", var_file, \"rows/cols:\", combined.shape)\n\n    # 5. pivot back wide but with only the distinct variables\n    cols = combined.columns.tolist()\n    combined[cols] = combined[cols].apply(lambda x: x.str.strip())\n    combined.columns = combined.columns.str.lower()\n    columns = ['campus','year','region','district','dname','cname','grade','format_type','subject','student_group','name']\n    combined = combined.set_index(columns)['value'].unstack().reset_index()\n    print(\"Combined:\", dat_file, \"+\", var_file, \"rows/cols:\", combined.shape)\n\n    # 6. append data for each grade into a single data structure and concatenate\n    all_grades.append(combined)\n\ndf = pd.concat(all_grades)\nprint(\"Final rows/cols:\", df.shape)\n\n# 7. write to compressed csv\ndf.to_csv(\"TX_STAAR_3_8_2021.csv.gz\", index=False, compression=\"gzip\")\n\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e3.dat rows/cols: (4643, 2093)\nPivoted: cfy21e3.dat rows/cols: (9685298, 9)\nJoined: cfy21e3.dat + fy21_varlist_g03.xls rows/cols: (9685298, 13)\nCombined: cfy21e3.dat + fy21_varlist_g03.xls rows/cols: (571089, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e4.dat rows/cols: (4622, 3075)\nPivoted: cfy21e4.dat rows/cols: (14180296, 9)\nJoined: cfy21e4.dat + fy21_varlist_g04.xls rows/cols: (14180296, 13)\nCombined: cfy21e4.dat + fy21_varlist_g04.xls rows/cols: (850448, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e5.dat rows/cols: (4415, 3197)\nPivoted: cfy21e5.dat rows/cols: (14083850, 9)\nJoined: cfy21e5.dat + fy21_varlist_g05.xls rows/cols: (14083850, 13)\nCombined: cfy21e5.dat + fy21_varlist_g05.xls rows/cols: (812360, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e6.dat rows/cols: (2700, 2093)\nPivoted: cfy21e6.dat rows/cols: (5632200, 9)\nJoined: cfy21e6.dat + fy21_varlist_g06.xls rows/cols: (5632200, 13)\nCombined: cfy21e6.dat + fy21_varlist_g06.xls rows/cols: (332100, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e7.dat rows/cols: (2337, 3075)\nPivoted: cfy21e7.dat rows/cols: (7169916, 9)\nJoined: cfy21e7.dat + fy21_varlist_g07.xls rows/cols: (7169916, 13)\nCombined: cfy21e7.dat + fy21_varlist_g07.xls rows/cols: (430008, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e8.dat rows/cols: (2370, 4301)\nPivoted: cfy21e8.dat rows/cols: (10176780, 9)\nJoined: cfy21e8.dat + fy21_varlist_g08.xls rows/cols: (10176780, 13)\nCombined: cfy21e8.dat + fy21_varlist_g08.xls rows/cols: (580650, 35)\nFinal rows/cols: (3576655, 35)\n\n\n\ndf\n\n\n\n\n\n\n\nname\ncampus\nyear\nregion\ndistrict\ndname\ncname\ngrade\nformat_type\nsubject\nstudent_group\nNaN\navg_scale_score\nn_absent_not_tested\nn_answer_documents_submitted\nn_approaches_grade_level_performance\nn_avg_items_correctrpt_cat_1\nn_avg_items_correctrpt_cat_2\nn_avg_items_correctrpt_cat_3\nn_avg_items_correctrpt_cat_4\nn_did_not_meet_grade_level_performance\nn_masters_grade_level_performance\nn_meets_grade_level_performance\nn_other_not_tested\nn_tested\npct_absent_not_tested\npct_answer_documents_submitted\npct_approaches_grade_level_performance\npct_avg_items_correctrpt_cat_1\npct_avg_items_correctrpt_cat_2\npct_avg_items_correctrpt_cat_3\npct_avg_items_correctrpt_cat_4\npct_did_not_meet_grade_level_performance\npct_masters_grade_level_performance\npct_meets_grade_level_performance\npct_other_not_tested\n\n\n\n\n0\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\n&lt;NA&gt;\nNaN\nNaN\n\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAll Students\nNaN\n1607\n0\n26\n22\n6.3\n10.7\n5\n3.3\n4\n16\n19\n0\n26\n0\n100\n85\n79\n83\n71\n84\n15\n62\n73\n0\n\n\n2\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAmerican Indian or Alaska Native Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n3\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAsian Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n4\n001902103\n21\n07\n001902\nCAYUGA ISD\nCAYUGA ELEM.\n03\nNumeric\nMathematics\nAt-Risk Students\nNaN\n1465\nNaN\nNaN\n5\n5.1\n9.3\n3.3\n2.7\n2\n2\n3\nNaN\n7\nNaN\nNaN\n71\n64\n71\n47\n68\n29\n29\n43\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n580645\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTitle-I Participant Students Codes: 6 7 9\nNaN\n3425\nNaN\nNaN\n12\n6.9\n5.4\n4.2\n3.2\n23\n0\n4\nNaN\n35\nNaN\nNaN\n34\n41\n54\n42\n46\n66\n0\n11\nNaN\n\n\n580646\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTransitional Bilingual/Early Exit Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n580647\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTransitional Bilingual/Late Exit Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n580648\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nTwo or More Races Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n0\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n580649\n254902001\n21\n20\n254902\nLA PRYOR ISD\nLA PRYOR H.S.\n08\nNumeric\nSocial Studies\nWhite Students\nNaN\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n1\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\n\n\n\n\n3576655 rows × 35 columns"
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#conclusion",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html#conclusion",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "The resulting dataset/file ends up being ~3.5 million rows in length and a consistent 35 columns wide. You can download the reshaped dataset here: TX STAAR 2021 - Grades 3-8\nWe can now run simpler queries on the reshaped data such as:\nSELECT *\nFROM TX_STAAR_2021\nWHERE subject = 'Reading'\nAND student_group = 'All Students'\nAND grade IN ('6', '7', '8')\nA similar query using the original datasets would’ve required unioning three different datasets and a much more complicated set of column matching to filter down to the right subject and student group.\nIn our production environment, the approach is similar but we initially load the raw dat and var files into Google Cloud Storage and as well as the resulting csv. That is then converted to avro for better query performance and an external table is generated in BigQuery using their API."
  },
  {
    "objectID": "posts/simple-api/index.html",
    "href": "posts/simple-api/index.html",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "",
    "text": "Suppose you have a database with tables and views that you want to expose as JSON through a REST API but you don’t want to couple the database tables with object models in your API but rather have it simply expose the data for querying (read-only).\nHow could this be done easily and with minimal code?"
  },
  {
    "objectID": "posts/simple-api/index.html#the-concept",
    "href": "posts/simple-api/index.html#the-concept",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "",
    "text": "Suppose you have a database with tables and views that you want to expose as JSON through a REST API but you don’t want to couple the database tables with object models in your API but rather have it simply expose the data for querying (read-only).\nHow could this be done easily and with minimal code?"
  },
  {
    "objectID": "posts/simple-api/index.html#the-approach",
    "href": "posts/simple-api/index.html#the-approach",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "The Approach",
    "text": "The Approach\nFlask has to be the simplest web app microframework I’ve ever dealt with (though Node’s Express and Ruby’s Sinatra certainly come close). It has a great function jsonify that makes it dead simple to serve a JSON response to a client request.\n# app.py\nfrom flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/api/\")\ndef index():\n    data = {\n        \"name\": \"dchess\",\n        \"text\": \"Hello, World\",\n    }\n    return jsonify(data)\nSpin it up on localhost:5000/api/ and voila, data!"
  },
  {
    "objectID": "posts/simple-api/index.html#here-comes-the-sorcery",
    "href": "posts/simple-api/index.html#here-comes-the-sorcery",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Here comes the Sorcery",
    "text": "Here comes the Sorcery\nThat’s simple enough but how do we substitute the hard coded dictionary for a database query? This is where SQLAlchemy and Pandas come in handy.\nI almost always want to use these two packages together and so a while back I created a pypi package to provide a simple facade with some syntactic sugar to make that even simpler called SQLSorcery. It’s built on top of both of them and has a simple way to optionally install database adapter packages like pyodbc.\nThat allows me to easily pass in my database connection credentials with environment variables and make sql queries using the Pandas .read_sql_query() method."
  },
  {
    "objectID": "posts/simple-api/index.html#environment",
    "href": "posts/simple-api/index.html#environment",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Environment",
    "text": "Environment\nI’m a fan of Pipenv. It’s so simple to install python package dependencies into a local virtual environment and also handle environment variables from a .env file. Effectively combining all the functionality of pip, venv, and python-dotenv.\n$ pipenv install Flask, sqlsorcery[mssql]\nAnd just like that we’re ready to develop.\nLet’s add some environment variables to a .env file to start:\nDB_SERVER=\nDB=\nDB_SCHEMA=\nDB_USER=\nDB_PWD=\n\nFLASK_APP=app.py"
  },
  {
    "objectID": "posts/simple-api/index.html#querying-our-data",
    "href": "posts/simple-api/index.html#querying-our-data",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Querying our Data",
    "text": "Querying our Data\nThen we can return data from a table as simple as:\nfrom sqlsorcery import MSSQL\nimport pandas as pd\n\ndb = MSSQL()\n\ndata = pd.read_sql_table(\"your_table_name\", con=db.engine, schema=db.schema)\nIt’s that simple!\nTo quickly convert that dataframe into a list of dictionaries (to return as JSON), we can use the Pandas to_dict() method.\ndata = data.to_dict(orient=\"records\")\nWhat if we want to list all the tables in our database schema? Simple!\ntables = db.engine.table_names(schema=db.schema)\nWith those two approaches and Flask’s jsonify we have everything we need to make a quick, easy, and minimal API on top of any tables in our database schema."
  },
  {
    "objectID": "posts/simple-api/index.html#a-user-interface",
    "href": "posts/simple-api/index.html#a-user-interface",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "A User Interface",
    "text": "A User Interface\nWhile we can create an API for machines to read from, it’d be nice to have at least an index of tables that a human can read, navigate, and learn what data exists before pointing tools like curl, postman, or Requests at it.\nNo worries! A tiny jinja HTML template should suffice for a quick list of table names linked to their api endpoint will be just minimal enough to work!\n# templates/index.html\n&lt;ul&gt;\n    {% for table in tables %}\n    &lt;li&gt;&lt;a href=\"/api/{{ table}}\"&gt;{{ table }}&lt;/a&gt;&lt;/li&gt;\n    {% endfor %}\n&lt;/ul&gt;"
  },
  {
    "objectID": "posts/simple-api/index.html#putting-it-all-together",
    "href": "posts/simple-api/index.html#putting-it-all-together",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Putting it all together",
    "text": "Putting it all together\nAt this point we should have a file directory that looks like this:\n.\n├── Pipfile\n├── .env\n├── app.py\n└── templates\n    └── index.html\nLet’s finish off our app.py and give it a test run:\n# app.py\n\nfrom flask import Flask, jsonify, render_template\nfrom sqlsorcery import MSSQL\nimport pandas as pd\n\napp = Flask(__name__)\ndb = MSSQL()\n\n\n@app.route(\"/api\")\ndef index():\n    tables = db.engine.table_names(schema=db.schema)\n    return render_template(\"index.html\", tables=tables)\n\n\n@app.route(\"/api/&lt;table&gt;\", methods=[\"GET\"])\ndef endpoint(table):\n    data = pd.read_sql_table(table, con=db.engine, schema=db.schema)\n    data = data.to_dict(orient=\"records\")\n    return jsonify(data)\nSpin it up on localhost:5000/api/ and explore your data!\n$ pipenv run flask run"
  },
  {
    "objectID": "posts/simple-api/index.html#conclusion",
    "href": "posts/simple-api/index.html#conclusion",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Conclusion",
    "text": "Conclusion\nI wouldn’t take this and deploy it to production anywhere without some serious security decisions, but it makes for a nice proof-of-concept. And certainly could be expanded to include user authentication, a proper production server like gunicorn and a nicer user interface. But at that point you might be better off with Flask-API or Django REST Framework, both of which I’ve used with success.\nBut still, not bad for less than 20 lines of code.\nAll the code for this blog can be found on my github. Feel free to fork and use it however you like.\nSQLSorcery is also MIT licensed and free to use. It’s in active development and contributors are welcome."
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html",
    "title": "SQL Basics using DuckDB",
    "section": "",
    "text": "I’ve been wanting to put together a simple resource for learning the basics of querying data with SQL for some time now. However, one of the roadblocks has been the overhead of initial setup instructions for creating a database to query from. Even cloud options have hurdles that prevent just jumping in and writing SQL."
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#duckdb-simple-and-effective",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#duckdb-simple-and-effective",
    "title": "SQL Basics using DuckDB",
    "section": "DuckDB: Simple and Effective",
    "text": "DuckDB: Simple and Effective\nOne of my favorite new tools is DuckDB. Especially when combined with pandas dataframes. It enables me to use pandas to quickly read a data source into a dataframe and then shift away from the often confusing array of pandas querying syntax and shift back to something I am most comfortable with in my data analysis toolbelt: SQL.\nTo start, we’ll need some data to work with. Similar to the last tutorial, let’s download the 2021-2022 NCES national school directory to work with.\n\nimport pandas as pd\n\n\ndirectory = pd.read_csv(\"ccd_sch_029_2122_w_1a_071722.csv\", dtype=str)\n\n\n# convert column names to lowercase, \n# because I'm not a fan of my column names screaming at me in ALL CAPS\ndirectory.columns = [c.lower() for c in directory.columns] \n\ndirectory.head()\n\n\n\n\n\n\n\n\nschool_year\nfipst\nstatename\nst\nsch_name\nlea_name\nstate_agency_no\nunion\nst_leaid\nleaid\n...\ng_10_offered\ng_11_offered\ng_12_offered\ng_13_offered\ng_ug_offered\ng_ae_offered\ngslo\ngshi\nlevel\nigoffered\n\n\n\n\n0\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\nPK\nKG\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns"
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#a-quick-pause-for-vocabulary",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#a-quick-pause-for-vocabulary",
    "title": "SQL Basics using DuckDB",
    "section": "A quick pause for vocabulary",
    "text": "A quick pause for vocabulary\nBefore we dive in to writing SQL queries, let’s pause to define a few common terms that will come up.\n\nTable: A table is a fundamental component of a database. It represents a collection of related data organized in rows and columns. Each row in a table is called a record, and each column represents a field.\nField: A field is a single piece of data within a table. It corresponds to the columns in a table and defines the type of data it can hold, such as text, numbers, dates, or binary data.\nRecord: A record, also known as a row, is a complete set of related data in a table. It contains values for each field, representing a single entity or data entry within the database.\nSchema: A schema is a blueprint or structure that defines the organization of a database. It outlines the tables, fields, data types, relationships, and constraints that form the database’s structure.\nQuery: A query is a request made to the database to retrieve or manipulate data. It uses a structured query language (SQL) to interact with the database.\n\n\nUnderstanding table schema\nUnderstanding the schema of a table is essential before writing a SQL query. The schema defines the structure of the table, including the names of columns, data types, and constraints. When exploring unfamiliar datasets, understanding the schema provides insights into the available data and helps you identify relevant tables and columns for analysis. It also ensures that you refer to the correct column names and use the appropriate data types in your SQL query, leading to accurate and valid results. Knowing the data types of each column helps you format your query correctly.\nFor example, when dealing with dates or numerical values, understanding the data types ensures you use the right functions and operators for calculations. When writing queries, you often need to filter or sort data based on certain criteria. Understanding the schema allows you to apply the correct conditions and sorting instructions to retrieve the desired results effectively.\nNow, let’s orient ourselves to the data in this dataset and it’s schema. To do so, let’s get a list of columns and get a sense of the data volume.\n\ndirectory.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 102130 entries, 0 to 102129\nData columns (total 65 columns):\n #   Column               Non-Null Count   Dtype \n---  ------               --------------   ----- \n 0   school_year          102130 non-null  object\n 1   fipst                102130 non-null  object\n 2   statename            102130 non-null  object\n 3   st                   102130 non-null  object\n 4   sch_name             102130 non-null  object\n 5   lea_name             102130 non-null  object\n 6   state_agency_no      102130 non-null  object\n 7   union                2440 non-null    object\n 8   st_leaid             102130 non-null  object\n 9   leaid                102130 non-null  object\n 10  st_schid             102130 non-null  object\n 11  ncessch              102130 non-null  object\n 12  schid                102130 non-null  object\n 13  mstreet1             102130 non-null  object\n 14  mstreet2             1672 non-null    object\n 15  mstreet3             2 non-null       object\n 16  mcity                102130 non-null  object\n 17  mstate               102130 non-null  object\n 18  mzip                 102130 non-null  object\n 19  mzip4                60951 non-null   object\n 20  lstreet1             102127 non-null  object\n 21  lstreet2             579 non-null     object\n 22  lstreet3             4 non-null       object\n 23  lcity                102130 non-null  object\n 24  lstate               102130 non-null  object\n 25  lzip                 102130 non-null  object\n 26  lzip4                59351 non-null   object\n 27  phone                102130 non-null  object\n 28  website              66812 non-null   object\n 29  sy_status            102130 non-null  object\n 30  sy_status_text       102130 non-null  object\n 31  updated_status       102130 non-null  object\n 32  updated_status_text  102130 non-null  object\n 33  effective_date       102130 non-null  object\n 34  sch_type_text        102130 non-null  object\n 35  sch_type             102130 non-null  object\n 36  recon_status         102130 non-null  object\n 37  out_of_state_flag    102130 non-null  object\n 38  charter_text         102130 non-null  object\n 39  chartauth1           6549 non-null    object\n 40  chartauthn1          6553 non-null    object\n 41  chartauth2           97 non-null      object\n 42  chartauthn2          97 non-null      object\n 43  nogrades             102130 non-null  object\n 44  g_pk_offered         102130 non-null  object\n 45  g_kg_offered         102130 non-null  object\n 46  g_1_offered          102130 non-null  object\n 47  g_2_offered          102130 non-null  object\n 48  g_3_offered          102130 non-null  object\n 49  g_4_offered          102130 non-null  object\n 50  g_5_offered          102130 non-null  object\n 51  g_6_offered          102130 non-null  object\n 52  g_7_offered          102130 non-null  object\n 53  g_8_offered          102130 non-null  object\n 54  g_9_offered          102130 non-null  object\n 55  g_10_offered         102130 non-null  object\n 56  g_11_offered         102130 non-null  object\n 57  g_12_offered         102130 non-null  object\n 58  g_13_offered         102130 non-null  object\n 59  g_ug_offered         102130 non-null  object\n 60  g_ae_offered         102130 non-null  object\n 61  gslo                 102130 non-null  object\n 62  gshi                 102130 non-null  object\n 63  level                102130 non-null  object\n 64  igoffered            102130 non-null  object\ndtypes: object(65)\nmemory usage: 50.6+ MB\n\n\nThis shows us a number of things:\n\nall the column names\ntheir data types (recall we have set all of them to strings, which pandas lists as object)\nthe number of non-null values in each column\nthe number of columns: 65\nthe number of rows: 102,130\nit even shows the amount of memory used to hold this data: 50.6 MB\n\n\n\nA note about null\nIn SQL and databases, null represents the absence of a value or unknown data for a specific field in a table. It’s not the same as zero or an empty string and requires special handling in SQL queries. The null value can be used in any data type, whether it’s a string, numeric, date, or any other data type.\nDealing with null values in SQL queries can be tricky, and can lead to unexpected or incorrect results."
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#getting-started",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#getting-started",
    "title": "SQL Basics using DuckDB",
    "section": "Getting Started",
    "text": "Getting Started\nNow that we have a basic lay-of-the-land for our data, we can begin querying it. At this point, I will shift away from pandas syntax and use SQL queries.\nTo start, let’s install duckdb.\npip install duckdb\n\nimport duckdb\n\nquery = \"select * from directory limit 5\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nschool_year\nfipst\nstatename\nst\nsch_name\nlea_name\nstate_agency_no\nunion\nst_leaid\nleaid\n...\ng_10_offered\ng_11_offered\ng_12_offered\ng_13_offered\ng_ug_offered\ng_ae_offered\ngslo\ngshi\nlevel\nigoffered\n\n\n\n\n0\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\nPK\nKG\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns\n\n\n\nThis basic query is equivalent to directory.head().\nselect * \nfrom directory\nlimit 5\nWhat is each keyword doing here?\nselect: The select keyword is used to specify which columns or expressions to include in the query result. It allows you to choose the data you want to retrieve from the tables in the database. In SQL, the asterisk (*) is used in a select statement to represent all columns in a table.\nfrom: The from keyword is used to specify the table from which the data will be retrieved. It tells the query where to find the data to be selected. With duckdb, we can specify a dataframe as the table to be queried.\nlimit: The limit keyword is used to restrict the number of rows returned in the query result. It is particularly useful when you only need to see a small subset of the data."
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#asking-questions",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#asking-questions",
    "title": "SQL Basics using DuckDB",
    "section": "Asking Questions",
    "text": "Asking Questions\nSQL Queries are fundamentally a way of interrogating your data. You usually start with a question you want to answer and then form your query (question) is such a way that the database can understand it and return the relevant data.\nOne of the basic ways to interrogate the data is to constrain it in some way. Filtering the data to show only a subset allows you to begin to identify possible patterns. The where clause is particularly valuable for this purpose.\nLet’s translate this question into a SQL query: Which schools in Delaware offer 12th grade enrollment?\n\nquery = \"\"\"\n  select sch_name\n  from directory\n  where statename = 'DELAWARE'\n    and g_12_offered = 'Yes'\n  order by sch_name\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nsch_name\n\n\n\n\n0\nAppoquinimink High School\n\n\n1\nBrandywine High School\n\n\n2\nBrandywine SITE\n\n\n3\nBrennen School (The)\n\n\n4\nCaesar Rodney High School\n\n\n5\nCalloway (Cab) School of the Arts\n\n\n6\nCape Henlopen High School\n\n\n7\nCharter School of Wilmington\n\n\n8\nChristiana High School\n\n\n9\nConcord High School\n\n\n10\nConrad Schools of Science\n\n\n11\nDelaware Military Academy\n\n\n12\nDelaware School for the Deaf\n\n\n13\nDelcastle Technical High School\n\n\n14\nDelmar High School\n\n\n15\nDickinson (John) School\n\n\n16\nDouglass School\n\n\n17\nDover High School\n\n\n18\nEarly College High School at Del State\n\n\n19\nEnnis (Howard T.) School\n\n\n20\nFirst State Military Academy\n\n\n21\nFreire Charter School\n\n\n22\nGlasgow High School\n\n\n23\nGreat Oaks Charter School\n\n\n24\nHodgson (Paul M.) Vocational Technical High School\n\n\n25\nHoward High School of Technology\n\n\n26\nIndian River High School\n\n\n27\nIndian River Intensive Learning Center\n\n\n28\nJohn S. Charlton School\n\n\n29\nKent County Community School\n\n\n30\nKent County Secondary ILC\n\n\n31\nLake Forest High School\n\n\n32\nLaurel Senior High School\n\n\n33\nLeach (John G.) School\n\n\n34\nMOT Charter School\n\n\n35\nMcKean (Thomas) High School\n\n\n36\nMeadowood Program\n\n\n37\nMiddletown High School\n\n\n38\nMilford Senior High School\n\n\n39\nMount Pleasant High School\n\n\n40\nNewark Charter School\n\n\n41\nNewark High School\n\n\n42\nOdyssey Charter School\n\n\n43\nPOLYTECH High School\n\n\n44\nPenn (William) High School\n\n\n45\nPositive Outcomes Charter School\n\n\n46\nSeaford Senior High School\n\n\n47\nSmyrna High School\n\n\n48\nSt. Georges Technical High School\n\n\n49\nSussex Academy\n\n\n50\nSussex Central High School\n\n\n51\nSussex Consortium\n\n\n52\nSussex Orthopedic Program\n\n\n53\nSussex Technical High School\n\n\n54\nThe Wallace Wallin School\n\n\n55\nWoodbridge High School\n\n\n56\nduPont (Alexis I.) High School\n\n\n\n\n\n\n\nNOTE: You may have noticed the use of \"\"\" in the code above. This enables a string to be broken out across multiple lines which I find easier to read for SQL queries.\nWhat is happening here? Let’s break down this query.\nselect sch_name -- return only the school name column\nfrom directory -- from the data in our dataframe\nwhere statename = 'DELAWARE' -- where the state name is DELAWARE (state names are all caps in the dataset)\n  and g_12_offered = 'Yes' -- and where grade 12 is offered\norder by sch_name -- sort the resulting data alphabetically by the school name\nNext, let’s refine this query to identify which of those schools are charter schools or not.\n\nquery = \"\"\"\n  select \n    sch_name,\n    charter_text,\n  from directory\n  where statename = 'DELAWARE'\n    and g_12_offered = 'Yes'\n  order by sch_name\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nsch_name\ncharter_text\n\n\n\n\n0\nAppoquinimink High School\nNo\n\n\n1\nBrandywine High School\nNo\n\n\n2\nBrandywine SITE\nNo\n\n\n3\nBrennen School (The)\nNo\n\n\n4\nCaesar Rodney High School\nNo\n\n\n5\nCalloway (Cab) School of the Arts\nNo\n\n\n6\nCape Henlopen High School\nNo\n\n\n7\nCharter School of Wilmington\nYes\n\n\n8\nChristiana High School\nNo\n\n\n9\nConcord High School\nNo\n\n\n10\nConrad Schools of Science\nNo\n\n\n11\nDelaware Military Academy\nYes\n\n\n12\nDelaware School for the Deaf\nNo\n\n\n13\nDelcastle Technical High School\nNo\n\n\n14\nDelmar High School\nNo\n\n\n15\nDickinson (John) School\nNo\n\n\n16\nDouglass School\nNo\n\n\n17\nDover High School\nNo\n\n\n18\nEarly College High School at Del State\nYes\n\n\n19\nEnnis (Howard T.) School\nNo\n\n\n20\nFirst State Military Academy\nYes\n\n\n21\nFreire Charter School\nYes\n\n\n22\nGlasgow High School\nNo\n\n\n23\nGreat Oaks Charter School\nYes\n\n\n24\nHodgson (Paul M.) Vocational Technical High School\nNo\n\n\n25\nHoward High School of Technology\nNo\n\n\n26\nIndian River High School\nNo\n\n\n27\nIndian River Intensive Learning Center\nNo\n\n\n28\nJohn S. Charlton School\nNo\n\n\n29\nKent County Community School\nNo\n\n\n30\nKent County Secondary ILC\nNo\n\n\n31\nLake Forest High School\nNo\n\n\n32\nLaurel Senior High School\nNo\n\n\n33\nLeach (John G.) School\nNo\n\n\n34\nMOT Charter School\nYes\n\n\n35\nMcKean (Thomas) High School\nNo\n\n\n36\nMeadowood Program\nNo\n\n\n37\nMiddletown High School\nNo\n\n\n38\nMilford Senior High School\nNo\n\n\n39\nMount Pleasant High School\nNo\n\n\n40\nNewark Charter School\nYes\n\n\n41\nNewark High School\nNo\n\n\n42\nOdyssey Charter School\nYes\n\n\n43\nPOLYTECH High School\nNo\n\n\n44\nPenn (William) High School\nNo\n\n\n45\nPositive Outcomes Charter School\nYes\n\n\n46\nSeaford Senior High School\nNo\n\n\n47\nSmyrna High School\nNo\n\n\n48\nSt. Georges Technical High School\nNo\n\n\n49\nSussex Academy\nYes\n\n\n50\nSussex Central High School\nNo\n\n\n51\nSussex Consortium\nNo\n\n\n52\nSussex Orthopedic Program\nNo\n\n\n53\nSussex Technical High School\nNo\n\n\n54\nThe Wallace Wallin School\nNo\n\n\n55\nWoodbridge High School\nNo\n\n\n56\nduPont (Alexis I.) High School\nNo\n\n\n\n\n\n\n\nAnd if we want to limit our list to just the charter schools, we can add another filter to the where clause.\n\nquery = \"\"\"\n  select \n    sch_name,\n    charter_text,\n  from directory\n  where statename = 'DELAWARE'\n    and g_12_offered = 'Yes'\n    and charter_text = 'Yes'\n  order by sch_name\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nsch_name\ncharter_text\n\n\n\n\n0\nCharter School of Wilmington\nYes\n\n\n1\nDelaware Military Academy\nYes\n\n\n2\nEarly College High School at Del State\nYes\n\n\n3\nFirst State Military Academy\nYes\n\n\n4\nFreire Charter School\nYes\n\n\n5\nGreat Oaks Charter School\nYes\n\n\n6\nMOT Charter School\nYes\n\n\n7\nNewark Charter School\nYes\n\n\n8\nOdyssey Charter School\nYes\n\n\n9\nPositive Outcomes Charter School\nYes\n\n\n10\nSussex Academy\nYes"
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#counting-a-common-type-of-question-to-ask-about-data",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#counting-a-common-type-of-question-to-ask-about-data",
    "title": "SQL Basics using DuckDB",
    "section": "Counting: A common type of question to ask about data",
    "text": "Counting: A common type of question to ask about data\nAfter this initial investigation, my curiosity has been piqued. I am wondering how many schools are charters vs. traditional public schools in Delaware (regardless of grades offered). Let’s translate that question into a sql query using some basic aggregation.\n\nquery = \"\"\"\n  select \n    statename,\n    charter_text,\n    count(sch_name) as num_schools,\n  from directory\n  where statename = 'DELAWARE'\n  group by \n    statename, \n    charter_text\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\ncharter_text\nnum_schools\n\n\n\n\n0\nDELAWARE\nYes\n23\n\n\n1\nDELAWARE\nNo\n208\n\n\n\n\n\n\n\nHow does this work?\nThe count() function, predictably, will count the number of values in a given column (or all records if the * is passed instead). You can further expand this to count only unique school names by using count(distinct sch_name).\nWe are also aliasing the count using the as keyword to give the resulting column a name: num_schools.\nThe group by clause is necessary for this aggregation because it defines the context in which the count() function operates. Without the group by clause, the count() function would treat the entire result set as a single group.\nConsider this example instead (without grouping):\n\nquery = \"\"\"\n  select \n    count(sch_name) as num_schools,\n  from directory\n  where statename = 'DELAWARE'\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nnum_schools\n\n\n\n\n0\n231\n\n\n\n\n\n\n\nIn order to get a count for each value of the Yes/No flag in the charter_text column, we need to group our counts by each of those results. In addition, in order to include the statename in our result set, we need to include it in the grouping as well.\nIf we wanted to determine the same counts for all states in the dataset, we could simply remove the where clause limiting the data to Delaware.\n\nquery = \"\"\"\n  select \n    statename,\n    charter_text,\n    count(sch_name) as num_schools,\n  from directory\n  group by \n    statename, \n    charter_text\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\ncharter_text\nnum_schools\n\n\n\n\n0\nALABAMA\nNo\n1553\n\n\n1\nALABAMA\nYes\n13\n\n\n2\nALASKA\nNo\n478\n\n\n3\nALASKA\nYes\n31\n\n\n4\nARIZONA\nNo\n1909\n\n\n...\n...\n...\n...\n\n\n100\nGUAM\nYes\n3\n\n\n101\nNORTHERN MARIANAS\nNot applicable\n35\n\n\n102\nPUERTO RICO\nNo\n846\n\n\n103\nPUERTO RICO\nYes\n7\n\n\n104\nU.S. VIRGIN ISLANDS\nNot applicable\n23\n\n\n\n\n105 rows × 3 columns\n\n\n\nThis gives us some sense of the breakdown, but it actually might be useful to rephrase our question as: What number and percent of schools in each state are charter schools?\nWith that additional clarity, we can refine our query a bit using some conditional logic.\n\nquery = \"\"\"\n  select \n    statename,\n    count(\n        if(charter_text = 'Yes', sch_name, null)\n    ) as num_charter_schools,\n    count(sch_name) as num_schools,\n  from directory\n  group by statename\n  order by statename\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\nnum_charter_schools\nnum_schools\n\n\n\n\n0\nALABAMA\n13\n1566\n\n\n1\nALASKA\n31\n509\n\n\n2\nAMERICAN SAMOA\n0\n29\n\n\n3\nARIZONA\n606\n2515\n\n\n4\nARKANSAS\n93\n1099\n\n\n5\nBUREAU OF INDIAN EDUCATION\n0\n174\n\n\n6\nCALIFORNIA\n1337\n10456\n\n\n7\nCOLORADO\n268\n1960\n\n\n8\nCONNECTICUT\n21\n1009\n\n\n9\nDELAWARE\n23\n231\n\n\n10\nDISTRICT OF COLUMBIA\n125\n243\n\n\n11\nFLORIDA\n767\n4306\n\n\n12\nGEORGIA\n95\n2327\n\n\n13\nGUAM\n3\n44\n\n\n14\nHAWAII\n37\n296\n\n\n15\nIDAHO\n81\n801\n\n\n16\nILLINOIS\n138\n4409\n\n\n17\nINDIANA\n121\n1933\n\n\n18\nIOWA\n2\n1333\n\n\n19\nKANSAS\n9\n1360\n\n\n20\nKENTUCKY\n0\n1548\n\n\n21\nLOUISIANA\n150\n1383\n\n\n22\nMAINE\n13\n602\n\n\n23\nMARYLAND\n49\n1424\n\n\n24\nMASSACHUSETTS\n78\n1859\n\n\n25\nMICHIGAN\n386\n3570\n\n\n26\nMINNESOTA\n306\n2773\n\n\n27\nMISSISSIPPI\n7\n1054\n\n\n28\nMISSOURI\n81\n2469\n\n\n29\nMONTANA\n0\n832\n\n\n30\nNEBRASKA\n0\n1098\n\n\n31\nNEVADA\n99\n769\n\n\n32\nNEW HAMPSHIRE\n39\n498\n\n\n33\nNEW JERSEY\n87\n2582\n\n\n34\nNEW MEXICO\n102\n894\n\n\n35\nNEW YORK\n331\n4842\n\n\n36\nNORTH CAROLINA\n223\n2765\n\n\n37\nNORTH DAKOTA\n0\n536\n\n\n38\nNORTHERN MARIANAS\n0\n35\n\n\n39\nOHIO\n325\n3689\n\n\n40\nOKLAHOMA\n72\n1803\n\n\n41\nOREGON\n132\n1290\n\n\n42\nPENNSYLVANIA\n184\n2963\n\n\n43\nPUERTO RICO\n7\n853\n\n\n44\nRHODE ISLAND\n41\n319\n\n\n45\nSOUTH CAROLINA\n93\n1288\n\n\n46\nSOUTH DAKOTA\n0\n732\n\n\n47\nTENNESSEE\n121\n1931\n\n\n48\nTEXAS\n1056\n9652\n\n\n49\nU.S. VIRGIN ISLANDS\n0\n23\n\n\n50\nUTAH\n141\n1123\n\n\n51\nVERMONT\n0\n305\n\n\n52\nVIRGINIA\n7\n2136\n\n\n53\nWASHINGTON\n17\n2561\n\n\n54\nWEST VIRGINIA\n3\n700\n\n\n55\nWISCONSIN\n243\n2263\n\n\n56\nWYOMING\n5\n366\n\n\n\n\n\n\n\nThat gets us the counts of charter schools in each state as well as the total number of schools in each state. Let’s breakdown what this is doing:\ncount(\n  if(charter_text = 'Yes', sch_name, null)\n) as num_charter_schools,\nThe if() function checks whether a statement evaluates to true or not and then returns one value for true and another for false. Here, we are leveraging that functionality to count the school name if the value of the charter_text column is Yes but returning null if not. That works because the count() function ignores null values when aggregating.\nA similar, but slightly different approach could be done using the sum() function instead:\nsum(\n  if(charter_text = 'Yes', 1, 0)\n) as num_charter_schools,\nThis would treat each charter school as being equivalent to the value 1 and each non-charter schools as equivalent to the value 0 and then would add them up. The result is the same in this case. There can be subtle differences in the approach depending on the data being aggregated. It can be useful, at times, to compare the resulting differences (if any).\n\nCalculating the percentage of schools\nWe have our counts now, but we don’t have the percentage of schools and that is ultimately the question we are asking. To determine that, we need to divide the number of charters in each state by the total number of schools.\nWe could do that like this:\n\nquery = \"\"\"\n  select \n    statename,\n    count(if(charter_text = 'Yes', sch_name, null)) as num_charter_schools,\n    count(sch_name) as num_schools,\n    count(if(charter_text = 'Yes', sch_name, null))/count(sch_name) as percent_of_schools,\n  from directory\n  group by statename\n  order by statename\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\nnum_charter_schools\nnum_schools\npercent_of_schools\n\n\n\n\n0\nALABAMA\n13\n1566\n0.008301\n\n\n1\nALASKA\n31\n509\n0.060904\n\n\n2\nAMERICAN SAMOA\n0\n29\n0.000000\n\n\n3\nARIZONA\n606\n2515\n0.240954\n\n\n4\nARKANSAS\n93\n1099\n0.084622\n\n\n5\nBUREAU OF INDIAN EDUCATION\n0\n174\n0.000000\n\n\n6\nCALIFORNIA\n1337\n10456\n0.127869\n\n\n7\nCOLORADO\n268\n1960\n0.136735\n\n\n8\nCONNECTICUT\n21\n1009\n0.020813\n\n\n9\nDELAWARE\n23\n231\n0.099567\n\n\n10\nDISTRICT OF COLUMBIA\n125\n243\n0.514403\n\n\n11\nFLORIDA\n767\n4306\n0.178124\n\n\n12\nGEORGIA\n95\n2327\n0.040825\n\n\n13\nGUAM\n3\n44\n0.068182\n\n\n14\nHAWAII\n37\n296\n0.125000\n\n\n15\nIDAHO\n81\n801\n0.101124\n\n\n16\nILLINOIS\n138\n4409\n0.031300\n\n\n17\nINDIANA\n121\n1933\n0.062597\n\n\n18\nIOWA\n2\n1333\n0.001500\n\n\n19\nKANSAS\n9\n1360\n0.006618\n\n\n20\nKENTUCKY\n0\n1548\n0.000000\n\n\n21\nLOUISIANA\n150\n1383\n0.108460\n\n\n22\nMAINE\n13\n602\n0.021595\n\n\n23\nMARYLAND\n49\n1424\n0.034410\n\n\n24\nMASSACHUSETTS\n78\n1859\n0.041958\n\n\n25\nMICHIGAN\n386\n3570\n0.108123\n\n\n26\nMINNESOTA\n306\n2773\n0.110350\n\n\n27\nMISSISSIPPI\n7\n1054\n0.006641\n\n\n28\nMISSOURI\n81\n2469\n0.032807\n\n\n29\nMONTANA\n0\n832\n0.000000\n\n\n30\nNEBRASKA\n0\n1098\n0.000000\n\n\n31\nNEVADA\n99\n769\n0.128739\n\n\n32\nNEW HAMPSHIRE\n39\n498\n0.078313\n\n\n33\nNEW JERSEY\n87\n2582\n0.033695\n\n\n34\nNEW MEXICO\n102\n894\n0.114094\n\n\n35\nNEW YORK\n331\n4842\n0.068360\n\n\n36\nNORTH CAROLINA\n223\n2765\n0.080651\n\n\n37\nNORTH DAKOTA\n0\n536\n0.000000\n\n\n38\nNORTHERN MARIANAS\n0\n35\n0.000000\n\n\n39\nOHIO\n325\n3689\n0.088100\n\n\n40\nOKLAHOMA\n72\n1803\n0.039933\n\n\n41\nOREGON\n132\n1290\n0.102326\n\n\n42\nPENNSYLVANIA\n184\n2963\n0.062099\n\n\n43\nPUERTO RICO\n7\n853\n0.008206\n\n\n44\nRHODE ISLAND\n41\n319\n0.128527\n\n\n45\nSOUTH CAROLINA\n93\n1288\n0.072205\n\n\n46\nSOUTH DAKOTA\n0\n732\n0.000000\n\n\n47\nTENNESSEE\n121\n1931\n0.062662\n\n\n48\nTEXAS\n1056\n9652\n0.109407\n\n\n49\nU.S. VIRGIN ISLANDS\n0\n23\n0.000000\n\n\n50\nUTAH\n141\n1123\n0.125557\n\n\n51\nVERMONT\n0\n305\n0.000000\n\n\n52\nVIRGINIA\n7\n2136\n0.003277\n\n\n53\nWASHINGTON\n17\n2561\n0.006638\n\n\n54\nWEST VIRGINIA\n3\n700\n0.004286\n\n\n55\nWISCONSIN\n243\n2263\n0.107380\n\n\n56\nWYOMING\n5\n366\n0.013661\n\n\n\n\n\n\n\nNot bad, but I find that decimal representation of the percentage a bit harder to read. Let’s transform that to look like 12.5% by:\n\nmultiplying the decimal by 100 to get a whole number\nrounding to a single decimal digit\nconverting it to a string and adding the % sign\n\n\nquery = \"\"\"\n  select \n    statename,\n    count(if(charter_text = 'Yes', sch_name, null)) as num_charter_schools,\n    count(sch_name) as num_schools,\n    concat(cast(round((count(if(charter_text = 'Yes', sch_name, null))/count(sch_name)) * 100, 1) as string), '%') as percent_of_schools,\n  from directory\n  group by statename\n  order by statename\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\nnum_charter_schools\nnum_schools\npercent_of_schools\n\n\n\n\n0\nALABAMA\n13\n1566\n0.8%\n\n\n1\nALASKA\n31\n509\n6.1%\n\n\n2\nAMERICAN SAMOA\n0\n29\n0.0%\n\n\n3\nARIZONA\n606\n2515\n24.1%\n\n\n4\nARKANSAS\n93\n1099\n8.5%\n\n\n5\nBUREAU OF INDIAN EDUCATION\n0\n174\n0.0%\n\n\n6\nCALIFORNIA\n1337\n10456\n12.8%\n\n\n7\nCOLORADO\n268\n1960\n13.7%\n\n\n8\nCONNECTICUT\n21\n1009\n2.1%\n\n\n9\nDELAWARE\n23\n231\n10.0%\n\n\n10\nDISTRICT OF COLUMBIA\n125\n243\n51.4%\n\n\n11\nFLORIDA\n767\n4306\n17.8%\n\n\n12\nGEORGIA\n95\n2327\n4.1%\n\n\n13\nGUAM\n3\n44\n6.8%\n\n\n14\nHAWAII\n37\n296\n12.5%\n\n\n15\nIDAHO\n81\n801\n10.1%\n\n\n16\nILLINOIS\n138\n4409\n3.1%\n\n\n17\nINDIANA\n121\n1933\n6.3%\n\n\n18\nIOWA\n2\n1333\n0.2%\n\n\n19\nKANSAS\n9\n1360\n0.7%\n\n\n20\nKENTUCKY\n0\n1548\n0.0%\n\n\n21\nLOUISIANA\n150\n1383\n10.8%\n\n\n22\nMAINE\n13\n602\n2.2%\n\n\n23\nMARYLAND\n49\n1424\n3.4%\n\n\n24\nMASSACHUSETTS\n78\n1859\n4.2%\n\n\n25\nMICHIGAN\n386\n3570\n10.8%\n\n\n26\nMINNESOTA\n306\n2773\n11.0%\n\n\n27\nMISSISSIPPI\n7\n1054\n0.7%\n\n\n28\nMISSOURI\n81\n2469\n3.3%\n\n\n29\nMONTANA\n0\n832\n0.0%\n\n\n30\nNEBRASKA\n0\n1098\n0.0%\n\n\n31\nNEVADA\n99\n769\n12.9%\n\n\n32\nNEW HAMPSHIRE\n39\n498\n7.8%\n\n\n33\nNEW JERSEY\n87\n2582\n3.4%\n\n\n34\nNEW MEXICO\n102\n894\n11.4%\n\n\n35\nNEW YORK\n331\n4842\n6.8%\n\n\n36\nNORTH CAROLINA\n223\n2765\n8.1%\n\n\n37\nNORTH DAKOTA\n0\n536\n0.0%\n\n\n38\nNORTHERN MARIANAS\n0\n35\n0.0%\n\n\n39\nOHIO\n325\n3689\n8.8%\n\n\n40\nOKLAHOMA\n72\n1803\n4.0%\n\n\n41\nOREGON\n132\n1290\n10.2%\n\n\n42\nPENNSYLVANIA\n184\n2963\n6.2%\n\n\n43\nPUERTO RICO\n7\n853\n0.8%\n\n\n44\nRHODE ISLAND\n41\n319\n12.9%\n\n\n45\nSOUTH CAROLINA\n93\n1288\n7.2%\n\n\n46\nSOUTH DAKOTA\n0\n732\n0.0%\n\n\n47\nTENNESSEE\n121\n1931\n6.3%\n\n\n48\nTEXAS\n1056\n9652\n10.9%\n\n\n49\nU.S. VIRGIN ISLANDS\n0\n23\n0.0%\n\n\n50\nUTAH\n141\n1123\n12.6%\n\n\n51\nVERMONT\n0\n305\n0.0%\n\n\n52\nVIRGINIA\n7\n2136\n0.3%\n\n\n53\nWASHINGTON\n17\n2561\n0.7%\n\n\n54\nWEST VIRGINIA\n3\n700\n0.4%\n\n\n55\nWISCONSIN\n243\n2263\n10.7%\n\n\n56\nWYOMING\n5\n366\n1.4%"
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#cleaning-this-up-for-readability",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#cleaning-this-up-for-readability",
    "title": "SQL Basics using DuckDB",
    "section": "Cleaning this up for readability",
    "text": "Cleaning this up for readability\nAt this point we have a working query, but the syntax is getting a bit dense and hard to read. I’d like to break this up a bit to make it more clear what is happening.\nOur code has three main steps:\n\ncalculate the counts of charter schools and all schools for each state\ndivide the counts to calculate a percentage value as decimal\ntransform the decimal value to a string representation of the percentage\n\nWe can use nested subqueries to do this, but I consider that an anti-pattern in SQL and prefer the use of CTEs (common table expressions). They allow us to designate a query to be an intermediate result that can be further queried.\n\nNew Vocabulary\n\nCommon Table Expression (CTE) is a temporary result set that is defined within the scope of a single SQL statement. It allows you to create a named query that can be referenced multiple times within the main query. CTEs are primarily used to improve the readability, modularity, and reusability of SQL queries\n\n\n\nNote: Subqueries as an anti-pattern\nUsing nested subqueries in SQL can become an anti-pattern because it can lead to complex, hard-to-maintain queries and potential performance issues. Nested subqueries involve placing one query inside another, often making the overall query difficult to read, understand, and optimize. While nested subqueries can solve specific problems, using them excessively can lead to unreadable, inefficient, and hard-to-maintain SQL queries. It’s generally better to explore alternative techniques like joins, CTEs, and window functions for clearer, more optimized code.\n\nquery = \"\"\"\n  with school_counts as (\n    select \n      statename,\n      count(if(charter_text = 'Yes', sch_name, null)) as num_charter_schools,\n      count(sch_name) as num_schools,\n    from directory\n    group by statename\n  ),\n\n  percents as (\n      select\n        statename,\n        num_charter_schools,\n        num_schools,\n        round((num_charter_schools/num_schools) * 100, 1) as percent_of_schools\n      from school_counts\n  )\n\n  select\n    statename,\n    num_charter_schools,\n    num_schools,\n    concat(cast(percent_of_schools as string),'%') as percent_of_schools\n  from percents\n\"\"\"\n\ncharters = duckdb.sql(query).df()\n\nAnother way to do this that is unique to duckdb would be to store each query into a variable and execute queries against those.\n\nquery = \"\"\"\n  select \n      statename,\n      count(if(charter_text = 'Yes', sch_name, null)) as num_charter_schools,\n      count(sch_name) as num_schools,\n  from directory\n  group by statename\n\"\"\"\nschool_counts = duckdb.sql(query).df()\n\nquery = \"\"\"\n  select\n    statename,\n    num_charter_schools,\n    num_schools,\n    round((num_charter_schools/num_schools) * 100, 1) as percent_of_schools\n  from school_counts\n\"\"\"\npercents = duckdb.sql(query).df()\n\nquery = \"\"\"\n  select\n    statename,\n    num_charter_schools,\n    num_schools,\n    concat(cast(percent_of_schools as string),'%') as percent_of_schools\n  from percents\n\"\"\"\ncharters = duckdb.sql(query).df()\n\n\ncharters\n\n\n\n\n\n\n\n\nstatename\nnum_charter_schools\nnum_schools\npercent_of_schools\n\n\n\n\n0\nALABAMA\n13\n1566\n0.8%\n\n\n1\nALASKA\n31\n509\n6.1%\n\n\n2\nARIZONA\n606\n2515\n24.1%\n\n\n3\nARKANSAS\n93\n1099\n8.5%\n\n\n4\nCALIFORNIA\n1337\n10456\n12.8%\n\n\n5\nCOLORADO\n268\n1960\n13.7%\n\n\n6\nCONNECTICUT\n21\n1009\n2.1%\n\n\n7\nDELAWARE\n23\n231\n10.0%\n\n\n8\nDISTRICT OF COLUMBIA\n125\n243\n51.4%\n\n\n9\nFLORIDA\n767\n4306\n17.8%\n\n\n10\nGEORGIA\n95\n2327\n4.1%\n\n\n11\nHAWAII\n37\n296\n12.5%\n\n\n12\nIDAHO\n81\n801\n10.1%\n\n\n13\nILLINOIS\n138\n4409\n3.1%\n\n\n14\nINDIANA\n121\n1933\n6.3%\n\n\n15\nIOWA\n2\n1333\n0.2%\n\n\n16\nKANSAS\n9\n1360\n0.7%\n\n\n17\nKENTUCKY\n0\n1548\n0.0%\n\n\n18\nLOUISIANA\n150\n1383\n10.8%\n\n\n19\nMAINE\n13\n602\n2.2%\n\n\n20\nMARYLAND\n49\n1424\n3.4%\n\n\n21\nMASSACHUSETTS\n78\n1859\n4.2%\n\n\n22\nMICHIGAN\n386\n3570\n10.8%\n\n\n23\nMINNESOTA\n306\n2773\n11.0%\n\n\n24\nMISSISSIPPI\n7\n1054\n0.7%\n\n\n25\nMISSOURI\n81\n2469\n3.3%\n\n\n26\nMONTANA\n0\n832\n0.0%\n\n\n27\nNEBRASKA\n0\n1098\n0.0%\n\n\n28\nNEVADA\n99\n769\n12.9%\n\n\n29\nNEW HAMPSHIRE\n39\n498\n7.8%\n\n\n30\nNEW JERSEY\n87\n2582\n3.4%\n\n\n31\nNEW MEXICO\n102\n894\n11.4%\n\n\n32\nNEW YORK\n331\n4842\n6.8%\n\n\n33\nNORTH CAROLINA\n223\n2765\n8.1%\n\n\n34\nNORTH DAKOTA\n0\n536\n0.0%\n\n\n35\nOHIO\n325\n3689\n8.8%\n\n\n36\nOKLAHOMA\n72\n1803\n4.0%\n\n\n37\nOREGON\n132\n1290\n10.2%\n\n\n38\nPENNSYLVANIA\n184\n2963\n6.2%\n\n\n39\nRHODE ISLAND\n41\n319\n12.9%\n\n\n40\nSOUTH CAROLINA\n93\n1288\n7.2%\n\n\n41\nSOUTH DAKOTA\n0\n732\n0.0%\n\n\n42\nTENNESSEE\n121\n1931\n6.3%\n\n\n43\nTEXAS\n1056\n9652\n10.9%\n\n\n44\nUTAH\n141\n1123\n12.6%\n\n\n45\nVERMONT\n0\n305\n0.0%\n\n\n46\nVIRGINIA\n7\n2136\n0.3%\n\n\n47\nWASHINGTON\n17\n2561\n0.7%\n\n\n48\nWEST VIRGINIA\n3\n700\n0.4%\n\n\n49\nWISCONSIN\n243\n2263\n10.7%\n\n\n50\nWYOMING\n5\n366\n1.4%\n\n\n51\nBUREAU OF INDIAN EDUCATION\n0\n174\n0.0%\n\n\n52\nAMERICAN SAMOA\n0\n29\n0.0%\n\n\n53\nGUAM\n3\n44\n6.8%\n\n\n54\nNORTHERN MARIANAS\n0\n35\n0.0%\n\n\n55\nPUERTO RICO\n7\n853\n0.8%\n\n\n56\nU.S. VIRGIN ISLANDS\n0\n23\n0.0%\n\n\n\n\n\n\n\nEither way, we can now see the answer to our initial question. However, at this point a new question has come to mind: Which states have the highest percentage of charter schools?\nTo answer that, we can simply sort the results:\n\nquery = \"\"\"\n  select * \n  from charters \n  order by percent_of_schools desc\n\"\"\"\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\nnum_charter_schools\nnum_schools\npercent_of_schools\n\n\n\n\n0\nOHIO\n325\n3689\n8.8%\n\n\n1\nARKANSAS\n93\n1099\n8.5%\n\n\n2\nNORTH CAROLINA\n223\n2765\n8.1%\n\n\n3\nNEW HAMPSHIRE\n39\n498\n7.8%\n\n\n4\nSOUTH CAROLINA\n93\n1288\n7.2%\n\n\n5\nNEW YORK\n331\n4842\n6.8%\n\n\n6\nGUAM\n3\n44\n6.8%\n\n\n7\nINDIANA\n121\n1933\n6.3%\n\n\n8\nTENNESSEE\n121\n1931\n6.3%\n\n\n9\nPENNSYLVANIA\n184\n2963\n6.2%\n\n\n10\nALASKA\n31\n509\n6.1%\n\n\n11\nDISTRICT OF COLUMBIA\n125\n243\n51.4%\n\n\n12\nMASSACHUSETTS\n78\n1859\n4.2%\n\n\n13\nGEORGIA\n95\n2327\n4.1%\n\n\n14\nOKLAHOMA\n72\n1803\n4.0%\n\n\n15\nMARYLAND\n49\n1424\n3.4%\n\n\n16\nNEW JERSEY\n87\n2582\n3.4%\n\n\n17\nMISSOURI\n81\n2469\n3.3%\n\n\n18\nILLINOIS\n138\n4409\n3.1%\n\n\n19\nARIZONA\n606\n2515\n24.1%\n\n\n20\nMAINE\n13\n602\n2.2%\n\n\n21\nCONNECTICUT\n21\n1009\n2.1%\n\n\n22\nFLORIDA\n767\n4306\n17.8%\n\n\n23\nCOLORADO\n268\n1960\n13.7%\n\n\n24\nNEVADA\n99\n769\n12.9%\n\n\n25\nRHODE ISLAND\n41\n319\n12.9%\n\n\n26\nCALIFORNIA\n1337\n10456\n12.8%\n\n\n27\nUTAH\n141\n1123\n12.6%\n\n\n28\nHAWAII\n37\n296\n12.5%\n\n\n29\nNEW MEXICO\n102\n894\n11.4%\n\n\n30\nMINNESOTA\n306\n2773\n11.0%\n\n\n31\nTEXAS\n1056\n9652\n10.9%\n\n\n32\nLOUISIANA\n150\n1383\n10.8%\n\n\n33\nMICHIGAN\n386\n3570\n10.8%\n\n\n34\nWISCONSIN\n243\n2263\n10.7%\n\n\n35\nOREGON\n132\n1290\n10.2%\n\n\n36\nIDAHO\n81\n801\n10.1%\n\n\n37\nDELAWARE\n23\n231\n10.0%\n\n\n38\nWYOMING\n5\n366\n1.4%\n\n\n39\nALABAMA\n13\n1566\n0.8%\n\n\n40\nPUERTO RICO\n7\n853\n0.8%\n\n\n41\nKANSAS\n9\n1360\n0.7%\n\n\n42\nWASHINGTON\n17\n2561\n0.7%\n\n\n43\nMISSISSIPPI\n7\n1054\n0.7%\n\n\n44\nWEST VIRGINIA\n3\n700\n0.4%\n\n\n45\nVIRGINIA\n7\n2136\n0.3%\n\n\n46\nIOWA\n2\n1333\n0.2%\n\n\n47\nNORTH DAKOTA\n0\n536\n0.0%\n\n\n48\nSOUTH DAKOTA\n0\n732\n0.0%\n\n\n49\nNEBRASKA\n0\n1098\n0.0%\n\n\n50\nMONTANA\n0\n832\n0.0%\n\n\n51\nVERMONT\n0\n305\n0.0%\n\n\n52\nKENTUCKY\n0\n1548\n0.0%\n\n\n53\nBUREAU OF INDIAN EDUCATION\n0\n174\n0.0%\n\n\n54\nAMERICAN SAMOA\n0\n29\n0.0%\n\n\n55\nNORTHERN MARIANAS\n0\n35\n0.0%\n\n\n56\nU.S. VIRGIN ISLANDS\n0\n23\n0.0%\n\n\n\n\n\n\n\nOh no!!!\nOur conversion of the percentage into a string has ruined our ability to sort the values. This is why it is so vital to understand data types. Strings are sorted alphabetically but numbers are sorted numerically. What that means in this context is that when sorting descending (reverse), the string \"8\" is larger than the string \"50\" because 8 comes after 5. If the value were still numbers that would not be the case.\nTo resolve this we can preserve the original decimal column for sorting rather than replace it entirely.\n\nquery = \"\"\"\n  with school_counts as (\n    select \n      statename,\n      count(if(charter_text = 'Yes', sch_name, null)) as num_charter_schools,\n      count(sch_name) as num_schools,\n    from directory\n    group by statename\n  ),\n\n  percents as (\n      select\n        statename,\n        num_charter_schools,\n        num_schools,\n        round((num_charter_schools/num_schools) * 100, 1) as percent_of_schools_numeric\n      from school_counts\n  )\n\n  select\n    statename,\n    num_charter_schools,\n    num_schools,\n    concat(cast(percent_of_schools_numeric as string),'%') as percent_of_schools\n  from percents\n  order by percent_of_schools_numeric desc\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\nnum_charter_schools\nnum_schools\npercent_of_schools\n\n\n\n\n0\nDISTRICT OF COLUMBIA\n125\n243\n51.4%\n\n\n1\nARIZONA\n606\n2515\n24.1%\n\n\n2\nFLORIDA\n767\n4306\n17.8%\n\n\n3\nCOLORADO\n268\n1960\n13.7%\n\n\n4\nNEVADA\n99\n769\n12.9%\n\n\n5\nRHODE ISLAND\n41\n319\n12.9%\n\n\n6\nCALIFORNIA\n1337\n10456\n12.8%\n\n\n7\nUTAH\n141\n1123\n12.6%\n\n\n8\nHAWAII\n37\n296\n12.5%\n\n\n9\nNEW MEXICO\n102\n894\n11.4%\n\n\n10\nMINNESOTA\n306\n2773\n11.0%\n\n\n11\nTEXAS\n1056\n9652\n10.9%\n\n\n12\nLOUISIANA\n150\n1383\n10.8%\n\n\n13\nMICHIGAN\n386\n3570\n10.8%\n\n\n14\nWISCONSIN\n243\n2263\n10.7%\n\n\n15\nOREGON\n132\n1290\n10.2%\n\n\n16\nIDAHO\n81\n801\n10.1%\n\n\n17\nDELAWARE\n23\n231\n10.0%\n\n\n18\nOHIO\n325\n3689\n8.8%\n\n\n19\nARKANSAS\n93\n1099\n8.5%\n\n\n20\nNORTH CAROLINA\n223\n2765\n8.1%\n\n\n21\nNEW HAMPSHIRE\n39\n498\n7.8%\n\n\n22\nSOUTH CAROLINA\n93\n1288\n7.2%\n\n\n23\nNEW YORK\n331\n4842\n6.8%\n\n\n24\nGUAM\n3\n44\n6.8%\n\n\n25\nINDIANA\n121\n1933\n6.3%\n\n\n26\nTENNESSEE\n121\n1931\n6.3%\n\n\n27\nPENNSYLVANIA\n184\n2963\n6.2%\n\n\n28\nALASKA\n31\n509\n6.1%\n\n\n29\nMASSACHUSETTS\n78\n1859\n4.2%\n\n\n30\nGEORGIA\n95\n2327\n4.1%\n\n\n31\nOKLAHOMA\n72\n1803\n4.0%\n\n\n32\nMARYLAND\n49\n1424\n3.4%\n\n\n33\nNEW JERSEY\n87\n2582\n3.4%\n\n\n34\nMISSOURI\n81\n2469\n3.3%\n\n\n35\nILLINOIS\n138\n4409\n3.1%\n\n\n36\nMAINE\n13\n602\n2.2%\n\n\n37\nCONNECTICUT\n21\n1009\n2.1%\n\n\n38\nWYOMING\n5\n366\n1.4%\n\n\n39\nALABAMA\n13\n1566\n0.8%\n\n\n40\nPUERTO RICO\n7\n853\n0.8%\n\n\n41\nKANSAS\n9\n1360\n0.7%\n\n\n42\nMISSISSIPPI\n7\n1054\n0.7%\n\n\n43\nWASHINGTON\n17\n2561\n0.7%\n\n\n44\nWEST VIRGINIA\n3\n700\n0.4%\n\n\n45\nVIRGINIA\n7\n2136\n0.3%\n\n\n46\nIOWA\n2\n1333\n0.2%\n\n\n47\nKENTUCKY\n0\n1548\n0.0%\n\n\n48\nMONTANA\n0\n832\n0.0%\n\n\n49\nNEBRASKA\n0\n1098\n0.0%\n\n\n50\nNORTH DAKOTA\n0\n536\n0.0%\n\n\n51\nSOUTH DAKOTA\n0\n732\n0.0%\n\n\n52\nVERMONT\n0\n305\n0.0%\n\n\n53\nBUREAU OF INDIAN EDUCATION\n0\n174\n0.0%\n\n\n54\nAMERICAN SAMOA\n0\n29\n0.0%\n\n\n55\nNORTHERN MARIANAS\n0\n35\n0.0%\n\n\n56\nU.S. VIRGIN ISLANDS\n0\n23\n0.0%\n\n\n\n\n\n\n\nNow we can see the Washington DC has the highest percent of charter schools by a large margin. That is the legacy of Michelle Rhee’s impact on DC schools after passage of the D.C. Public Education Reform Amendment Act in 2007."
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#the-devil-is-in-the-details",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#the-devil-is-in-the-details",
    "title": "SQL Basics using DuckDB",
    "section": "The Devil is in the details",
    "text": "The Devil is in the details\nWhile DC might have the largest percent of charter schools it is out of a small number of schools to begin with given the size of Washington DC. If we sort by count we can see that California and Texas have the largest total number of charter schools, but because of the size of the population in those states, the total represents a smaller percentage.\nWashington DC is not even in the top 10.\n\nquery = \"\"\"\n  select * \n  from charters \n  order by num_charter_schools desc \n  limit 10\n\"\"\"\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nstatename\nnum_charter_schools\nnum_schools\npercent_of_schools\n\n\n\n\n0\nCALIFORNIA\n1337\n10456\n12.8%\n\n\n1\nTEXAS\n1056\n9652\n10.9%\n\n\n2\nFLORIDA\n767\n4306\n17.8%\n\n\n3\nARIZONA\n606\n2515\n24.1%\n\n\n4\nMICHIGAN\n386\n3570\n10.8%\n\n\n5\nNEW YORK\n331\n4842\n6.8%\n\n\n6\nOHIO\n325\n3689\n8.8%\n\n\n7\nMINNESOTA\n306\n2773\n11.0%\n\n\n8\nCOLORADO\n268\n1960\n13.7%\n\n\n9\nWISCONSIN\n243\n2263\n10.7%\n\n\n\n\n\n\n\nThis begs the question: Which states have the most students in charter schools?\nNot all school sizes are equal and there could be a large amount of schools with smaller enrollments or fewer schools with larger enrollments. Unfortunately, our initial dataset doesn’t have enrollment information. To determine this, we need new data. Going back to NCES, we can pull the school membership data as well. This is a much larger dataset (198 MB compressed) than the directory info but doesn’t have as many of the descriptors as our original dataset (such as the charter flag).\nTo answer this final query, we’ll need to combine the data in these two datasets somehow.\nThis is where SQL really shines. The sql join is one of the fundamental features of the language that makes it so powerful for data analysis. SQL joins are like puzzle pieces that help you combine information from different tables in a database. They’re useful when you want to see related data together, such as matching customers with their orders or students with their courses. Instead of keeping all the information in one big table, you can split it into smaller ones and then use joins to bring the pieces together, making it easier to understand and work with your data.\n\nenrollment = pd.read_csv(\"ccd_SCH_052_2122_l_1a_071722.csv\", dtype=str)\nenrollment.columns = [c.lower() for c in enrollment.columns]\nenrollment.head()\n\n\n\n\n\n\n\n\nschool_year\nfipst\nstatename\nst\nsch_name\nstate_agency_no\nunion\nst_leaid\nleaid\nst_schid\nncessch\nschid\ngrade\nrace_ethnicity\nsex\nstudent_count\ntotal_indicator\ndms_flag\n\n\n\n\n0\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\n01\nNaN\nAL-101\n0100005\nAL-101-0010\n010000500870\n0100870\nGrade 7\nAmerican Indian or Alaska Native\nFemale\n1\nCategory Set A - By Race/Ethnicity; Sex; Grade\nReported\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\n01\nNaN\nAL-101\n0100005\nAL-101-0010\n010000500870\n0100870\nGrade 7\nAmerican Indian or Alaska Native\nMale\n3\nCategory Set A - By Race/Ethnicity; Sex; Grade\nReported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\n01\nNaN\nAL-101\n0100005\nAL-101-0010\n010000500870\n0100870\nGrade 7\nAsian\nFemale\n2\nCategory Set A - By Race/Ethnicity; Sex; Grade\nReported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\n01\nNaN\nAL-101\n0100005\nAL-101-0010\n010000500870\n0100870\nGrade 7\nAsian\nMale\n2\nCategory Set A - By Race/Ethnicity; Sex; Grade\nReported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\n01\nNaN\nAL-101\n0100005\nAL-101-0010\n010000500870\n0100870\nGrade 7\nBlack or African American\nFemale\n7\nCategory Set A - By Race/Ethnicity; Sex; Grade\nReported"
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#exploring-enrollment",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#exploring-enrollment",
    "title": "SQL Basics using DuckDB",
    "section": "Exploring Enrollment",
    "text": "Exploring Enrollment\nMy goal here is to simply pull out the school-wide total enrollment for each school, but a quick scan of that dataframe preview indicates this data is pretty granular. It is broken out into grade levels, each student group for race/ethnicity, and even sex. But I do see there is a column called total_indicator that might be useful. I’d like to get a sense of what the values of that column are so I can determine if it could be used to filter the data somehow. To do that I will query the distinct values in that column.\n\npd.set_option('max_colwidth', 0) # this is to make sure pandas doesn't truncate the text in the column\n\nquery = \"\"\"\nselect distinct\n  total_indicator\nfrom enrollment\n\"\"\"\n\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\ntotal_indicator\n\n\n\n\n0\nCategory Set A - By Race/Ethnicity; Sex; Grade\n\n\n1\nDerived - Education Unit Total minus Adult Education Count\n\n\n2\nDerived - Subtotal by Race/Ethnicity and Sex minus Adult Education Count\n\n\n3\nEducation Unit Total\n\n\n4\nSubtotal 4 - By Grade\n\n\n\n\n\n\n\nWe can see that there are two values that might work here: - Education Unit Total - Derived - Education Unit Total minus Adult Education Count\nWe can use the first as it seems more straight-forward and includes the full number of students (including adult ed).\nBefore we do that, let’s pull out the relevant data from the dataset that we want to join to our directory data.\n\nquery = \"\"\"\n  select \n    school_year,\n    statename,\n    st,\n    sch_name,\n    st_schid,\n    ncessch,\n    student_count,\n  from enrollment\n  where total_indicator = 'Education Unit Total'\n\"\"\"\n\ntotal_enrollment = duckdb.sql(query).df()\n\n\ntotal_enrollment\n\n\n\n\n\n\n\n\nschool_year\nstatename\nst\nsch_name\nst_schid\nncessch\nstudent_count\n\n\n\n\n0\n2021-2022\nALABAMA\nAL\nAlbertville Middle School\nAL-101-0010\n010000500870\n920\n\n\n1\n2021-2022\nALABAMA\nAL\nAlbertville High School\nAL-101-0020\n010000500871\n1665\n\n\n2\n2021-2022\nALABAMA\nAL\nAlbertville Intermediate School\nAL-101-0110\n010000500879\n924\n\n\n3\n2021-2022\nALABAMA\nAL\nAlbertville Elementary School\nAL-101-0200\n010000500889\n891\n\n\n4\n2021-2022\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAL-101-0035\n010000501616\n579\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n100024\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nLOCKHART ELEMENTARY SCHOOL\nVI-001-15\n780003000024\n284\n\n\n100025\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nULLA F MULLER ELEMENTARY SCHOOL\nVI-001-17\n780003000026\n437\n\n\n100026\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nYVONNE BOWSKY ELEMENTARY SCHOOL\nVI-001-23\n780003000027\n381\n\n\n100027\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nCANCRYN JUNIOR HIGH SCHOOL\nVI-001-25\n780003000033\n744\n\n\n100028\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nBERTHA BOSCHULTE JUNIOR HIGH\nVI-001-9\n780003000034\n515\n\n\n\n\n100029 rows × 7 columns\n\n\n\nI’m going to include a few different fields that may be necessary for joining such as the school_year, statename, and a couple of different identifiers since we’re not 100% sure which one is unique enough to join yet.\nWe can do the same to the directory data and just keep the relevant fields for our analysis.\n\nquery = \"\"\"\n  select\n    school_year,\n    statename,\n    st,\n    sch_name,\n    st_schid,\n    ncessch,\n    charter_text\n  from directory\n\"\"\"\ndirectory_info = duckdb.sql(query).df()\n\n\ndirectory_info\n\n\n\n\n\n\n\n\nschool_year\nstatename\nst\nsch_name\nst_schid\nncessch\ncharter_text\n\n\n\n\n0\n2021-2022\nALABAMA\nAL\nAlbertville Middle School\nAL-101-0010\n010000500870\nNo\n\n\n1\n2021-2022\nALABAMA\nAL\nAlbertville High School\nAL-101-0020\n010000500871\nNo\n\n\n2\n2021-2022\nALABAMA\nAL\nAlbertville Intermediate School\nAL-101-0110\n010000500879\nNo\n\n\n3\n2021-2022\nALABAMA\nAL\nAlbertville Elementary School\nAL-101-0200\n010000500889\nNo\n\n\n4\n2021-2022\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAL-101-0035\n010000501616\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n102125\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nLOCKHART ELEMENTARY SCHOOL\nVI-001-15\n780003000024\nNot applicable\n\n\n102126\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nULLA F MULLER ELEMENTARY SCHOOL\nVI-001-17\n780003000026\nNot applicable\n\n\n102127\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nYVONNE BOWSKY ELEMENTARY SCHOOL\nVI-001-23\n780003000027\nNot applicable\n\n\n102128\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nCANCRYN JUNIOR HIGH SCHOOL\nVI-001-25\n780003000033\nNot applicable\n\n\n102129\n2021-2022\nU.S. VIRGIN ISLANDS\nVI\nBERTHA BOSCHULTE JUNIOR HIGH\nVI-001-9\n780003000034\nNot applicable\n\n\n\n\n102130 rows × 7 columns\n\n\n\nOne thing to note is that the directory dataset has 102,130 rows and the enrollment dataset has 100,029 rows. So there is a difference of 2,101 rows between them. When we join them we should expect our returned data to be within those ranges if we join accurately and depending on the type of join used.\nFor more info on the types of joins check out this great tutorial from dataschool.\n\nquery = \"\"\"\n  select\n    e.school_year,\n    e.statename,\n    e.st,\n    e.sch_name,\n    e.st_schid,\n    e.ncessch,\n    cast(e.student_count as int) as student_count,\n    d.charter_text\n  from directory_info d\n  inner join total_enrollment e\n    on d.school_year = e.school_year\n    and d.st = e.st\n    and d.ncessch = e.ncessch\n\"\"\"\nenrollments_with_info = duckdb.sql(query).df()\n\n\nenrollments_with_info\n\n\n\n\n\n\n\n\nschool_year\nstatename\nst\nsch_name\nst_schid\nncessch\nstudent_count\ncharter_text\n\n\n\n\n0\n2021-2022\nALABAMA\nAL\nAlbertville High School\nAL-101-0020\n010000500871\n1665.0\nNo\n\n\n1\n2021-2022\nALABAMA\nAL\nAlbertville Intermediate School\nAL-101-0110\n010000500879\n924.0\nNo\n\n\n2\n2021-2022\nALABAMA\nAL\nAlbertville Elementary School\nAL-101-0200\n010000500889\n891.0\nNo\n\n\n3\n2021-2022\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAL-101-0035\n010000501616\n579.0\nNo\n\n\n4\n2021-2022\nALABAMA\nAL\nAlbertville Primary School\nAL-101-0005\n010000502150\n977.0\nNo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n100024\n2021-2022\nPUERTO RICO\nPR\nALEJANDRO JR CRUZ (GALATEO PARCELAS)\nPR-01-71449\n720003002078\n129.0\nNo\n\n\n100025\n2021-2022\nPUERTO RICO\nPR\nBERWIND SUPERIOR\nPR-01-67942\n720003001333\n228.0\nNo\n\n\n100026\n2021-2022\nPUERTO RICO\nPR\nDR. ANTONIO S. PEDREIRA (ESPECIALIZADA)\nPR-01-61333\n720003001370\n299.0\nNo\n\n\n100027\n2021-2022\nPUERTO RICO\nPR\nTRINA PADILLA DE SANZ\nPR-01-61440\n720003001379\n425.0\nNo\n\n\n100028\n2021-2022\nPUERTO RICO\nPR\nJUDITH A VIVAS\nPR-01-16220\n720003001971\n247.0\nNo\n\n\n\n\n100029 rows × 8 columns\n\n\n\nThe inner join is something like the intersection of a venn diagram. It will return records (based on a matching condition or set of conditions) that appear in both datasets. Whereas a left join would return all the records in the first dataset along with null values for any records that don’t appear in the set that is being joined. This can be very useful when trying to determine which records are missing. For our purposes, we only only care about schools with actual enrollment (since we can’t calculate non-existing numbers of students).\n  from directory_info d\n  inner join total_enrollment e\n    on d.school_year = e.school_year\n    and d.st = e.st\n    and d.ncessch = e.ncessch\nNotice the use of table aliases (d and e resprectively) to help distinguish between columns in either dataset. Another way to write that join without the aliases would be:\n  from directory_info\n  inner join total_enrollment\n    on directory_info.school_year = total_enrollment.school_year\n    and directory_info.st = total_enrollment.st\n    and directory_info.ncessch = total_enrollment.ncessch\nFor smaller table names this can be fine, but it does get tedious over time and sensible aliases can make the syntax of queries easier to read.\nTable aliases and explicit table references are technically only required when the columns share names across both datasets (since the SQL interpreter can’t distinguish which one you mean) but it is generally considered a best practice when using joins to always specify the table source (preferably with an alias) in both the join conditions as well as in the select statement.\nBecause both of our original dataset are constrained by year, we could probably remove some of the join conditions.\n  from directory_info d\n  inner join total_enrollment e\n    on d.st = e.st\n    and d.ncessch = e.ncessch\nAnd if we are certain the NCES IDs are unique across states (they are), we could further simplify as:\n  from directory_info d\n  inner join total_enrollment e\n    on d.ncessch = e.ncessch"
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#calculating-of-students-in-charters",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#calculating-of-students-in-charters",
    "title": "SQL Basics using DuckDB",
    "section": "Calculating % of students in charters",
    "text": "Calculating % of students in charters\nNow that we’ve combined our datasets, we can write our aggregations similar to what we did for schools earlier. We could do this using CTEs or since we’ve stored the joined data in an output variable (as a dataframe) we can query that directly.\n\nquery = \"\"\"\n  select\n    school_year,\n    statename,\n    count(if(charter_text = 'Yes', sch_name, null)) as charter_schools,\n    count(sch_name) as total_schools,\n    round((count(if(charter_text = 'Yes', sch_name, null))/count(sch_name)) * 100, 1) as percent_charter_schools,\n    cast(sum(if(charter_text = 'Yes', student_count, 0)) as int) as charter_students,\n    cast(sum(student_count) as int) as total_students,\n    round((sum(if(charter_text = 'Yes', student_count, 0))/sum(student_count)) * 100, 1) as percent_charter_students,\n  from enrollments_with_info \n  group by \n    school_year,\n    statename\n  having sum(student_count) is not null\n    and sum(if(charter_text = 'Yes', student_count, 0)) &gt; 0\n  order by percent_charter_students desc\n\"\"\"\ncharter_enrollments = duckdb.sql(query).df()\n\n\ncharter_enrollments\n\n\n\n\n\n\n\n\nschool_year\nstatename\ncharter_schools\ntotal_schools\npercent_charter_schools\ncharter_students\ntotal_students\npercent_charter_students\n\n\n\n\n0\n2021-2022\nDISTRICT OF COLUMBIA\n123\n240\n51.2\n39476\n88543\n44.6\n\n\n1\n2021-2022\nARIZONA\n581\n2418\n24.0\n231195\n1131888\n20.4\n\n\n2\n2021-2022\nCOLORADO\n265\n1941\n13.7\n130279\n880582\n14.8\n\n\n3\n2021-2022\nNEVADA\n93\n746\n12.5\n63944\n488251\n13.1\n\n\n4\n2021-2022\nFLORIDA\n707\n4191\n16.9\n361634\n2832516\n12.8\n\n\n5\n2021-2022\nLOUISIANA\n146\n1366\n10.7\n86301\n680793\n12.7\n\n\n6\n2021-2022\nDELAWARE\n23\n229\n10.0\n17201\n139935\n12.3\n\n\n7\n2021-2022\nCALIFORNIA\n1291\n10167\n12.7\n678056\n5874619\n11.5\n\n\n8\n2021-2022\nUTAH\n137\n1105\n12.4\n77733\n687107\n11.3\n\n\n9\n2021-2022\nMICHIGAN\n374\n3538\n10.6\n150327\n1396598\n10.8\n\n\n10\n2021-2022\nPENNSYLVANIA\n179\n2941\n6.1\n163372\n1671899\n9.8\n\n\n11\n2021-2022\nNEW MEXICO\n99\n890\n11.1\n30160\n316785\n9.5\n\n\n12\n2021-2022\nIDAHO\n77\n784\n9.8\n28051\n313909\n8.9\n\n\n13\n2021-2022\nARKANSAS\n93\n1084\n8.6\n42341\n489565\n8.6\n\n\n14\n2021-2022\nNORTH CAROLINA\n204\n2719\n7.5\n131624\n1525223\n8.6\n\n\n15\n2021-2022\nOKLAHOMA\n70\n1792\n3.9\n59753\n696411\n8.6\n\n\n16\n2021-2022\nRHODE ISLAND\n40\n315\n12.7\n11387\n136864\n8.3\n\n\n17\n2021-2022\nTEXAS\n1010\n9105\n11.1\n443548\n5428609\n8.2\n\n\n18\n2021-2022\nOREGON\n131\n1285\n10.2\n42668\n540687\n7.9\n\n\n19\n2021-2022\nMINNESOTA\n278\n2661\n10.4\n66595\n868742\n7.7\n\n\n20\n2021-2022\nHAWAII\n37\n294\n12.6\n12114\n173178\n7.0\n\n\n21\n2021-2022\nNEW YORK\n330\n4802\n6.9\n173341\n2526204\n6.9\n\n\n22\n2021-2022\nOHIO\n324\n3659\n8.9\n115021\n1682397\n6.8\n\n\n23\n2021-2022\nSOUTH CAROLINA\n88\n1267\n6.9\n49344\n780403\n6.3\n\n\n24\n2021-2022\nGUAM\n3\n44\n6.8\n1783\n28402\n6.3\n\n\n25\n2021-2022\nWISCONSIN\n236\n2243\n10.5\n49715\n827393\n6.0\n\n\n26\n2021-2022\nALASKA\n30\n500\n6.0\n7621\n129944\n5.9\n\n\n27\n2021-2022\nMASSACHUSETTS\n78\n1842\n4.2\n48399\n911529\n5.3\n\n\n28\n2021-2022\nINDIANA\n119\n1915\n6.2\n50073\n1033897\n4.8\n\n\n29\n2021-2022\nNEW JERSEY\n87\n2558\n3.4\n58780\n1339937\n4.4\n\n\n30\n2021-2022\nTENNESSEE\n116\n1906\n6.1\n43908\n995549\n4.4\n\n\n31\n2021-2022\nGEORGIA\n94\n2314\n4.1\n69242\n1740875\n4.0\n\n\n32\n2021-2022\nILLINOIS\n136\n4386\n3.1\n60496\n1867412\n3.2\n\n\n33\n2021-2022\nNEW HAMPSHIRE\n36\n494\n7.3\n4934\n168797\n2.9\n\n\n34\n2021-2022\nMARYLAND\n48\n1417\n3.4\n24104\n881461\n2.7\n\n\n35\n2021-2022\nMISSOURI\n78\n2453\n3.2\n24146\n887077\n2.7\n\n\n36\n2021-2022\nCONNECTICUT\n21\n1000\n2.1\n11047\n496795\n2.2\n\n\n37\n2021-2022\nMAINE\n13\n571\n2.3\n2732\n168236\n1.6\n\n\n38\n2021-2022\nMISSISSIPPI\n7\n1040\n0.7\n2921\n442000\n0.7\n\n\n39\n2021-2022\nWYOMING\n5\n359\n1.4\n642\n92848\n0.7\n\n\n40\n2021-2022\nKANSAS\n9\n1355\n0.7\n2877\n480069\n0.6\n\n\n41\n2021-2022\nPUERTO RICO\n7\n843\n0.8\n1515\n259535\n0.6\n\n\n42\n2021-2022\nALABAMA\n11\n1494\n0.7\n3566\n747846\n0.5\n\n\n43\n2021-2022\nWASHINGTON\n17\n2546\n0.7\n4571\n1080143\n0.4\n\n\n44\n2021-2022\nVIRGINIA\n7\n1948\n0.4\n1278\n1244624\n0.1\n\n\n45\n2021-2022\nIOWA\n2\n1325\n0.2\n149\n503735\n0.0\n\n\n\n\n\n\n\nYou’ll notice some similar syntax as our previous aggregate query, but I have omitted the step converting the percent to a string here to aid in future analysis."
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#wrapping-up-with-some-rankings",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#wrapping-up-with-some-rankings",
    "title": "SQL Basics using DuckDB",
    "section": "Wrapping up with some rankings",
    "text": "Wrapping up with some rankings\nNow that we have the data we need for our investigation we can sort and re-sort the data and find answers to our inquiries, however one fundamental issue is coming up: the answer to our question is it sort of depends …\nWhich is very common in data analysis. Depending on how you phrase your question you might get different answers. Wouldn’t it be nice to have a definitive answer about which state, broadly, ranks highest for charter presence?\nTo do this, let’s explore a little more advanced of a topic in SQL queries: window functions.\nI won’t delve into them too deeply, but a basic understanding is helpful here:\n\nImagine you’re looking at a row of data in a table, like a line of people. A window function in SQL helps you calculate things for each person in the line, like finding their position compared to others, without changing the order. It’s like having a magic window that lets you peek at everyone’s info while keeping them in the same order they were in. This helps you figure out things like who’s first, who’s last, and who’s in the middle without messing up the original line.\n\nIn this case, we’d like to understand the ranking of each state for each calculation we created earlier. I.E. who is the #1 state for most charter schools and who is the #1 state for the most students in charter schools, etc. For thank we will use the rank() window function.\nAll window functions require an accompanying over() clause. They can get complicated but for our purposes we are just using them to specify the order to rank.\n\nquery = \"\"\"\n  select *,\n    rank() over(order by percent_charter_schools desc) as percent_schools_rank,\n    rank() over(order by charter_schools desc) as count_schools_rank,\n    rank() over(order by percent_charter_students desc) as percent_students_rank,\n    rank() over(order by charter_students desc) as count_students_rank,\n  from charter_enrollments\n  order by statename\n\"\"\"\n\nrankings = duckdb.sql(query).df()\n\n\nrankings\n\n\n\n\n\n\n\n\nschool_year\nstatename\ncharter_schools\ntotal_schools\npercent_charter_schools\ncharter_students\ntotal_students\npercent_charter_students\npercent_schools_rank\ncount_schools_rank\npercent_students_rank\ncount_students_rank\n\n\n\n\n0\n2021-2022\nALABAMA\n11\n1494\n0.7\n3566\n747846\n0.5\n41\n39\n43\n38\n\n\n1\n2021-2022\nALASKA\n30\n500\n6.0\n7621\n129944\n5.9\n29\n34\n27\n35\n\n\n2\n2021-2022\nARIZONA\n581\n2418\n24.0\n231195\n1131888\n20.4\n2\n4\n2\n4\n\n\n3\n2021-2022\nARKANSAS\n93\n1084\n8.6\n42341\n489565\n8.6\n20\n22\n14\n25\n\n\n4\n2021-2022\nCALIFORNIA\n1291\n10167\n12.7\n678056\n5874619\n11.5\n5\n1\n8\n1\n\n\n5\n2021-2022\nCOLORADO\n265\n1941\n13.7\n130279\n880582\n14.8\n4\n9\n3\n9\n\n\n6\n2021-2022\nCONNECTICUT\n21\n1000\n2.1\n11047\n496795\n2.2\n38\n36\n37\n34\n\n\n7\n2021-2022\nDELAWARE\n23\n229\n10.0\n17201\n139935\n12.3\n17\n35\n7\n31\n\n\n8\n2021-2022\nDISTRICT OF COLUMBIA\n123\n240\n51.2\n39476\n88543\n44.6\n1\n17\n1\n26\n\n\n9\n2021-2022\nFLORIDA\n707\n4191\n16.9\n361634\n2832516\n12.8\n3\n3\n5\n3\n\n\n10\n2021-2022\nGEORGIA\n94\n2314\n4.1\n69242\n1740875\n4.0\n31\n21\n32\n13\n\n\n11\n2021-2022\nGUAM\n3\n44\n6.8\n1783\n28402\n6.3\n25\n45\n24\n42\n\n\n12\n2021-2022\nHAWAII\n37\n294\n12.6\n12114\n173178\n7.0\n7\n32\n21\n32\n\n\n13\n2021-2022\nIDAHO\n77\n784\n9.8\n28051\n313909\n8.9\n18\n28\n13\n28\n\n\n14\n2021-2022\nILLINOIS\n136\n4386\n3.1\n60496\n1867412\n3.2\n36\n15\n33\n16\n\n\n15\n2021-2022\nINDIANA\n119\n1915\n6.2\n50073\n1033897\n4.8\n26\n18\n29\n19\n\n\n16\n2021-2022\nIOWA\n2\n1325\n0.2\n149\n503735\n0.0\n46\n46\n46\n46\n\n\n17\n2021-2022\nKANSAS\n9\n1355\n0.7\n2877\n480069\n0.6\n41\n40\n41\n40\n\n\n18\n2021-2022\nLOUISIANA\n146\n1366\n10.7\n86301\n680793\n12.7\n12\n13\n6\n11\n\n\n19\n2021-2022\nMAINE\n13\n571\n2.3\n2732\n168236\n1.6\n37\n38\n38\n41\n\n\n20\n2021-2022\nMARYLAND\n48\n1417\n3.4\n24104\n881461\n2.7\n33\n30\n35\n30\n\n\n21\n2021-2022\nMASSACHUSETTS\n78\n1842\n4.2\n48399\n911529\n5.3\n30\n26\n28\n22\n\n\n22\n2021-2022\nMICHIGAN\n374\n3538\n10.6\n150327\n1396598\n10.8\n13\n5\n10\n7\n\n\n23\n2021-2022\nMINNESOTA\n278\n2661\n10.4\n66595\n868742\n7.7\n15\n8\n20\n14\n\n\n24\n2021-2022\nMISSISSIPPI\n7\n1040\n0.7\n2921\n442000\n0.7\n41\n41\n39\n39\n\n\n25\n2021-2022\nMISSOURI\n78\n2453\n3.2\n24146\n887077\n2.7\n35\n26\n35\n29\n\n\n26\n2021-2022\nNEVADA\n93\n746\n12.5\n63944\n488251\n13.1\n8\n22\n4\n15\n\n\n27\n2021-2022\nNEW HAMPSHIRE\n36\n494\n7.3\n4934\n168797\n2.9\n22\n33\n34\n36\n\n\n28\n2021-2022\nNEW JERSEY\n87\n2558\n3.4\n58780\n1339937\n4.4\n33\n25\n30\n18\n\n\n29\n2021-2022\nNEW MEXICO\n99\n890\n11.1\n30160\n316785\n9.5\n10\n20\n12\n27\n\n\n30\n2021-2022\nNEW YORK\n330\n4802\n6.9\n173341\n2526204\n6.9\n23\n6\n22\n5\n\n\n31\n2021-2022\nNORTH CAROLINA\n204\n2719\n7.5\n131624\n1525223\n8.6\n21\n11\n14\n8\n\n\n32\n2021-2022\nOHIO\n324\n3659\n8.9\n115021\n1682397\n6.8\n19\n7\n23\n10\n\n\n33\n2021-2022\nOKLAHOMA\n70\n1792\n3.9\n59753\n696411\n8.6\n32\n29\n14\n17\n\n\n34\n2021-2022\nOREGON\n131\n1285\n10.2\n42668\n540687\n7.9\n16\n16\n19\n24\n\n\n35\n2021-2022\nPENNSYLVANIA\n179\n2941\n6.1\n163372\n1671899\n9.8\n27\n12\n11\n6\n\n\n36\n2021-2022\nPUERTO RICO\n7\n843\n0.8\n1515\n259535\n0.6\n40\n41\n41\n43\n\n\n37\n2021-2022\nRHODE ISLAND\n40\n315\n12.7\n11387\n136864\n8.3\n5\n31\n17\n33\n\n\n38\n2021-2022\nSOUTH CAROLINA\n88\n1267\n6.9\n49344\n780403\n6.3\n23\n24\n24\n21\n\n\n39\n2021-2022\nTENNESSEE\n116\n1906\n6.1\n43908\n995549\n4.4\n27\n19\n30\n23\n\n\n40\n2021-2022\nTEXAS\n1010\n9105\n11.1\n443548\n5428609\n8.2\n10\n2\n18\n2\n\n\n41\n2021-2022\nUTAH\n137\n1105\n12.4\n77733\n687107\n11.3\n9\n14\n9\n12\n\n\n42\n2021-2022\nVIRGINIA\n7\n1948\n0.4\n1278\n1244624\n0.1\n45\n41\n45\n44\n\n\n43\n2021-2022\nWASHINGTON\n17\n2546\n0.7\n4571\n1080143\n0.4\n41\n37\n44\n37\n\n\n44\n2021-2022\nWISCONSIN\n236\n2243\n10.5\n49715\n827393\n6.0\n14\n10\n26\n20\n\n\n45\n2021-2022\nWYOMING\n5\n359\n1.4\n642\n92848\n0.7\n39\n44\n39\n45\n\n\n\n\n\n\n\n\nCombining Rankings\nThese rankings are useful and can certainly make it easier to sort, but they haven’t provided us with any more definitive of an answer. To do that, let’s combine the rankings across metrics and find which state has the lowest value (ie. closest to #1 across all combined metrics).\nWe’ll once again use the rank function but across the sum of the other ranks.\n\nquery = \"\"\"\n  select *,\n    rank() over(order by (percent_schools_rank + count_schools_rank + percent_students_rank + count_students_rank)) as combined_ranking\n  from rankings\n  order by combined_ranking\n\"\"\"\nduckdb.sql(query).df()\n\n\n\n\n\n\n\n\nschool_year\nstatename\ncharter_schools\ntotal_schools\npercent_charter_schools\ncharter_students\ntotal_students\npercent_charter_students\npercent_schools_rank\ncount_schools_rank\npercent_students_rank\ncount_students_rank\ncombined_ranking\n\n\n\n\n0\n2021-2022\nARIZONA\n581\n2418\n24.0\n231195\n1131888\n20.4\n2\n4\n2\n4\n1\n\n\n1\n2021-2022\nFLORIDA\n707\n4191\n16.9\n361634\n2832516\n12.8\n3\n3\n5\n3\n2\n\n\n2\n2021-2022\nCALIFORNIA\n1291\n10167\n12.7\n678056\n5874619\n11.5\n5\n1\n8\n1\n3\n\n\n3\n2021-2022\nCOLORADO\n265\n1941\n13.7\n130279\n880582\n14.8\n4\n9\n3\n9\n4\n\n\n4\n2021-2022\nTEXAS\n1010\n9105\n11.1\n443548\n5428609\n8.2\n10\n2\n18\n2\n5\n\n\n5\n2021-2022\nMICHIGAN\n374\n3538\n10.6\n150327\n1396598\n10.8\n13\n5\n10\n7\n6\n\n\n6\n2021-2022\nLOUISIANA\n146\n1366\n10.7\n86301\n680793\n12.7\n12\n13\n6\n11\n7\n\n\n7\n2021-2022\nUTAH\n137\n1105\n12.4\n77733\n687107\n11.3\n9\n14\n9\n12\n8\n\n\n8\n2021-2022\nDISTRICT OF COLUMBIA\n123\n240\n51.2\n39476\n88543\n44.6\n1\n17\n1\n26\n9\n\n\n9\n2021-2022\nNEVADA\n93\n746\n12.5\n63944\n488251\n13.1\n8\n22\n4\n15\n10\n\n\n10\n2021-2022\nNORTH CAROLINA\n204\n2719\n7.5\n131624\n1525223\n8.6\n21\n11\n14\n8\n11\n\n\n11\n2021-2022\nNEW YORK\n330\n4802\n6.9\n173341\n2526204\n6.9\n23\n6\n22\n5\n12\n\n\n12\n2021-2022\nPENNSYLVANIA\n179\n2941\n6.1\n163372\n1671899\n9.8\n27\n12\n11\n6\n12\n\n\n13\n2021-2022\nMINNESOTA\n278\n2661\n10.4\n66595\n868742\n7.7\n15\n8\n20\n14\n14\n\n\n14\n2021-2022\nOHIO\n324\n3659\n8.9\n115021\n1682397\n6.8\n19\n7\n23\n10\n15\n\n\n15\n2021-2022\nNEW MEXICO\n99\n890\n11.1\n30160\n316785\n9.5\n10\n20\n12\n27\n16\n\n\n16\n2021-2022\nWISCONSIN\n236\n2243\n10.5\n49715\n827393\n6.0\n14\n10\n26\n20\n17\n\n\n17\n2021-2022\nOREGON\n131\n1285\n10.2\n42668\n540687\n7.9\n16\n16\n19\n24\n18\n\n\n18\n2021-2022\nARKANSAS\n93\n1084\n8.6\n42341\n489565\n8.6\n20\n22\n14\n25\n19\n\n\n19\n2021-2022\nRHODE ISLAND\n40\n315\n12.7\n11387\n136864\n8.3\n5\n31\n17\n33\n20\n\n\n20\n2021-2022\nIDAHO\n77\n784\n9.8\n28051\n313909\n8.9\n18\n28\n13\n28\n21\n\n\n21\n2021-2022\nDELAWARE\n23\n229\n10.0\n17201\n139935\n12.3\n17\n35\n7\n31\n22\n\n\n22\n2021-2022\nHAWAII\n37\n294\n12.6\n12114\n173178\n7.0\n7\n32\n21\n32\n23\n\n\n23\n2021-2022\nINDIANA\n119\n1915\n6.2\n50073\n1033897\n4.8\n26\n18\n29\n19\n23\n\n\n24\n2021-2022\nOKLAHOMA\n70\n1792\n3.9\n59753\n696411\n8.6\n32\n29\n14\n17\n23\n\n\n25\n2021-2022\nSOUTH CAROLINA\n88\n1267\n6.9\n49344\n780403\n6.3\n23\n24\n24\n21\n23\n\n\n26\n2021-2022\nGEORGIA\n94\n2314\n4.1\n69242\n1740875\n4.0\n31\n21\n32\n13\n27\n\n\n27\n2021-2022\nTENNESSEE\n116\n1906\n6.1\n43908\n995549\n4.4\n27\n19\n30\n23\n28\n\n\n28\n2021-2022\nILLINOIS\n136\n4386\n3.1\n60496\n1867412\n3.2\n36\n15\n33\n16\n29\n\n\n29\n2021-2022\nMASSACHUSETTS\n78\n1842\n4.2\n48399\n911529\n5.3\n30\n26\n28\n22\n30\n\n\n30\n2021-2022\nNEW JERSEY\n87\n2558\n3.4\n58780\n1339937\n4.4\n33\n25\n30\n18\n30\n\n\n31\n2021-2022\nALASKA\n30\n500\n6.0\n7621\n129944\n5.9\n29\n34\n27\n35\n32\n\n\n32\n2021-2022\nMISSOURI\n78\n2453\n3.2\n24146\n887077\n2.7\n35\n26\n35\n29\n32\n\n\n33\n2021-2022\nNEW HAMPSHIRE\n36\n494\n7.3\n4934\n168797\n2.9\n22\n33\n34\n36\n32\n\n\n34\n2021-2022\nMARYLAND\n48\n1417\n3.4\n24104\n881461\n2.7\n33\n30\n35\n30\n35\n\n\n35\n2021-2022\nGUAM\n3\n44\n6.8\n1783\n28402\n6.3\n25\n45\n24\n42\n36\n\n\n36\n2021-2022\nCONNECTICUT\n21\n1000\n2.1\n11047\n496795\n2.2\n38\n36\n37\n34\n37\n\n\n37\n2021-2022\nMAINE\n13\n571\n2.3\n2732\n168236\n1.6\n37\n38\n38\n41\n38\n\n\n38\n2021-2022\nWASHINGTON\n17\n2546\n0.7\n4571\n1080143\n0.4\n41\n37\n44\n37\n39\n\n\n39\n2021-2022\nMISSISSIPPI\n7\n1040\n0.7\n2921\n442000\n0.7\n41\n41\n39\n39\n40\n\n\n40\n2021-2022\nALABAMA\n11\n1494\n0.7\n3566\n747846\n0.5\n41\n39\n43\n38\n41\n\n\n41\n2021-2022\nKANSAS\n9\n1355\n0.7\n2877\n480069\n0.6\n41\n40\n41\n40\n42\n\n\n42\n2021-2022\nPUERTO RICO\n7\n843\n0.8\n1515\n259535\n0.6\n40\n41\n41\n43\n43\n\n\n43\n2021-2022\nWYOMING\n5\n359\n1.4\n642\n92848\n0.7\n39\n44\n39\n45\n44\n\n\n44\n2021-2022\nVIRGINIA\n7\n1948\n0.4\n1278\n1244624\n0.1\n45\n41\n45\n44\n45\n\n\n45\n2021-2022\nIOWA\n2\n1325\n0.2\n149\n503735\n0.0\n46\n46\n46\n46\n46"
  },
  {
    "objectID": "posts/sql-basics-with-duckdb/sql_basics.html#conclusion",
    "href": "posts/sql-basics-with-duckdb/sql_basics.html#conclusion",
    "title": "SQL Basics using DuckDB",
    "section": "Conclusion",
    "text": "Conclusion\nWe can see that Arizona ranks highest for charter presence, followed by Florida. A quite different story than we were getting previously with our one-off questions. Fascinating!\nThere is much more you can do as you expand your knowledge of sql; including taking this data and visualizing it for presentation to an audience. That’s a topic for another day but let’s store our results in a new dataset.\n\nquery = \"\"\"\n  select *,\n    rank() over(order by (percent_schools_rank + count_schools_rank + percent_students_rank + count_students_rank)) as combined_ranking\n  from rankings\n  order by combined_ranking\n\"\"\"\ncharter_presence_rankings = duckdb.sql(query).df()\ncharter_presence_rankings.to_csv(\"charter_presence_rankings.csv.gz\", compression=\"gzip\", index=False)\n\nYou can download this data here if you like.\n\nSummary\nIn this tutorial we learned: 1. some basic SQL vocabulary 2. how to read data from files into dataframes for querying 3. how to explore the structure of a new dataset 4. how to select data from a dataframe using duckdb and sql syntax 5. how to filter and limit data 6. how to translate questions into queries 7. how to do some basic aggregations like count and sum 8. how to better organize complex queries with CTEs 9. a valuable lesson about data types 10. how to join data from multiple datasets 11. how to leverage a basic window function rank to come to a more definitive answer 12. how to store the output of our analysis for future use\nI hope this was helpful and gave you a taste of how to iterate on a set of questions using SQL to find an answer in a structured way (even without access to a database)."
  },
  {
    "objectID": "posts/new-years-resolutions/index.html",
    "href": "posts/new-years-resolutions/index.html",
    "title": "Resolutions for the New Year",
    "section": "",
    "text": "Well, it’s 2018 and it’s been two years since I last made an update to my blog. I originally published it to test out Jekyll and to put something basic on my personal domain. I was working with Ruby on Rails at the time and Jekyll seemed the natural choice.\nIn the intervening time, my work has shifted and we are more and more aligning on standardizing our entire stack to be Python based (the advantages of which, I will expound upon in another post). With that has come a desire to teach more of my colleagues (particularly those not directly involved in programming) to take advantage of Python in their work. Data analysts can leverage it for data science and predictive analytics work and back-office folks like our accountants and financial analysts can leverage it for task automation.\nI’ve also been getting requests from some friends and family to teach them a bit of programming and Python seems like a natural choice for beginners. With that in mind, I’ve made it my New Year’s resolution to teach as many people as possible this year to program.\nI’m rebooting my blog to post helpful resources and connect with folks interested in learning and that means giving my site a much needed boost of new energy. I’ve rebuilt it from the ground up in Pelican so that it’ll be in Python as well.\nHere’s to 2018, the year of Python!"
  },
  {
    "objectID": "posts/mermaid-diagrams/index.html",
    "href": "posts/mermaid-diagrams/index.html",
    "title": "Treating Diagrams Like Code",
    "section": "",
    "text": "There is a common saying in software development that “code should speak for itself” and while I agree with the ideas behind that statement and the need for code that is inherently readable and “self-documenting”, I often find myself drawing diagrams to explain many things related to code. Such as:\n\ndata models/schemas\ninfrastructure components\nworkflows\nsoftware architecture\n\nDiagrams are useful for a number of reasons. But mostly they are a vehicle for collaboration. Being able to take a complex idea and reduce it to a simpler high level explanation allows easier hand-off and/or onboarding of collaborators. Not only that, but it is helpful for communicating with less technical audiences without completely hand-waving the complexity away as technical wizardry.\nIt’s also incredibly useful for crystalizing a strategy into a coherent vision before executing. I.E. a low-effort way to vet ideas before spending time building them out.\nThat said, I find myself repeatedly at a loss for a good diagramming tool that feels like it is a part of the software development tool chain instead of set apart like any other document. For documentation, I can reach for yaml when writing dbt models and have that feel like a seemless part of my development workflow or markdown in other python repos. But for diagrams, I find myself creating them in some other tool (like Lucidchart) and then having to export them to an image and embed that in my other documentation. That feels lackluster on many levels, not the least of which is the lack of version control to capture the iteration of ideas in the diagrams but also that the source of that diagram is elsewhere.\nI’ve recently discovered mermaid and have begun experimenting with it and so far I am wanting to learn more. It has a ton of potential. The diagrams aren’t the prettiest but the fact that they are code-based and can be expressed in markdown and quarto is pretty exciting.\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]"
  },
  {
    "objectID": "posts/dev-env-setup/index.html",
    "href": "posts/dev-env-setup/index.html",
    "title": "Automating Developer Environment Setup",
    "section": "",
    "text": "I develop on Linux. Ubuntu to be specific. Most of the web apps I build I end up deploying to Heroku. Since their stack is based on Ubuntu and Postgres, I try to make my local environment as close to production as possible.\nThat means I don’t use simpler development databases like SQLite in my local setup and I don’t build on Windows or Mac and then hope my code will deploy properly in production. I don’t want any suprises!\nTo that effect, I try to automate my setup as much as possible.\nFor work, I develop within a Linux virtual machine (using Virtualbox). At home, I’m already using linux as my main os. But I want my setup, tools, and configurations to be identical no matter where I am working. I also want to be able to spin up a new VM at a moment’s notice and be ready to go in no time.\nTo help facilitate that need, I have a github repo where I store my dotfiles and a few shell scripts to easily setup my development environment.\nI prefer to work in the terminal as much as possible and so I use Vim as my text editor with a few custom configurations to emulate some IDE like features.\nI also prefer to handle version control with git in the terminal and have a few custom aliases I like to make sure are available wherever I’m working.\nBut the most valuable script I have is my database setup script. Installing Postgres can be daunting, particularly at the start of a new project. I don’t want that to be a hindrance for myself or other developers on my team. While I have a generic postgres setup script, I also have a more custom one for use in Django web apps that I store in the source code of each app I deploy. That can be a huge help when handing off to another developer on the team, so they have exactly the same database configuration as every other developer working on the project, with single button deployment.\n\nVIM Config\nMy Vim config is pretty simple compared to many I’ve seen, but there are a few things I can’t live without. I was big on Sublime before I moved to Vim and so I really appreciate having a fuzzy finder and file tree available. So I use NERDTree and CtrlP for that. I also like being able to easily comment blocks of code and use NERDCommenter for that.\nAs I’ve shifted to using Python more, I found this tutorial from Real Python really helpful for thinking about how I wanted Vim configured to better support Python specifically. The main call outs are white space config (both indents and trailing whitespace), code-completion, and PEP8 compliance. Using static analysis tools like Flake8 has greatly cut down on the need for those arguments during code review.\nBeyond those Vundle plugins, I keep a copy of my current .vimrc file in my repo for easy syncing. If I make a change locally, I push it to the repo so I can pull it down in any other environment I’m working in. I also like a certain color scheme and run a support script during setup to ensure both my terminal and Vim use the same colors. It cuts down on context switching when I have to hop in and out of my editor to run commands.\n\n\nGit Config\nMy git settings mostly consist of setting up my editor to be Vim by default and a series of helpful aliases. I drew a lot of inspiration from this article from You’ve Been Haacked.\nI use the quick commit and pretty formatted git log constantly. Likewise the grep functionality for the commit history is incredibly useful.\n\n\nBash Config\nMy .bashrc is pretty standard, with one key exception. I really like being able to see the git branch name in my prompt. I incorporated this tip from Coderwall, which couldn’t be any easier. Other than that I mostly have aliases to quickly jump to current project directories I’m working on. Although lately, I’ve been experimenting with some aliases for Django. Typing out python manage.py runserver or even ./manage.py makemigrations all the time can be a little tedious. I know I could use django-shortcuts but it really seems like a bash function ought to be able to handle my needs.\n\n\nPostgres Installation\nWhen I first was working in Rails and trying to set up PostgreSQL, I had a lot of hiccups initially. I’d install with the wrong account permissions, or I was missing some dependency. I’d get it configured, then port to a new system and have to spend time trying to get it all back to working order. It was a serious headache.\nThen, I found this great resource for installing Ruby on Rails on Ubuntu from Go Rails which had a small aside about Postgres. I was inspired by the simplicity of their directions and translated it into a shell script to automate it.\nWhile I have since largely put Rails behind me (I still have a few legacy apps that I maintain), I am absolutely indebted to the advice about databse setup.\nJust to show how simple this is, here is my postgre setup script. It’s 7 lines.\n#!/bin/bash\n\nuser=$(whoami)\n\nsudo sh -c \"echo 'deb http://apt.postgresql.org/pub/repos/apt/ xenial-pgdg main' &gt; /etc/apt/sources.list.d/pgdg.list\"\nwget --quiet -O - http://apt.postgresql.org/pub/repos/apt/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install -y postgresql-common\nsudo apt-get install -y postgresql libpq-dev\n\nsudo -u postgres createuser $user -s\nIt does all the heavy lifting. Figures out what account I am installing with, gets the necessary repository info, installs postgres, and even sets up a user based on my account to access it with.\nI’ve taken this idea a little further for Django and now run something a little more complicated.\n#!/bin/bash\n\n# Exit if command fails\nset -e\n# Treat unset variables as errors\nset -u\n\n# Set user as current account\nuser=$(whoami)\n\n# Install Postgres 10\nsudo sh -c \"echo 'deb http://apt.postgresql.org/pub/repos/apt/ xenial-pgdg main' &gt;&gt; /etc/apt/sources.list.d/pgdg.list\"\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install -y postgresql-common\nsudo apt-get install -y postgresql-10 postgresql-contrib libpq-dev\n\n# Create superuser account as self for local management\nsudo -u postgres createuser $user -s\n\n# Set env vars for colors\nYELLOW='\\033[1;33m'\nNC='\\033[0m'\n\n# Collect arguments from user\n# Project specific values\nprintf \"${YELLOW}Database name:\\n${NC}\"\nread database\nprintf \"${YELLOW}Username:\\n${NC}\"\nread username\nprintf \"${YELLOW}Password:\\n${NC}\"\nread password\n\n# Create database and user\nRUN_ON_PSQL=\"psql -X -U $user --set ON_ERROR_STOP=on --set AUTOCOMMIT=off postgres\"\n$RUN_ON_PSQL &lt;&lt;SQL\nCREATE DATABASE $database;\nCREATE USER $username WITH PASSWORD '$password';\nALTER ROLE $username SET client_encoding TO 'utf8';\nALTER ROLE $username SET default_transaction_isolation TO 'read committed';\nALTER ROLE $username SET timezone TO 'UTC';\nGRANT ALL PRIVILEGES ON DATABASE $database TO $username; \nALTER USER $username CREATEDB;\ncommit;\nSQL\n\nexit 0\nNow my team can clone my repo, run this app and be good to go in seconds. It could not be any easier to hand off my code!\n\n\nConclusion\nThere are always ways to automate your workflow. I am constantly on the look out for ways I can take the guess work out of my development configuration.\nThere are other things I rely on that I haven’t mentioned here in detail (but you can find them in my repo) like installing Heroku CLI or ngrok.\nI keep adding more automation to my setup as I go. In fact, I’m pretty sure that’s my most active public repo!"
  },
  {
    "objectID": "posts/google-classroom-connector/index.html",
    "href": "posts/google-classroom-connector/index.html",
    "title": "InnovateEDU Webinar - 19 November 2020",
    "section": "",
    "text": "In the wake of COVID19, we needed to provide our schools with visibility into student engagement with distance learning. The majority of our schools were using Google Classroom as their LMS (Learning Management System) and we knew that pulling this data down into our data warehouse ASAP would be a vital piece of that puzzle.\nWe initially built a data pipeline or “connector” using the Google Classroom API and published it to a public repo for others to use. I partnered closely with Zach Kagin who had volunteered his time pro-bono to the project to support us in this vital time. Shout out to his incredible work on batching the API calls which substantially improved the performance of the codebase.\nSoon after, Erin Mote from InnovateEDU reached out about a possible partnership to fork the repo and produce a version that could load the data to BigQuery rather than SQL Server so it could be run on a free instance of Google Cloud Platform with minimal effort. I partnered with Marcos Alcozer to translate our inital work into something that would be easily deployable on Google Cloud Shell. His team then put together beautiful and simple documentation to make it even more effortless to spin up this solution for Districts and CMOs around the country."
  },
  {
    "objectID": "posts/google-classroom-connector/index.html#google-classroom-connector",
    "href": "posts/google-classroom-connector/index.html#google-classroom-connector",
    "title": "InnovateEDU Webinar - 19 November 2020",
    "section": "",
    "text": "In the wake of COVID19, we needed to provide our schools with visibility into student engagement with distance learning. The majority of our schools were using Google Classroom as their LMS (Learning Management System) and we knew that pulling this data down into our data warehouse ASAP would be a vital piece of that puzzle.\nWe initially built a data pipeline or “connector” using the Google Classroom API and published it to a public repo for others to use. I partnered closely with Zach Kagin who had volunteered his time pro-bono to the project to support us in this vital time. Shout out to his incredible work on batching the API calls which substantially improved the performance of the codebase.\nSoon after, Erin Mote from InnovateEDU reached out about a possible partnership to fork the repo and produce a version that could load the data to BigQuery rather than SQL Server so it could be run on a free instance of Google Cloud Platform with minimal effort. I partnered with Marcos Alcozer to translate our inital work into something that would be easily deployable on Google Cloud Shell. His team then put together beautiful and simple documentation to make it even more effortless to spin up this solution for Districts and CMOs around the country."
  },
  {
    "objectID": "posts/google-classroom-connector/index.html#webinar",
    "href": "posts/google-classroom-connector/index.html#webinar",
    "title": "InnovateEDU Webinar - 19 November 2020",
    "section": "Webinar",
    "text": "Webinar\nYou can see Erin and I presenting a tutorial on how to deploy this solution below.\n\n\n\n\nI’m incredibly proud of this work and the impact it was able to have in such a critical time. It was instrumental in my team’s ability to produce meaningful insights on distance learning engagement and even helped support tracking asynchronous attendance with a level of fidelity other orgs struggled to produce without massive manual overhead."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dchess.org",
    "section": "",
    "text": "SQL Basics using DuckDB\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nConfiguring Pandas with Yaml Files\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTreating Diagrams Like Code\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGoodbye Pelican, Hello Quarto\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nI Can Python, So Can You\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nParsing Texas Assessment Data\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nCreating WordClouds\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\nAuto-generated REST API for MS SQL database\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\nVirtual K-12 Tableau User Group - 14 January 2021\n\n\n\n\n\n\n\nprojects\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow I Teach Git\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nNormalize JSON with Pandas\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\nInnovateEDU Webinar - 19 November 2020\n\n\n\n\n\n\n\nprojects\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMy Manager README\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2018\n\n\n\n\n\n\n  \n\n\n\n\nAutomating Developer Environment Setup\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2018\n\n\n\n\n\n\n  \n\n\n\n\nResolutions for the New Year\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2018\n\n\n\n\n\n\n  \n\n\n\n\nHow Memes Inform My Work\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2018\n\n\n\n\n\n\n  \n\n\n\n\nStudent Email Sync\n\n\n\n\n\n\n\nprojects\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I got started, as a young teen, programming in BASIC on a used Commodore64 in the early 90s. I quickly found my way to HTML4 and built some of my first websites on GeoCities.\n\nIn college I tried my hand at Computer Science, but was more and more attracted to the Social Sciences and eventually ended up teaching high school Social Studies.\nI soon needed better tools to support my students and started building out some simple web apps in PHP on the backend of a Wordpress blog to help me teach in the days before blended learning and EdTech were ubiquitous in the education sector.\nAfter a few years of building my skills, I eventually found myself working for Aspire Public Schools on the Godzilla team alongside the folks who went on to start Schoolzilla. I’ve been hacking away ever since building data integrations and web applications to support our students, teachers, and back-office staff.\nI’m a polyglot programmer with a deep understanding of data integrations, data warehousing, and back-end APIs. I’m a decent full stack developer when it comes to enterprise web apps and work primarily in python these days.\n\n\n\nFront End: HTML5/CSS3/ES6, SASS, JQuery, Bootstrap, Material Design\nBack End: Python, Django, Flask, Ruby on Rails, NodeJS, C#, MS SQL, PostgreSQL, BigQuery, dbt\nScripting: Bash, PowerShell, Python, Ruby\nDevOps: VirtualBox, Heroku, AWS, S3, Docker, DigitalOcean, Google Cloud Platform\n\n\n\n\nI am an avid homebrewer and will wax philosophical on the merits of late boil hop additions if given the chance. But I’m also happy to enjoy some lawn-mower beer while sitting on the patio.\nI love making music and recently started publishing some of my work on Spotify under the pseudonym Mister Yeti.\nI’m also a husband and father and love nothing better than sitting around playing with my son. He’s still young, but I’m really looking forward to sharing the joy of programming with him when he’s a little older."
  },
  {
    "objectID": "about.html#former-teacher-turned-software-engineer",
    "href": "about.html#former-teacher-turned-software-engineer",
    "title": "About",
    "section": "",
    "text": "I got started, as a young teen, programming in BASIC on a used Commodore64 in the early 90s. I quickly found my way to HTML4 and built some of my first websites on GeoCities.\n\nIn college I tried my hand at Computer Science, but was more and more attracted to the Social Sciences and eventually ended up teaching high school Social Studies.\nI soon needed better tools to support my students and started building out some simple web apps in PHP on the backend of a Wordpress blog to help me teach in the days before blended learning and EdTech were ubiquitous in the education sector.\nAfter a few years of building my skills, I eventually found myself working for Aspire Public Schools on the Godzilla team alongside the folks who went on to start Schoolzilla. I’ve been hacking away ever since building data integrations and web applications to support our students, teachers, and back-office staff.\nI’m a polyglot programmer with a deep understanding of data integrations, data warehousing, and back-end APIs. I’m a decent full stack developer when it comes to enterprise web apps and work primarily in python these days.\n\n\n\nFront End: HTML5/CSS3/ES6, SASS, JQuery, Bootstrap, Material Design\nBack End: Python, Django, Flask, Ruby on Rails, NodeJS, C#, MS SQL, PostgreSQL, BigQuery, dbt\nScripting: Bash, PowerShell, Python, Ruby\nDevOps: VirtualBox, Heroku, AWS, S3, Docker, DigitalOcean, Google Cloud Platform\n\n\n\n\nI am an avid homebrewer and will wax philosophical on the merits of late boil hop additions if given the chance. But I’m also happy to enjoy some lawn-mower beer while sitting on the patio.\nI love making music and recently started publishing some of my work on Spotify under the pseudonym Mister Yeti.\nI’m also a husband and father and love nothing better than sitting around playing with my son. He’s still young, but I’m really looking forward to sharing the joy of programming with him when he’s a little older."
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html",
    "href": "posts/pandas-yaml-config/index.html",
    "title": "Configuring Pandas with Yaml Files",
    "section": "",
    "text": "Reading data from files into pandas dataframes is largely straight-forward and easy to do. Choose the right input function depending on the file type, pass in the filepath, and add a few parameters to configure how to read the data from the file.\nFor a csv file, that might look something like this:\nIf this were a tab delimited file, we could override the default delimiter param (\",\"):\nIf there were some non-data values in the footer of the file (say the last 3 rows), we could add a param to handle that as well:\nDepending on the number of different configuration needs, the list of optional parameters can get quite long which can be seen by the length of the read_csv function signature."
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html#when-this-falls-apart",
    "href": "posts/pandas-yaml-config/index.html#when-this-falls-apart",
    "title": "Configuring Pandas with Yaml Files",
    "section": "When this falls apart",
    "text": "When this falls apart\nI work with a lot of file-based tabular data for my day-to-day work. Most of it is research files from various state departments of education as well as NCES. Individually parsing each file this way would result in massive amounts of scripts each customizing the function parameters for various pandas input functions. Instead, one way to mitigate this burden is to create configuration files for each file that specifies how it should be parsed and pass those into a single script that handles interpreting the configuration."
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html#getting-an-example-file",
    "href": "posts/pandas-yaml-config/index.html#getting-an-example-file",
    "title": "Configuring Pandas with Yaml Files",
    "section": "Getting an example file",
    "text": "Getting an example file\nTo start, let’s download the 2021-2022 national directory of schools from the NCES Common Core of Data (ccd) website to use as a demonstration of how this might work. You can download the file directly here.\nOnce downloaded, we can open it in pandas:\n\nimport pandas as pd\n\ndf = pd.read_csv(\"ccd_sch_029_2122_w_1a_071722.csv\")\ndf.head()\n\n/var/folders/q8/dgbdr1_566nf3r38kwhrw0zh0000gn/T/ipykernel_25748/4258698622.py:3: DtypeWarning: Columns (14,15,21,22,39,41,42) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_data)\n\n\n\n\n\n\n\n\n\nSCHOOL_YEAR\nFIPST\nSTATENAME\nST\nSCH_NAME\nLEA_NAME\nSTATE_AGENCY_NO\nUNION\nST_LEAID\nLEAID\n...\nG_10_OFFERED\nG_11_OFFERED\nG_12_OFFERED\nG_13_OFFERED\nG_UG_OFFERED\nG_AE_OFFERED\nGSLO\nGSHI\nLEVEL\nIGOFFERED\n\n\n\n\n0\n2021-2022\n1\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n1\nNaN\nAL-101\n100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n1\n2021-2022\n1\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n1\nNaN\nAL-101\n100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n2\n2021-2022\n1\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n1\nNaN\nAL-101\n100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n3\n2021-2022\n1\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n1\nNaN\nAL-101\n100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n4\n2021-2022\n1\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAlbertville City\n1\nNaN\nAL-101\n100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\nPK\nKG\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns\n\n\n\nTwo things are apparent on first inspection:\n\na few of the columns have mixed data types we will need to handle\nthere are many columns (its a very wide file) with obscure naming conventions we will need to interpret\n\nTo get our head around the structure of this data, luckily there is a companion documentation file available from NCES as well.\nThe full list of column names is:\n\ndf.columns\n\nIndex(['SCHOOL_YEAR', 'FIPST', 'STATENAME', 'ST', 'SCH_NAME', 'LEA_NAME',\n       'STATE_AGENCY_NO', 'UNION', 'ST_LEAID', 'LEAID', 'ST_SCHID', 'NCESSCH',\n       'SCHID', 'MSTREET1', 'MSTREET2', 'MSTREET3', 'MCITY', 'MSTATE', 'MZIP',\n       'MZIP4', 'LSTREET1', 'LSTREET2', 'LSTREET3', 'LCITY', 'LSTATE', 'LZIP',\n       'LZIP4', 'PHONE', 'WEBSITE', 'SY_STATUS', 'SY_STATUS_TEXT',\n       'UPDATED_STATUS', 'UPDATED_STATUS_TEXT', 'EFFECTIVE_DATE',\n       'SCH_TYPE_TEXT', 'SCH_TYPE', 'RECON_STATUS', 'OUT_OF_STATE_FLAG',\n       'CHARTER_TEXT', 'CHARTAUTH1', 'CHARTAUTHN1', 'CHARTAUTH2',\n       'CHARTAUTHN2', 'NOGRADES', 'G_PK_OFFERED', 'G_KG_OFFERED',\n       'G_1_OFFERED', 'G_2_OFFERED', 'G_3_OFFERED', 'G_4_OFFERED',\n       'G_5_OFFERED', 'G_6_OFFERED', 'G_7_OFFERED', 'G_8_OFFERED',\n       'G_9_OFFERED', 'G_10_OFFERED', 'G_11_OFFERED', 'G_12_OFFERED',\n       'G_13_OFFERED', 'G_UG_OFFERED', 'G_AE_OFFERED', 'GSLO', 'GSHI', 'LEVEL',\n       'IGOFFERED'],\n      dtype='object')\n\n\nWe can override the data type inference that pandas is doing and specify either column specific data types or simply a default across all columns. In this case, let’s treat everything as a string. As this is a directory file, much of the data is descriptive and even values like numeric IDs that could be treated as integers will likely benefit from being handled as a string instead to preserve leading zeros that may be important for joining to other data later.\n\ndf = pd.read_csv(csv_data, dtype=str)\ndf.head()\n\n\n\n\n\n\n\n\nSCHOOL_YEAR\nFIPST\nSTATENAME\nST\nSCH_NAME\nLEA_NAME\nSTATE_AGENCY_NO\nUNION\nST_LEAID\nLEAID\n...\nG_10_OFFERED\nG_11_OFFERED\nG_12_OFFERED\nG_13_OFFERED\nG_UG_OFFERED\nG_AE_OFFERED\nGSLO\nGSHI\nLEVEL\nIGOFFERED\n\n\n\n\n0\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\nPK\nKG\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns\n\n\n\nThat has gotten rid of the Dtype warning, but we still have these column names to deal with. Let’s create a dictionary mapping the original names to what we want them to be instead.\n\ncolumn_renames = {\n    \"SCHOOL_YEAR\": \"school_year\", \n    \"FIPST\": \"state_fips_code\", \n    \"STATENAME\": \"state_name\", \n    \"ST\": \"state_abbrev\", \n    \"SCH_NAME\": \"school_name\", \n    \"LEA_NAME\": \"lea_name\",\n    \"STATE_AGENCY_NO\": \"state_agency_id\", \n    \"UNION\": \"supervisory_union_id\", \n    \"ST_LEAID\": \"state_lea_id\", \n    \"LEAID\": \"nces_lea_id\", \n    \"ST_SCHID\": \"state_school_id\", \n    \"NCESSCH\": \"nces_school_id\",\n    \"SCHID\": \"school_id\", \n    \"MSTREET1\": \"street1_mailing_address\", \n    \"MSTREET2\": \"street2_mailing_address\", \n    \"MSTREET3\": \"street3_mailing_address\", \n    \"MCITY\": \"city_mailing_address\", \n    \"MSTATE\": \"state_mailing_address\", \n    \"MZIP\": \"zipcode_mailing_address\",\n    \"MZIP4\": \"secondary_zipcode_mailing_address\", \n    \"LSTREET1\": \"street1_location\", \n    \"LSTREET2\": \"street2_location\", \n    \"LSTREET3\": \"street3_location\", \n    \"LCITY\": \"city_location\", \n    \"LSTATE\": \"state_location\", \n    \"LZIP\": \"zipcode_location\",\n    \"LZIP4\": \"secondary_zipcode_location\", \n    \"PHONE\": \"phone_number\", \n    \"WEBSITE\": \"website_url\", \n    \"SY_STATUS\": \"start_of_year_status_code\", \n    \"SY_STATUS_TEXT\": \"start_of_year_status_description\",\n    \"UPDATED_STATUS\": \"updated_status_code\", \n    \"UPDATED_STATUS_TEXT\": \"updated_status_description\", \n    \"EFFECTIVE_DATE\": \"updated_status_effective_date\",\n    \"SCH_TYPE_TEXT\": \"school_type_description\", \n    \"SCH_TYPE\": \"school_type_code\",\n    \"RECON_STATUS\": \"is_reconstituted\", \n    \"OUT_OF_STATE_FLAG\": \"is_out_of_state_location\",\n    \"CHARTER_TEXT\": \"is_charter_school\", \n    \"CHARTAUTH1\": \"charter_authorizer_id1\", \n    \"CHARTAUTHN1\": \"charter_authorizer_name1\", \n    \"CHARTAUTH2\": \"charter_authorizer_id2\",\n    \"CHARTAUTHN2\": \"charter_authorizer_name2\", \n    \"NOGRADES\": \"no_grades_offered\", \n    \"G_PK_OFFERED\": \"grade_pk_offered\", \n    \"G_KG_OFFERED\": \"grade_k_offered\",\n    \"G_1_OFFERED\": \"grade_1_offered\", \n    \"G_2_OFFERED\": \"grade_2_offered\", \n    \"G_3_OFFERED\": \"grade_3_offered\", \n    \"G_4_OFFERED\": \"grade_4_offered\",\n    \"G_5_OFFERED\": \"grade_5_offered\", \n    \"G_6_OFFERED\": \"grade_6_offered\", \n    \"G_7_OFFERED\": \"grade_7_offered\", \n    \"G_8_OFFERED\": \"grade_8_offered\",\n    \"G_9_OFFERED\": \"grade_9_offered\", \n    \"G_10_OFFERED\": \"grade_10_offered\", \n    \"G_11_OFFERED\": \"grade_11_offered\", \n    \"G_12_OFFERED\": \"grade_12_offered\",\n    \"G_13_OFFERED\": \"grade_13_offered\", \n    \"G_UG_OFFERED\": \"ungraded_offered\", \n    \"G_AE_OFFERED\": \"adult_education_offered\", \n    \"GSLO\": \"lowest_grade_offered\", \n    \"GSHI\": \"highest_grade_offered\", \n    \"LEVEL\": \"school_level\",\n    \"IGOFFERED\": \"any_grades_offered_field_adjusted\",\n}\n\nA bit tedious, but at this point we could rename our columns and be done.\n\ndf.rename(columns=column_renames, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nschool_year\nstate_fips_code\nstate_name\nstate_abbrev\nschool_name\nlea_name\nstate_agency_id\nsupervisory_union_id\nstate_lea_id\nnces_lea_id\n...\ngrade_10_offered\ngrade_11_offered\ngrade_12_offered\ngrade_13_offered\nungraded_offered\nadult_education_offered\nlowest_grade_offered\nhighest_grade_offered\nschool_level\nany_grades_offered_field_adjusted\n\n\n\n\n0\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\nPK\nKG\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns\n\n\n\nSo why the need for a special yaml config file?\nWell, in this singular case it’s not necessary at all. If this was the one file we needed for analysis, then yes we could stop here. But, imagine that instead you had hundreds if not thousands of similar files with different file types (csv, excel, txt, etc) each with its own unique set of challenges such as footers or headers to skip, tabs to combine, and columns to rename. At that point, writing a separate python script for each file and running them as needed would become unwieldy rather quickly."
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html#introducing-yaml-jsons-cuter-cousin",
    "href": "posts/pandas-yaml-config/index.html#introducing-yaml-jsons-cuter-cousin",
    "title": "Configuring Pandas with Yaml Files",
    "section": "Introducing Yaml: Json’s cuter cousin",
    "text": "Introducing Yaml: Json’s cuter cousin\nOur column_map dictionary looks a lot like a json object and likely we could specify our configurations as a series of json files or even python files with dictionaries. But this syntax is full of unnecessary punctuation.\nCompare this list in python/json with a similar one in yaml:\nPython or Json:\ncolumns = [\n    \"school_year\", \n    \"state_fips_code\", \n    \"state_name\", \n    \"state_abbrev\", \n    \"school_name\", \n    \"lea_name\",\n    \"state_agency_id\", \n    \"supervisory_union_id\", \n    \"state_lea_id\", \n    \"nces_lea_id\", \n    \"state_school_id\", \n    \"nces_school_id\",\n    \"school_id\", \n]\nYaml:\ncolumns:\n  - school_year \n  - state_fips_code \n  - state_name \n  - state_abbrev \n  - school_name \n  - lea_name\n  - state_agency_id \n  - supervisory_union_id \n  - state_lea_id \n  - nces_lea_id \n  - state_school_id \n  - nces_school_id\n  - school_id\nAdmittedly, not a HUGE difference but one is clearly a bit more human readable. The more complex the configuration gets the more this becomes obvious. The trade off is that a native python dictionary or even a json string could be parsed using only the python standard lib. Currently, yaml support is not baked into python. Thankfully, there is a simple package that can be used: pyyaml.\nWe’ll install that with:\npip install pyyaml\nor if you’re like me and prefer pipenv:\npipenv install pyyaml\nThen let’s create a yaml config file for our nces directory file.\nname: nces_school_directory_2022\nfilepath_or_buffer: ccd_sch_029_2122_w_1a_071722.csv\nfile_type: csv\ndtype: str\ncolumns:\n  - school_year \n  - state_fips_code \n  - state_name \n  - state_abbrev \n  - school_name \n  - lea_name\n  - state_agency_id \n  - supervisory_union_id \n  - state_lea_id \n  - nces_lea_id \n  - state_school_id \n  - nces_school_id\n  - school_id \n  - street1_mailing_address \n  - street2_mailing_address \n  - street3_mailing_address \n  - city_mailing_address \n  - state_mailing_address \n  - zipcode_mailing_address\n  - secondary_zipcode_mailing_address \n  - street1_location \n  - street2_location \n  - street3_location \n  - city_location \n  - state_location \n  - zipcode_location\n  - secondary_zipcode_location \n  - phone_number \n  - website_url \n  - start_of_year_status_code \n  - start_of_year_status_description\n  - updated_status_code \n  - updated_status_description \n  - updated_status_effective_date\n  - school_type_description \n  - school_type_code\n  - is_reconstituted \n  - is_out_of_state_location\n  - is_charter_school \n  - charter_authorizer_id1 \n  - charter_authorizer_name1 \n  - charter_authorizer_id2\n  - charter_authorizer_name2 \n  - no_grades_offered \n  - grade_pk_offered \n  - grade_k_offered\n  - grade_1_offered \n  - grade_2_offered \n  - grade_3_offered \n  - grade_4_offered\n  - grade_5_offered \n  - grade_6_offered \n  - grade_7_offered \n  - grade_8_offered\n  - grade_9_offered \n  - grade_10_offered \n  - grade_11_offered \n  - grade_12_offered\n  - grade_13_offered \n  - ungraded_offered \n  - adult_education_offered \n  - lowest_grade_offered \n  - highest_grade_offered \n  - school_level\n  - any_grades_offered_field_adjusted"
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html#reading-yaml",
    "href": "posts/pandas-yaml-config/index.html#reading-yaml",
    "title": "Configuring Pandas with Yaml Files",
    "section": "Reading Yaml",
    "text": "Reading Yaml\nTo start, we’ll need to load the data from this yaml file into something Python can work with:\n\nfrom yaml import safe_load\n\nwith open(\"nces_school_directory_2022.yml\") as f:\n    config = safe_load(f)\n\nYou can see that this has converted it to a native python dictionary. Less for us to write, easier to read, but still as functional as writing it as a dictionary to begin with.\n\nconfig\n\n{'name': 'nces_school_directory_2022',\n 'filepath_or_buffer': 'ccd_sch_029_2122_w_1a_071722.csv',\n 'file_type': 'csv',\n 'dtype': 'str',\n 'columns': ['school_year',\n  'state_fips_code',\n  'state_name',\n  'state_abbrev',\n  'school_name',\n  'lea_name',\n  'state_agency_id',\n  'supervisory_union_id',\n  'state_lea_id',\n  'nces_lea_id',\n  'state_school_id',\n  'nces_school_id',\n  'school_id',\n  'street1_mailing_address',\n  'street2_mailing_address',\n  'street3_mailing_address',\n  'city_mailing_address',\n  'state_mailing_address',\n  'zipcode_mailing_address',\n  'secondary_zipcode_mailing_address',\n  'street1_location',\n  'street2_location',\n  'street3_location',\n  'city_location',\n  'state_location',\n  'zipcode_location',\n  'secondary_zipcode_location',\n  'phone_number',\n  'website_url',\n  'start_of_year_status_code',\n  'start_of_year_status_description',\n  'updated_status_code',\n  'updated_status_description',\n  'updated_status_effective_date',\n  'school_type_description',\n  'school_type_code',\n  'is_reconstituted',\n  'is_out_of_state_location',\n  'is_charter_school',\n  'charter_authorizer_id1',\n  'charter_authorizer_name1',\n  'charter_authorizer_id2',\n  'charter_authorizer_name2',\n  'no_grades_offered',\n  'grade_pk_offered',\n  'grade_k_offered',\n  'grade_1_offered',\n  'grade_2_offered',\n  'grade_3_offered',\n  'grade_4_offered',\n  'grade_5_offered',\n  'grade_6_offered',\n  'grade_7_offered',\n  'grade_8_offered',\n  'grade_9_offered',\n  'grade_10_offered',\n  'grade_11_offered',\n  'grade_12_offered',\n  'grade_13_offered',\n  'ungraded_offered',\n  'adult_education_offered',\n  'lowest_grade_offered',\n  'highest_grade_offered',\n  'school_level',\n  'any_grades_offered_field_adjusted']}\n\n\nTo use this dictionary as the parameters for the pandas read_csv function, we can take advantage of the unpacking operator ** to convert our dictionary into a set of keyword arguments (often referred to as kwargs).\nYou can learn more about args and kwargs from this helpful Real Python article.\n\ndf = pd.read_csv(**config)\ndf.head()\n\nTypeError: read_csv() got an unexpected keyword argument 'name'"
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html#but-wait-theres-more",
    "href": "posts/pandas-yaml-config/index.html#but-wait-theres-more",
    "title": "Configuring Pandas with Yaml Files",
    "section": "But wait, there’s more",
    "text": "But wait, there’s more\nWait! What’s this? One of the values in our yaml config isn’t an actual parameter/keyword for the read_csv function. While this adds some additional overhead to our usage of yaml, it provides some useful functionality. We can specify additional variables in the config that can drive behavior in our script beyond just the function parameters.\nFor instance, the file_type variable could be used to swap between use of read_csv when file_type: csv is specified or read_excel when file_type: excel is provided instead. Or the name variable could be used to determine the table name in our database where this data will be loaded.\nIn order to distinguish these extra variables from our function parameters, we need to be aware of which keywords are specific to the function. We could specify these explicitly OR we could infer them from the function signature.\n\nimport inspect\n\nparams = inspect.signature(pd.read_csv).parameters.keys()\n\n\nparams\n\nodict_keys(['filepath_or_buffer', 'sep', 'delimiter', 'header', 'names', 'index_col', 'usecols', 'dtype', 'engine', 'converters', 'true_values', 'false_values', 'skipinitialspace', 'skiprows', 'skipfooter', 'nrows', 'na_values', 'keep_default_na', 'na_filter', 'verbose', 'skip_blank_lines', 'parse_dates', 'infer_datetime_format', 'keep_date_col', 'date_parser', 'date_format', 'dayfirst', 'cache_dates', 'iterator', 'chunksize', 'compression', 'thousands', 'decimal', 'lineterminator', 'quotechar', 'quoting', 'doublequote', 'escapechar', 'comment', 'encoding', 'encoding_errors', 'dialect', 'on_bad_lines', 'delim_whitespace', 'low_memory', 'memory_map', 'float_precision', 'storage_options', 'dtype_backend'])\n\n\nNow we can compare the key/value pairs in our yaml config with any that match the function signature of pd.read_csv.\n\ndef get_kwargs(config):\n    params = inspect.signature(pd.read_csv).parameters.keys()\n    return {k: v for k, v in config.items() if k in params}\n\nThis will create a new dictionary with just the key/value pairs (k: v) that match the function signature. At this point, we can now pass those into our input function.\n\ndf = pd.read_csv(**get_kwargs(config))\ndf.head()\n\n\n\n\n\n\n\n\nSCHOOL_YEAR\nFIPST\nSTATENAME\nST\nSCH_NAME\nLEA_NAME\nSTATE_AGENCY_NO\nUNION\nST_LEAID\nLEAID\n...\nG_10_OFFERED\nG_11_OFFERED\nG_12_OFFERED\nG_13_OFFERED\nG_UG_OFFERED\nG_AE_OFFERED\nGSLO\nGSHI\nLEVEL\nIGOFFERED\n\n\n\n\n0\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Kindergarten and PreK\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\nPK\nKG\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns\n\n\n\nVery close. This hasn’t handled the column renaming. That’s because in pd.read_csv the column name values is set with the names param. We could just call our columns list names in the config or we could pass it explicitly instead (if, say, we needed to name it columns for some other functionality in our script).\n\ndf = pd.read_csv(**get_kwargs(config), names=config[\"columns\"])\ndf.head()\n\n\n\n\n\n\n\n\nschool_year\nstate_fips_code\nstate_name\nstate_abbrev\nschool_name\nlea_name\nstate_agency_id\nsupervisory_union_id\nstate_lea_id\nnces_lea_id\n...\ngrade_10_offered\ngrade_11_offered\ngrade_12_offered\ngrade_13_offered\nungraded_offered\nadult_education_offered\nlowest_grade_offered\nhighest_grade_offered\nschool_level\nany_grades_offered_field_adjusted\n\n\n\n\n0\nSCHOOL_YEAR\nFIPST\nSTATENAME\nST\nSCH_NAME\nLEA_NAME\nSTATE_AGENCY_NO\nUNION\nST_LEAID\nLEAID\n...\nG_10_OFFERED\nG_11_OFFERED\nG_12_OFFERED\nG_13_OFFERED\nG_UG_OFFERED\nG_AE_OFFERED\nGSLO\nGSHI\nLEVEL\nIGOFFERED\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns"
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html#icing-on-the-cake",
    "href": "posts/pandas-yaml-config/index.html#icing-on-the-cake",
    "title": "Configuring Pandas with Yaml Files",
    "section": "Icing on the cake",
    "text": "Icing on the cake\nI like to simplify all of this by creating a custom configuration class to handle this. That enables us to use class attributes for accessing the key/values and can wrap in the yaml reading as well.\n\nclass Config:\n    def __init__(self, yaml_file_path):\n        \"\"\"\n        Extract configuration from yaml file and set the values as \n        class attributes or kwargs based on the specified function\n        signature.\n        \"\"\"\n        with open(yaml_file_path) as f:\n            self.yaml = safe_load(f)\n        self.set_attrs()\n    \n    def set_attrs(self):\n        \"\"\"\n        Create attribute called kwargs that returns dictionary of all key/value\n        pairs that match the given function signature params.\n\n        Set any non-matching key as a class attribute.\n        \"\"\"\n        params = self.get_signature()\n        for k, v in self.yaml.items():\n            if k not in params:\n                setattr(self, k, v)\n        self.kwargs = {k: v for k, v in self.yaml.items() if k in params}\n\n    def get_signature(self):\n        \"\"\"\n        Select the corresponding function signature based on the specified\n        file_type of either csv or excel.\n\n        Note: this could be expanded to handle more file_types, such as:\n          - SPSS\n          - JSON\n          - Parquet\n          - XML\n        \"\"\"\n        signatures = {\n            \"csv\": inspect.signature(pd.read_csv).parameters.keys(),\n            \"excel\": inspect.signature(pd.read_csv).parameters.keys(),\n        }\n        return signatures.get(self.yaml[\"file_type\"])\n\nWith this class we can now create a Config object from a yaml file and pass that to our pandas functions. It also now handles for whether the file_type expects to use read_csv or read_excel.\n\nconfig = Config(\"nces_school_directory_2022.yml\")\ndf = pd.read_csv(**config.kwargs, names=config.columns)\ndf.head()\n\n\n\n\n\n\n\n\nschool_year\nstate_fips_code\nstate_name\nstate_abbrev\nschool_name\nlea_name\nstate_agency_id\nsupervisory_union_id\nstate_lea_id\nnces_lea_id\n...\ngrade_10_offered\ngrade_11_offered\ngrade_12_offered\ngrade_13_offered\nungraded_offered\nadult_education_offered\nlowest_grade_offered\nhighest_grade_offered\nschool_level\nany_grades_offered_field_adjusted\n\n\n\n\n0\nSCHOOL_YEAR\nFIPST\nSTATENAME\nST\nSCH_NAME\nLEA_NAME\nSTATE_AGENCY_NO\nUNION\nST_LEAID\nLEAID\n...\nG_10_OFFERED\nG_11_OFFERED\nG_12_OFFERED\nG_13_OFFERED\nG_UG_OFFERED\nG_AE_OFFERED\nGSLO\nGSHI\nLEVEL\nIGOFFERED\n\n\n1\n2021-2022\n01\nALABAMA\nAL\nAlbertville Middle School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n07\n08\nMiddle\nAs reported\n\n\n2\n2021-2022\n01\nALABAMA\nAL\nAlbertville High School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nYes\nYes\nYes\nNo\nNo\nNo\n09\n12\nHigh\nAs reported\n\n\n3\n2021-2022\n01\nALABAMA\nAL\nAlbertville Intermediate School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n05\n06\nMiddle\nAs reported\n\n\n4\n2021-2022\n01\nALABAMA\nAL\nAlbertville Elementary School\nAlbertville City\n01\nNaN\nAL-101\n0100005\n...\nNo\nNo\nNo\nNo\nNo\nNo\n03\n04\nElementary\nAs reported\n\n\n\n\n5 rows × 65 columns"
  },
  {
    "objectID": "posts/pandas-yaml-config/index.html#conclusion",
    "href": "posts/pandas-yaml-config/index.html#conclusion",
    "title": "Configuring Pandas with Yaml Files",
    "section": "Conclusion",
    "text": "Conclusion\nThere is a lot more parsing functionality that could be extended from this basic example using more configuration variables and/or expanding to different file types.\nWe’ve used a variation of this approach to great effect to parse hundreds of research files from most of the states in the U.S. as well as a variety of other public research data like NCES. This has even been used to parse Google Spreadsheet data directly in memory.\nWith this data now transformed into a pandas dataframe, it can be easily loaded into a data warehouse or analyzed in memory."
  },
  {
    "objectID": "posts/python-training/index.html",
    "href": "posts/python-training/index.html",
    "title": "I Can Python, So Can You",
    "section": "",
    "text": "Before I left Aspire, I had started an internal training on Python for those in the org who were interested in learning to do a bit of coding. Inspired by the tag line from an old favorite cooking show from the 1980s, Yan Can Cook, I titled the course “I Can Python, So Can You”.\nThe scope of that original course was heaviliy inspired by Al Swiegart’s Automate The Boring Stuff With Python and was focused on building skills around office task automation. My colleague, and good friend, Jason Baek helped me design the course and content and we co-taught it during our time there.\nWhen I left Aspire and began work at KIPP Northern California, there was once again an interest from colleagues to learn a little python. So I updated the course, added some additional homework and projects, and taught it once again. I learned a lot that time around and got some great feedback from my data team about the skills they were most interested in (such as learning pandas, APIs, and data visualization).\nIn the summer of 2019, I was asked by the KIPP Foundation to design a single day version of the course for a data convening in Chicago. I recruited my data engineering colleague, Charlie Bini, to help me build out a streamlined version of the content and to co-teach with me at the convening. We reworked the content to incorporate a more data-focused curriculum and taught it over the course of a six-hour session at the convening. We repeated the course with further updates the following year, although we did it remotely as we were in the midst of the pandemic.\nOne of the core ideas in all of the incarnations was that the environment setup and IDE learning curve would be a roadblock for many newbies and we didn’t want that to stand in the way of jumping in and learning to code in python. So we reached for Google Colab, which had the ease of Jupyter Notebooks but with an in-browser, cloud hosted environment, and was shareable like any other Google Doc.\nI’m hoping to write a new version of these lessons soon, but in the meantime I wanted to get them up on my blog as a reference for folks who ask about learning resources and to hopefully cultivate some additional feedback that I can incorporate in the next iteration.\nWithout further ado, here are the links to the colab notebooks. I hope they are useful to anyone who finds this post.\n\nI Can Python, So Can You\n\nIntroduction\nFundamentals\nControl Flow\nLoops & Lists\nRecap & APIs\nPandas\nCourse Capstone: Star Wars"
  },
  {
    "objectID": "posts/powerschool-api.md/index.html#project-summary",
    "href": "posts/powerschool-api.md/index.html#project-summary",
    "title": "Student Email Sync",
    "section": "Project Summary:",
    "text": "Project Summary:\n\nBusiness Objective:\nTo pull automatically generated student email addresses from our MS SQL data warehouse and push into production PowerSchool instances in multiple states.\n\n\nWhat I Built:\nA ruby script that connects to our data warehouse using an ActiveRecord adapter for SQL server, dynamically queries student records, and converts them into the expected format with a custom JSON serializer. Then it uses Faraday HTTP client to make a call to the PowerSchool API (one per instance in CA and TN) to request a token for authentication. Once authenticated, it sends the JSON request body to the API and returns a response set of success/error messages which are logged locally. The program is integrated into our SSIS integration job server using a nice little PowerShell script to dynamically call the ruby script for each state’s data.\nTL;DR: Wrote a ruby script that syncs 15,000+ student emails between a data warehouse and production PowerSchool servers in mutliple states on a daily basis.\n\n\nMade With:\n\nRuby\n\nFaraday\nRails-sqlserver adapter\nLogger\n\nPowerSchool API\nMS SQL\n\nStored procedure\nCustom views\n\nPowerShell\nSSIS\n\n\n\nCode Samples:\nSelect methods from the API class demonstrating how the program connects to the PowerSchool API and updates student emails.\n# @oauth is a class variable that stores the configuration file secrets\n# not exposed here for obvious reasons\n\nprivate\n  def connect\n    # open HTTP connection\n    Faraday.new(:url =&gt; @oauth['url'],:ssl =&gt; {:verify =&gt; false}) do |faraday|\n      faraday.request  :url_encoded               # form-encode POST params\n      faraday.adapter  Faraday.default_adapter    # make requests with Net::HTTP\n    end\n  end\n\nprivate\n  def access_token\n    result = connect.post do |request|\n      request.url \"/oauth/access_token\"\n      request.params['client_id'] = @oauth['id']\n      request.params['client_secret'] = @oauth['secret']\n      request.params['grant_type'] = 'client_credentials'\n    end\n    \n    # return token from JSON result\n    JSON.parse(result.body)['access_token']\n  end\n\npublic\n  def update_student_emails(body) # takes JSON as the body param\n    # POST request body of student emails to PowerSchool API\n    result = connect.post do |request|\n      request.url \"/ws/v1/student\"\n      request.options.timeout = 3000 # open/read timeout in seconds\n      request.options.open_timeout = 10 # connection open timeout in seconds\n      request.headers['Accept'] = 'application/json'\n      request.headers['Content-Type'] = 'application/json'\n      request.headers['Authorization'] = \"Bearer #{access_token}\"\n      request.body = body\n    end\n\n    # Return human readable JSON result\n    parsed = JSON.parse(result.body)\n    pretty = JSON.pretty_generate(parsed)\n  end\nRuby method for seralizing student emails from SQL to JSON. The PowerSchool Student API expects JSON in a non standard JSON format. That is why custom serialization is needed.\nSee PowerSchool API Documentation under “Update Students &gt; Request Body Example”\ndef self.serialize_student_emails(state)\n  # client_uid is the PowerSchool ID and id is the DCID\n  studentArray = []\n  connect('production')\n  query = sanitize_sql_array(\n    [\"SELECT * FROM dbo.StudentEmails WHERE State = ?\", state]\n  )\n  connection.select_all(query).each do |record|\n    studentArray &lt;&lt; {\n      'client_uid' =&gt; record['client_uid'], \n      'action' =&gt; 'UPDATE', \n      'id' =&gt; record['id'], \n      'contact_info' =&gt; {'email': record['email']}\n    }\n  end\n\n  studentHash = {'students' =&gt; {'student' =&gt; studentArray}}\n  \n  # return as JSON body\n  studentHash.to_json\nend\nPowerShell script: ssms_job.ps1\n# This is run from an SSMS job task step\n$erroractionpreference = \"Stop\"\n\ntry {\n  Set-Location 'C:\\powerSchoolApi\\app'\n  \n  # Ruby executable location called from Rails Installer\n  $RUBY = 'C:\\RailsInstaller\\Ruby2.2.0\\bin\\ruby.exe'\n  & $RUBY 'application.rb'\n}\ncatch {\n  throw 'The application encountered an error. See logs for further explanation.'\n  exit 1\n}"
  },
  {
    "objectID": "posts/quarto-blog/index.html",
    "href": "posts/quarto-blog/index.html",
    "title": "Goodbye Pelican, Hello Quarto",
    "section": "",
    "text": "Where it All Started\nSeven years ago, I first published a Jekyll blog hosted in Github pages and let it lie largely dormant for two years. Then, as a New Year’s resolution, I rebuilt the entire thing using Pelican so I could stop using Ruby and, finally, fully transition all my personal code projects into Python.\nIt’s been five years since that blog post, and in that time I’ve published only 13 post on this blog. That’s less than three posts a year.\nThat’s kind of sad.\n\n\nStarting Over\nIn the years since I began on that resolution to write more and educate on Python, I found myself struggling to generate new content. Not for a lack of ideas, but from a combination of excuse making (classic procrastination) and technical barriers that made publishing content less than streamlined.\nThe way the Pelican based blog was setup required some complexity with a multi-repo setup. One for the source markdown files and one for the html output.\nThis required the use of git submodules and memorization of Pelican commands to generate the content. I forgot them often enough that I ended up using the README of the source repo to add notes for myself so I wouldn’t forget how to publish.\nSoon, I was looking to write more content that included not just blocks of example code but also their output and Python notebooks seemed like a good way to go about that. However, integrating them into Pelican became a journey all of it’s own. I stumbled across the pelican-jupyter library and first things were great. Suddenly I could mix markdown and python code and publish to my blog, but the fun stopped soon enough. The package depended on nbconvert to convert the notebooks into the html needed for publishing but a version upgrade broke the dependencies and soon even pinning the older version no longer solved all the conflicts; as other packages quickly required newer versions while the package was unable to keep up.\nI started looking for a better way to publish python notebooks as a blog and stumbled across Quarto. After reading through the docs, I was blown away. How long had this been around? This was everything I needed and more.\n\n\nBetter Than Ever\nBefore long I had a proof of concept stood up in parallel and was able to not only publish without any archaic commands but I could render previews locally in VS Code, eliminate the need for a second repo, and the themes and layouts were based on simple bootstrap. The current look and feel is a bit more basic, but the focus is more on the content and there is even a built in search tool.\nEven better, they had explicit directions for publishing to Github Pages using Github Actions. This made it easier than ever to write content and publish withough worrying about the static site generation. In fact, I could even write markdown directly in Github (in the browser) and commit to the repo and the publish action would immediately deploy to my site! So cool.\nSuddenly, Github was my personal blog CMS without any need for a database or admin site. Just simple git commit.\nNow I have one less excuse to start writing more and much more functionality to play around with. I’m very excited about exploring more interactive data viz using Observable and the ability to switch between R and Python as I collaborate with some of my colleague who prefer to code in R, like Chris Haid."
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html",
    "href": "posts/json-normalize/json-normalize.html",
    "title": "Normalize JSON with Pandas",
    "section": "",
    "text": "When processing nested JSON data into a flat structure for importing into a relational database, it can be tricky to structure the data into the right shape. Pandas has a great tool for doing this called pandas.json_normalize() but the documentation doesn’t make it obvious how to leverage its capabilities for handling nested data structures.\nI thought I could provide a brief example using some randomly generated survey response data (using the Faker library) to illustrate it’s advantages."
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#setup",
    "href": "posts/json-normalize/json-normalize.html#setup",
    "title": "Normalize JSON with Pandas",
    "section": "Setup",
    "text": "Setup\nTo start, I’m going to be using pandas and Faker so we’ll import those. I’m also going to need to easily display the parsed json as well as the returned dataframes, so I’m importing the json module from the standard lib as well as some IPython notebook helpers for displaying dataframes as HTML tables.\n\nimport json\nfrom IPython.display import display, HTML\nfrom faker import Faker\nfrom faker.providers import BaseProvider, date_time, internet\nimport pandas as pd"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#fake-data",
    "href": "posts/json-normalize/json-normalize.html#fake-data",
    "title": "Normalize JSON with Pandas",
    "section": "Fake Data",
    "text": "Fake Data\nFaker doesn’t have a built in provider for survey questions, so let’s go ahead and add a simple one that creates non-sensical questions with a simple hack to the sentence provider.\n\nclass MyProvider(BaseProvider):\n    def question(self):\n        stems = ('Does', 'How does', 'Which', 'Why does')\n        stem = Faker().random_choices(elements=stems, length=1)[0]\n        sentence = Faker().sentence()\n        sentence = sentence[0].lower() + sentence[1:]\n        question = sentence.replace(\".\", \"?\")\n        question = f\"{stem} {question}\"\n        return question\n\nUsing this new question provider, we’ll construct a few records of fake survey response data with some respondent level data like a respondent_id, survey_date, and respondent email. Within that we’ll nest a list of responses which will in turn have it’s own dictionary of data at the question level: id, question text, and choices. The choices list will be singular here, but assume it has that structure because the API this comes from has to also account for multi-select options and we’ll need to parse it as a list regardless. Depending on our analysis needs, this might also be a place where we’d want to keep these in a comma separated string, but for our purposes here we’ll ignore that use case.\n\nfake = Faker()\nfake.add_provider(MyProvider)\nchoices = ('Strongly Agree', 'Agree', 'Neutral', 'Disagree', 'Strongly Disagree')\n\n\nsample_data = [\n    {\n        \"respondent_id\": fake.bothify(text=\"#?##??###?#\"),\n        \"survey_date\": fake.date(),\n        \"email\": fake.email(domain=\"example.com\"),\n        \"responses\": [\n            {\n                \"question_id\": fake.bothify(text=\"#??#??###?#\"),\n                \"question_text\": fake.question(),\n                \"choices\": [\n                    {\n                        \"choice\": fake.random_choices(elements=choices, length=1)[0],\n                        \"number\": fake.random_digit(),\n                    }\n                ]\n            } for _ in range(5)\n        ]\n    } for _ in range(2) \n]\n\nLet’s print out a single record to see the resulting data structure that has been generated randomly.\n\nprint(json.dumps(sample_data[0], indent=2))\n\n{\n  \"respondent_id\": \"1W38Sn628N9\",\n  \"survey_date\": \"1985-09-30\",\n  \"email\": \"ofrench@example.com\",\n  \"responses\": [\n    {\n      \"question_id\": \"9lQ8OH810H2\",\n      \"question_text\": \"How does mean action onto south usually prepare that?\",\n      \"choices\": [\n        {\n          \"choice\": \"Strongly Disagree\",\n          \"number\": 9\n        }\n      ]\n    },\n    {\n      \"question_id\": \"8YZ3Ri208p1\",\n      \"question_text\": \"Does million mean tax foot statement?\",\n      \"choices\": [\n        {\n          \"choice\": \"Strongly Disagree\",\n          \"number\": 4\n        }\n      ]\n    },\n    {\n      \"question_id\": \"2Ys2yn819r0\",\n      \"question_text\": \"How does decade what air scientist defense allow entire?\",\n      \"choices\": [\n        {\n          \"choice\": \"Disagree\",\n          \"number\": 2\n        }\n      ]\n    },\n    {\n      \"question_id\": \"3ii1tu719B3\",\n      \"question_text\": \"Does focus statement peace forward do relate?\",\n      \"choices\": [\n        {\n          \"choice\": \"Neutral\",\n          \"number\": 6\n        }\n      ]\n    },\n    {\n      \"question_id\": \"9xv2JX456C7\",\n      \"question_text\": \"Does explain ability plant?\",\n      \"choices\": [\n        {\n          \"choice\": \"Strongly Disagree\",\n          \"number\": 5\n        }\n      ]\n    }\n  ]\n}"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#reading-with-pandas",
    "href": "posts/json-normalize/json-normalize.html#reading-with-pandas",
    "title": "Normalize JSON with Pandas",
    "section": "Reading with Pandas",
    "text": "Reading with Pandas\nAs you can see below, simply reading this directly into a dataframe only parses the top level respondent data, but then keeps the responses data as a json array. Which isn’t great for simple analysis. Could you load that “as is” into a jsonb field in PostgreSQL? Sure. If you like parsing json with SQL. Yuck!\n\ndf = pd.DataFrame(sample_data)\ndf\n\n\n\n\n\n\n\n\nrespondent_id\nsurvey_date\nemail\nresponses\n\n\n\n\n0\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n[{'question_id': '9lQ8OH810H2', 'question_text...\n\n\n1\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n[{'question_id': '3sG5am762C6', 'question_text..."
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#a-messy-custom-parser",
    "href": "posts/json-normalize/json-normalize.html#a-messy-custom-parser",
    "title": "Normalize JSON with Pandas",
    "section": "A messy custom parser",
    "text": "A messy custom parser\nWe could attempt to reshape this by writing some custom functions to handle extracting the responses and merging that data with the top-level meta data about the respondent with some dictionary unpacking, but this gets messy and would fall apart quickly as the structure changed. While this approach works, it’s not ideal.\nSure this code could be further refactored to simplify the logic, but it’s not worth it since the pandas.json_normalize() can do this for us easily.\n\ndef record_format(responses):\n    data = []\n    for response in responses:\n        record = {\n            \"question_id\": response.get(\"question_id\"),\n            \"question_text\": response.get(\"question_text\"),\n            \"choice\": response.get(\"choices\")[0].get(\"choice\"),\n            \"number\": response.get(\"choices\")[0].get(\"number\"),\n        }\n        data.append(record)\n    return data\n\ndef parse_json(records):\n    data = []\n    for record in sample_data:\n        meta = {\n            \"respondent_id\": record.get(\"respondent_id\"),\n            \"survey_date\": record.get(\"survey_date\"),\n            \"email\": record.get(\"email\"),\n        }\n        responses = record.get(\"responses\")\n        formatted_responses = record_format(responses)\n        for response in formatted_responses:\n            combined = {**meta, **response}\n            data.append(combined)\n    return data\n\nclean_data = parse_json(sample_data)\ndf = pd.DataFrame(clean_data)\ndf\n\n\n\n\n\n\n\n\nrespondent_id\nsurvey_date\nemail\nquestion_id\nquestion_text\nchoice\nnumber\n\n\n\n\n0\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9lQ8OH810H2\nHow does mean action onto south usually prepar...\nStrongly Disagree\n9\n\n\n1\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n8YZ3Ri208p1\nDoes million mean tax foot statement?\nStrongly Disagree\n4\n\n\n2\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n2Ys2yn819r0\nHow does decade what air scientist defense all...\nDisagree\n2\n\n\n3\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n3ii1tu719B3\nDoes focus statement peace forward do relate?\nNeutral\n6\n\n\n4\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9xv2JX456C7\nDoes explain ability plant?\nStrongly Disagree\n5\n\n\n5\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3sG5am762C6\nWhy does know writer ball bad whole?\nAgree\n3\n\n\n6\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n5uM6LP013p7\nWhy does break chance boy enjoy call paper yet?\nStrongly Disagree\n9\n\n\n7\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3NE9pu846z1\nDoes note spring newspaper and that thing?\nNeutral\n7\n\n\n8\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n6ge7Zt058d3\nWhy does message next eat the stay?\nNeutral\n0\n\n\n9\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n2EB3vo658l2\nHow does no later then inside fill discover?\nAgree\n4"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#json-normalize",
    "href": "posts/json-normalize/json-normalize.html#json-normalize",
    "title": "Normalize JSON with Pandas",
    "section": "JSON Normalize",
    "text": "JSON Normalize\nThankfully there is the json_normalize() function, but it requires a little understanding to get it to satisfactorily parse flat. Simply passing it the sample data without any parameters results in a very familiar result that gets us no further than we started in the first attempt.\n\ndf = pd.json_normalize(sample_data)\ndf\n\n\n\n\n\n\n\n\nrespondent_id\nsurvey_date\nemail\nresponses\n\n\n\n\n0\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n[{'question_id': '9lQ8OH810H2', 'question_text...\n\n\n1\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n[{'question_id': '3sG5am762C6', 'question_text...\n\n\n\n\n\n\n\nA few optional parameters can be used here to parse the first nested array called responses. We can direct the pandas json parser to a specific key as the source of records. The record_path parameter takes either a string or list of strings to construct that path. The name of this parameter is a hint about how to think of this when passed as a list as we’ll see later.\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=\"responses\", \n)\ndf\n\n\n\n\n\n\n\n\nquestion_id\nquestion_text\nchoices\n\n\n\n\n0\n9lQ8OH810H2\nHow does mean action onto south usually prepar...\n[{'choice': 'Strongly Disagree', 'number': 9}]\n\n\n1\n8YZ3Ri208p1\nDoes million mean tax foot statement?\n[{'choice': 'Strongly Disagree', 'number': 4}]\n\n\n2\n2Ys2yn819r0\nHow does decade what air scientist defense all...\n[{'choice': 'Disagree', 'number': 2}]\n\n\n3\n3ii1tu719B3\nDoes focus statement peace forward do relate?\n[{'choice': 'Neutral', 'number': 6}]\n\n\n4\n9xv2JX456C7\nDoes explain ability plant?\n[{'choice': 'Strongly Disagree', 'number': 5}]\n\n\n5\n3sG5am762C6\nWhy does know writer ball bad whole?\n[{'choice': 'Agree', 'number': 3}]\n\n\n6\n5uM6LP013p7\nWhy does break chance boy enjoy call paper yet?\n[{'choice': 'Strongly Disagree', 'number': 9}]\n\n\n7\n3NE9pu846z1\nDoes note spring newspaper and that thing?\n[{'choice': 'Neutral', 'number': 7}]\n\n\n8\n6ge7Zt058d3\nWhy does message next eat the stay?\n[{'choice': 'Neutral', 'number': 0}]\n\n\n9\n2EB3vo658l2\nHow does no later then inside fill discover?\n[{'choice': 'Agree', 'number': 4}]\n\n\n\n\n\n\n\nBut when we direct the parser to just unpack the reponses array, we lose our data from the level above. Pandas can be instructed to keep this by giving it a list of metadata to repeat for each record it unpacks from the level above. We use the meta parameter and pass it a list of the fields to include.\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=\"responses\", \n     meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n    ],\n)\ndf\n   \n\n\n\n\n\n\n\n\nquestion_id\nquestion_text\nchoices\nrespondent_id\nsurvey_date\nemail\n\n\n\n\n0\n9lQ8OH810H2\nHow does mean action onto south usually prepar...\n[{'choice': 'Strongly Disagree', 'number': 9}]\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n1\n8YZ3Ri208p1\nDoes million mean tax foot statement?\n[{'choice': 'Strongly Disagree', 'number': 4}]\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n2\n2Ys2yn819r0\nHow does decade what air scientist defense all...\n[{'choice': 'Disagree', 'number': 2}]\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n3\n3ii1tu719B3\nDoes focus statement peace forward do relate?\n[{'choice': 'Neutral', 'number': 6}]\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n4\n9xv2JX456C7\nDoes explain ability plant?\n[{'choice': 'Strongly Disagree', 'number': 5}]\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n5\n3sG5am762C6\nWhy does know writer ball bad whole?\n[{'choice': 'Agree', 'number': 3}]\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n6\n5uM6LP013p7\nWhy does break chance boy enjoy call paper yet?\n[{'choice': 'Strongly Disagree', 'number': 9}]\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n7\n3NE9pu846z1\nDoes note spring newspaper and that thing?\n[{'choice': 'Neutral', 'number': 7}]\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n8\n6ge7Zt058d3\nWhy does message next eat the stay?\n[{'choice': 'Neutral', 'number': 0}]\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n9\n2EB3vo658l2\nHow does no later then inside fill discover?\n[{'choice': 'Agree', 'number': 4}]\n8J14yp561A1\n1970-07-08\nhalvarado@example.com"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#we-must-go-deeper",
    "href": "posts/json-normalize/json-normalize.html#we-must-go-deeper",
    "title": "Normalize JSON with Pandas",
    "section": "We must go deeper!",
    "text": "We must go deeper!\nThat works for the most part, but we still have that annoying choices json array that would be nice to split out into columns.\ndf5 = pd.json_normalize(\n    sample_data, \n    record_path=\"choices\", \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n    ],\n)\nSimply passing the choices field to the record_path param results in a KeyError though. This is because the choices field is actually nested in the responses field. So pandas need us to construct a path to reach it. We can get to it by passing each key as a record in the list to construct a path. Here that looks like [\"responses\", \"choices\"].\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=[\"responses\", \"choices\"], \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n    ],\n)\ndf\n\n\n\n\n\n\n\n\nchoice\nnumber\nrespondent_id\nsurvey_date\nemail\n\n\n\n\n0\nStrongly Disagree\n9\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n1\nStrongly Disagree\n4\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n2\nDisagree\n2\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n3\nNeutral\n6\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n4\nStrongly Disagree\n5\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n\n\n5\nAgree\n3\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n6\nStrongly Disagree\n9\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n7\nNeutral\n7\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n8\nNeutral\n0\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n9\nAgree\n4\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n\n\n\n\n\n\n\nBut when we do that, we lose our question_id and question_text fields. That’s because we need to add them in the meta list and pass their paths like the record path param. See below.\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=[\"responses\", \"choices\"], \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n        [\"responses\", \"question_id\"],\n        [\"responses\", \"question_text\"],\n    ],\n)\ndf\n\n\n\n\n\n\n\n\nchoice\nnumber\nrespondent_id\nsurvey_date\nemail\nresponses.question_id\nresponses.question_text\n\n\n\n\n0\nStrongly Disagree\n9\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9lQ8OH810H2\nHow does mean action onto south usually prepar...\n\n\n1\nStrongly Disagree\n4\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n8YZ3Ri208p1\nDoes million mean tax foot statement?\n\n\n2\nDisagree\n2\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n2Ys2yn819r0\nHow does decade what air scientist defense all...\n\n\n3\nNeutral\n6\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n3ii1tu719B3\nDoes focus statement peace forward do relate?\n\n\n4\nStrongly Disagree\n5\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9xv2JX456C7\nDoes explain ability plant?\n\n\n5\nAgree\n3\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3sG5am762C6\nWhy does know writer ball bad whole?\n\n\n6\nStrongly Disagree\n9\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n5uM6LP013p7\nWhy does break chance boy enjoy call paper yet?\n\n\n7\nNeutral\n7\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3NE9pu846z1\nDoes note spring newspaper and that thing?\n\n\n8\nNeutral\n0\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n6ge7Zt058d3\nWhy does message next eat the stay?\n\n\n9\nAgree\n4\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n2EB3vo658l2\nHow does no later then inside fill discover?\n\n\n\n\n\n\n\nOne last tweak: some databases (like MS SQL) don’t like naming columns with that period in the name. As a work around you can give the json_normalize function a custom separator such as an underscore instead.\n\ndf = pd.json_normalize(\n    sample_data, \n    sep=\"_\",\n    record_path=[\"responses\", \"choices\"], \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n        [\"responses\", \"question_id\"],\n        [\"responses\", \"question_text\"]\n    ],\n)\ndf\n\n\n\n\n\n\n\n\nchoice\nnumber\nrespondent_id\nsurvey_date\nemail\nresponses_question_id\nresponses_question_text\n\n\n\n\n0\nStrongly Disagree\n9\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9lQ8OH810H2\nHow does mean action onto south usually prepar...\n\n\n1\nStrongly Disagree\n4\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n8YZ3Ri208p1\nDoes million mean tax foot statement?\n\n\n2\nDisagree\n2\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n2Ys2yn819r0\nHow does decade what air scientist defense all...\n\n\n3\nNeutral\n6\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n3ii1tu719B3\nDoes focus statement peace forward do relate?\n\n\n4\nStrongly Disagree\n5\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9xv2JX456C7\nDoes explain ability plant?\n\n\n5\nAgree\n3\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3sG5am762C6\nWhy does know writer ball bad whole?\n\n\n6\nStrongly Disagree\n9\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n5uM6LP013p7\nWhy does break chance boy enjoy call paper yet?\n\n\n7\nNeutral\n7\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3NE9pu846z1\nDoes note spring newspaper and that thing?\n\n\n8\nNeutral\n0\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n6ge7Zt058d3\nWhy does message next eat the stay?\n\n\n9\nAgree\n4\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n2EB3vo658l2\nHow does no later then inside fill discover?\n\n\n\n\n\n\n\nFinally, we’ll order the columns from the least nested level on the left all the way to the most nested on the right for easier readability.\n\ncolumn_order = [\"respondent_id\", \"survey_date\", \"email\", \"responses_question_id\", \"responses_question_text\", \"choice\", \"number\"]\ndf = df[column_order]\ndf\n\n\n\n\n\n\n\n\nrespondent_id\nsurvey_date\nemail\nresponses_question_id\nresponses_question_text\nchoice\nnumber\n\n\n\n\n0\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9lQ8OH810H2\nHow does mean action onto south usually prepar...\nStrongly Disagree\n9\n\n\n1\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n8YZ3Ri208p1\nDoes million mean tax foot statement?\nStrongly Disagree\n4\n\n\n2\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n2Ys2yn819r0\nHow does decade what air scientist defense all...\nDisagree\n2\n\n\n3\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n3ii1tu719B3\nDoes focus statement peace forward do relate?\nNeutral\n6\n\n\n4\n1W38Sn628N9\n1985-09-30\nofrench@example.com\n9xv2JX456C7\nDoes explain ability plant?\nStrongly Disagree\n5\n\n\n5\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3sG5am762C6\nWhy does know writer ball bad whole?\nAgree\n3\n\n\n6\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n5uM6LP013p7\nWhy does break chance boy enjoy call paper yet?\nStrongly Disagree\n9\n\n\n7\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n3NE9pu846z1\nDoes note spring newspaper and that thing?\nNeutral\n7\n\n\n8\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n6ge7Zt058d3\nWhy does message next eat the stay?\nNeutral\n0\n\n\n9\n8J14yp561A1\n1970-07-08\nhalvarado@example.com\n2EB3vo658l2\nHow does no later then inside fill discover?\nAgree\n4"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#parsed-and-ready-to-import",
    "href": "posts/json-normalize/json-normalize.html#parsed-and-ready-to-import",
    "title": "Normalize JSON with Pandas",
    "section": "Parsed and ready to import",
    "text": "Parsed and ready to import\nAt this point our data is in a simple tabular format and ready to import into a database table with something like pandas.to_sql() function, but we’ll save that for another post."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html",
    "href": "posts/how-i-teach-git/index.html",
    "title": "How I Teach Git",
    "section": "",
    "text": "When I first learned to use Git, it supercharged my development and I was quickly sold on its value, but I had no real understanding of how it worked. I just memorized and/or frequently Googled the right commands and hoped I never encountered any issue I couldn’t solve on my own or find a solution for in Oh Shit, Git!?!\nEven since spending the time to understand it better, I’ve had a fewer and fewer encounters where a major malfunction like someone pushing production security keys to a public repo has resulted in nuking the repo and starting fresh. Luckily that’s only happened on very nascent projects where that really was the simplest solution.\nThat being said, I wouldn’t wish that learning curve on anyone with half a brain and any less persistence than me. I’ve been known to call myself the wrecking ball because when all else fails I keep throwing myself at the wall until it crumbles. I’m stubborn to a fault, but I get things done."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#teaching-commands-just-encourages-cargo-culting",
    "href": "posts/how-i-teach-git/index.html#teaching-commands-just-encourages-cargo-culting",
    "title": "How I Teach Git",
    "section": "",
    "text": "When I first learned to use Git, it supercharged my development and I was quickly sold on its value, but I had no real understanding of how it worked. I just memorized and/or frequently Googled the right commands and hoped I never encountered any issue I couldn’t solve on my own or find a solution for in Oh Shit, Git!?!\nEven since spending the time to understand it better, I’ve had a fewer and fewer encounters where a major malfunction like someone pushing production security keys to a public repo has resulted in nuking the repo and starting fresh. Luckily that’s only happened on very nascent projects where that really was the simplest solution.\nThat being said, I wouldn’t wish that learning curve on anyone with half a brain and any less persistence than me. I’ve been known to call myself the wrecking ball because when all else fails I keep throwing myself at the wall until it crumbles. I’m stubborn to a fault, but I get things done."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#how-git-tracks-changes",
    "href": "posts/how-i-teach-git/index.html#how-git-tracks-changes",
    "title": "How I Teach Git",
    "section": "How Git Tracks Changes",
    "text": "How Git Tracks Changes\nWhile searching for better ways to teach about the change tracking concepts in Git, I came across this interactive visualization which I thought was incredibly helpful for wrapping your head around what’s happening when you type in those magical commands.\n\n\n\nImage Git Change Tracking Visualized\n\n\n\nWorking Tree/Directory\nThis context encapsulates the untracked changes to files in an initialized (git init) repository.\n\n\nStaging Area\nThis context encapsulated the tracked changes to files and allows you to review atomic changes as well as control the granularity of commits. Files are added here to be tracked.\n\n\nLocal Repository\nThis is like a small database in which you can make save points or commits in your file history. It lives in a special hidden folder in the directory (.git/)\n\n\nRemote Repository\nThis is a centralized version/copy of the git repository that lives on a different computer or hosted service like Github, Gitlab, Bitbucket, or even a self hosted server. Changes are pushed to the remote repository."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#git-collaboration-with-feature-branch-workflow",
    "href": "posts/how-i-teach-git/index.html#git-collaboration-with-feature-branch-workflow",
    "title": "How I Teach Git",
    "section": "Git Collaboration with Feature Branch Workflow",
    "text": "Git Collaboration with Feature Branch Workflow\nI generally find the simplest way to collaborate with others in a codebase is the feature branch workflow approach. Each dev works on a separate and limited in scope feature branch, code reviews happen in pull requests, and then code is frequently merged into the main not master branch (see here for a great reason to start using main instead of master).\n\n\n\nImage Feature Branch Workflow\n\n\n\nGit Workflow in 10 Steps\nOk, so I know I said don’t just memorize commands, but these are the 10 commands to commit (see what I did there?) to memory.\n\n1. Clone new repo or pull changes to main branch for existing\n$ git clone https://github.com/the-repos-url.git\nOR\n$ git pull origin main\n\n\n2. Checkout a local feature branch to make changes\n$ git checkout -b my_new_branch\n\n\n3. Commit after making your code changes (writing good commits)\n$ git add -A\n$ git commit -m \"Add some new functionality\"\n\n\n4. Push changes to a remote feature branch\n$ git push origin my_new_branch\n\n\n5. Open a Pull Request (and request reviewers)\n\n\n\nImage Github PR button\n\n\n\n\n6. Code Review: Responding to change requests\n$ git add -A\n$ git commit -m \"Fix issue raised in code review\"\n\n\n7. Code Review: Approved\n\n\n\nImage Github PR approval\n\n\n\n\n8. Merge (or squash merge) into main branch\n\n\n\nImage Github merge approved changes dialog\n\n\n\n\n9. Checkout main (locally)\n$ git checkout main\n\n\n10. Pull changes (back to step 1)\n$ git pull origin main\nRinse and repeat for the next feature!"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#common-git-commands-and-what-they-do",
    "href": "posts/how-i-teach-git/index.html#common-git-commands-and-what-they-do",
    "title": "How I Teach Git",
    "section": "Common Git Commands and What They Do",
    "text": "Common Git Commands and What They Do\n$ git clone # copy remote repos to local\n$ git checkout -b branch_name # create and checkout a new branch\n$ git add -A # add all changed files to staging\n$ git status # view status of the working directory and staging area\n$ git diff # view changes made on file(s)\n$ git commit -m “Your message goes here” #  save changes to local repo\n$ git pull origin main #  get changes to main branch from remote repo\n$ git fetch #  get references to all remote branches, so they can be checked out\n$ git push origin branch_name #  push local changes to remote repo\n$ git merge main #  merge changes to main branch into another branch\n$ git log --oneline #  view list of commit messages\n\nA Few More and When to Use Them\n$ git stash #  temporarily store changes in order to switch contexts\n$ git stash pop #  restore temporarily stored changes\n$ git reset #  reset tracked file(s) to untracked (can also be used for dangerous reverts)\n$ git revert #  safer way to undo changes (fail forward)"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#creating-branches-when-why-how",
    "href": "posts/how-i-teach-git/index.html#creating-branches-when-why-how",
    "title": "How I Teach Git",
    "section": "Creating Branches (when, why, how)",
    "text": "Creating Branches (when, why, how)\n\n\n\nImage Checkout Branch Diagram\n\n\n\nWhen\nAs a rule you should use branches liberally. Anytime you need to make a change or set of changes to the code base, you should create a “feature” branch to isolate those changes.\n\n\nWhy\nIsolating your changes to a branch prevents conflicts with other collaborators and helps you experiment without fear of making “breaking” changes to the main codebase. Great for experimenting!\n\n\nHow\n$ git branch #  list all local branches\n$ git branch new_branch_name #  create a new branch\n$ git checkout new_branch_name #  switch to that new branch\n$ git checkout -b new_branch_name #  create and checkout in one step\n$ git branch -d new_branch_name #  delete a branch safely (error if unmerged)\n$ git branch -D new_branch_name #  force delete (can delete unmerged)"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#pull-requests",
    "href": "posts/how-i-teach-git/index.html#pull-requests",
    "title": "How I Teach Git",
    "section": "Pull Requests",
    "text": "Pull Requests\nPull requests let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch\nCode Review (Approve, Request Changes, Comment) → Merge → Deploy"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#pulling-changes-from-main-to-a-local-branch-aka-merging-locally",
    "href": "posts/how-i-teach-git/index.html#pulling-changes-from-main-to-a-local-branch-aka-merging-locally",
    "title": "How I Teach Git",
    "section": "Pulling Changes from main to a Local Branch AKA Merging Locally",
    "text": "Pulling Changes from main to a Local Branch AKA Merging Locally\n\n\n\nImage Local Merge Diagram\n\n\n\nWhen\nA change on main affects your local branch and you need to integrate them into your local changes. This is the same process Github runs when a PR is approved and merged into the main branch.\n\n\nWhy\nResolving merge conflicts locally before creating PR allows for simpler code review and prevents complications in getting your changes integrated into the main branch.\n\n\nHow\nFirst commit or stash any changes you are currently working on. Then checkout main and pull changes from the remote repo. Checkout your feature branch again and run git merge main to begin the merge process. If there are any merge conflicts, resolve them by choosing which segment of code to keep and then make a new commit."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#links-to-additional-resources",
    "href": "posts/how-i-teach-git/index.html#links-to-additional-resources",
    "title": "How I Teach Git",
    "section": "Links to Additional Resources",
    "text": "Links to Additional Resources\n\nGuides\n\nHow to Use Git with DataGrip\nGithub Hello World Guide\nGit Feature Branch Workflow\nHow to Write a Git Commit Message\nGit Reset\nGit Revert\nGit Alias\ngit config\nOh Shit, Git!?!\nWhy secrets inside git are such a problem\nGit Training\n\n\n\nVisualizations\n\nGit Init Tutorial\nGitflow: Animated\n\n\n\nTools\n\nGithub Plugin for DataGrip\nTerminal Plugin for DataGrip\nGit History Plugin for VS Code\nGitLens Plugin for VS Code"
  },
  {
    "objectID": "posts/how-memes-inform-my-work/index.html",
    "href": "posts/how-memes-inform-my-work/index.html",
    "title": "How Memes Inform My Work",
    "section": "",
    "text": "I remember fondly how excited I was the first time I understood an obscure coding joke on r/programmerhumor. It felt like finally being in “the club” and not feeling like such an outsider.\nWhile I’ve been programming in some capacity or another for over 20 years, I didn’t get here through the regular channels. I didn’t get a degree in Computer Science. I’ve never worked at a startup or one of the Big 4 (or is it Frightful 5 now?). And as a result, I (like many programmers) often succumb to bouts of Imposter Syndrome. So finally feeling like I was part of the in-crowd was a victory in and of itself back then.\nPart of that victory is not only understanding those in-jokes and memes, but also internalizing many of the lessons they have to teach. I’ve definitely purloined many a meme, joke, or comic along the way and made it my own.\n\nOk. Not like that. I don’t mean, I’ve claimed ownership in the conventional sense, but I mean I have taken quite a few to heart in ways that make them a part of who I am professionally and philosophically.\nWhat follows is my attempt to call out some of those that have impacted me the most, as well as explain why so many of these are pinned to the walls of my cubicle.\n\nPick Two\n\nThis is probably one of the first memes I actually felt spoke to me and my frustrations with the limitations of project work in the software world. I remember just going, “Yeah. That.” And then feeling a deep need to print it out, stick it to the wall of my cubicle for reference the next time somebody asked for the impossible. Also known as all three.\nAll too often, we as developers, are asked to build quality products while still meeting deadlines and budgets with limited resources. Because so much of what we do is invisible to stakeholders and end-users, it is often hand-waved away as “magic” while somehow also simultaneously being dismissed as the type of work you can just hand off to the interchangable cogs. And if they can’t meet the feature requests then it’s likely just their competency that’s at issue.\nWe’ve all been spoiled by the free high-tech apps we consume on a daily basis and assume similar quality can be built by tiny teams. I am often comforted in those moments by the fundamental laws of production this meme espouses.\n\n\nThe Two States of Every Programmer\n\nWhile we’re on the topic of imposter syndrome and our own limitations, this meme is one I identify with all the time. It doesn’t matter how good you get at programming. The field of what we know and what is possible is constantly shifting and growing. And there isn’t a day that goes by while debugging something or trying to get a new feature implemented that I don’t find myself osciallating between these states.\nIt’s important to remember we all feel this way and to remind ourselves of the opposite state when we’re stuck in the other.\nI think what truly separates those that drown in the imposter syndrome river and those that keep paddling on is a healthy level of the growth mindset. I’m not afraid of failure and feeling like that confused dog, if every now and then I crack some puzzling code and feel lke a damn genius. That dopamine rush, however temporary or illusory, is a bit of an addiction. One I am more than happy to indulge at the cost of a little ego-death now and then.\n\n\nIs it Worth the Time?\n\nThere are plenty of XKCD comics with jokes and references that fly right over my head, but this one not only speaks to me, it is damn well a guidepost in my work. This one is likewise hanging above my desk as a constant reminder and reference.\nI spend a lot of time automating tasks. My own, my teams, and more often than not, those of other departments in the org. The automation of boring stuff is one of the most powerful impacts of code and one that I think gets at the heart of why we do what we do.\nThat being said, it’s incredibly easy to fall down your own rabbit hole trying to build the perfect solution in search of time savings. At the start of every new project request, I consult this chart and think deeply about the time being invested in the solution versus the time it’s actually saving. Return on investment is an important concept to grasp early on as a developer, because we’re inquisitive types who are more than willing to live in that damn rabbit hole if it means catching a white rabbit. Even if he is just a figment of our imagination.\n\n\nYou Don’t Have to Solve Everything Today\n\nI’m pretty sure this meme’s original context was in regards to depression, but one that nonetheless serves as a daily reminder to walk away sometimes. It’s easy to get so wrapped up in a problem to completely lose track of time. I can be hacking away while the office slowly clears out. It’s easy to think “I’ll go home once I solve this problem, or make this commit” but that can spiral out of control fast.\nThe older I get and the more my priorities focus on family and sustaining a healthy work/life balance, the more I look to this meme as a reminder that it really can “wait until tomorrow”.\nRemember, burn out is real and no one says on their death bed that they wish they had worked more. In the work/life balance equation, you’ve got to work to live not live to work. i\nIt’s corny. I know. But that doesn’t make it any less true.\n\n\nExploits of a Mom (AKA Little Bobby Tables)\n\nI started my professional career as a programmer largely working with SQL. And it’s still a daily part of my workload. Security has been everyone’s minds in the last few years with all the data breaches and hacks. Education is not immune either, and this comic about SQL injection is a good reminder that it’s important to tighten up those holes in your apps lest someone exploit them for fun or profit!\nEven in 2017, SQL injection was still the number one risk in the OWASP Top Ten Most Critical Web Application Security Risks.\n\n\nOffice Space Lumberg\n\nI don’t think I truly appreciated the brilliance of Mike Judge’s Office Space until I started to work in an office job. Although, even as a teacher I identified with beating the crap out of a copier/fax machine. I laughed at all the jokes and obvious exaggerations, but now I find myself trying to figure out which of the characters I am and which ones I never want to be.\nThis is particularly true now that I’ve moved into a role as a manager. Lumberg is the quintessential terrible boss and every time I find myself leaning on one of my teammate’s cubicles with coffee cup in hand, I can’t help feeling like I might accidentally utter, “Yeah if you could just… that’d be great.”\nDon’t be that guy. Don’t be that guy. Don’t be that guy.\nI joke about it practically every time, but I really have internalized the idea that being someone’s boss isn’t about assigning them work and cracking the whip. That never helped me as an employee and I would never want to be that to someone else.\nI’m still trying to find my way as I transition into this role, but so far, I’m finding that leading from the front is much more effective than shouting orders from the back. “Come with me” is so much more potent than “Go that way”.\nWhat memes have you taken to heart in your work or personal life? Which comics or jokes do you comfort yourself with in your moments of desparation or exhilaration?"
  },
  {
    "objectID": "posts/student-support-spotlight/index.html",
    "href": "posts/student-support-spotlight/index.html",
    "title": "Virtual K-12 Tableau User Group - 14 January 2021",
    "section": "",
    "text": "In the wake of COVID19, we needed to provide our schools with a way to identify students who would most benefit from additional supports to access distance learning.\nMy colleague, Tvisi Ravi and I presented our approach to developing this tool in partnership with our school teams and provided a demo of how the tool can be used. Lastly, we gave an overview of the impact it has had in our schools so far.\nI’ve preset the video below to begin when we started presenting, but the prior demo from Boulder Valley School District is also worth a watch!"
  },
  {
    "objectID": "posts/student-support-spotlight/index.html#student-support-spotlight",
    "href": "posts/student-support-spotlight/index.html#student-support-spotlight",
    "title": "Virtual K-12 Tableau User Group - 14 January 2021",
    "section": "",
    "text": "In the wake of COVID19, we needed to provide our schools with a way to identify students who would most benefit from additional supports to access distance learning.\nMy colleague, Tvisi Ravi and I presented our approach to developing this tool in partnership with our school teams and provided a demo of how the tool can be used. Lastly, we gave an overview of the impact it has had in our schools so far.\nI’ve preset the video below to begin when we started presenting, but the prior demo from Boulder Valley School District is also worth a watch!"
  },
  {
    "objectID": "posts/student-support-spotlight/index.html#using-demo-data",
    "href": "posts/student-support-spotlight/index.html#using-demo-data",
    "title": "Virtual K-12 Tableau User Group - 14 January 2021",
    "section": "Using Demo Data",
    "text": "Using Demo Data\nOne technique we used in order to easily prepare our student data for a public audience was to obfuscate our student names and alias our school names to quickly make this data anonymous without having to build a tool in parallel or have to duplicate the underlying data source with completely fake data.\nTo accomplish this we used two approaches, in our underlying student data in Schoolzilla there is a ScrambledName field we swapped our actual student names with. Secondly, we used Tableau’s aliasing feature to rename our schools and published the alternate report to our Tableau server as a “Demo Version”.\nThis has also been useful internally for developing screencast tutorials without needing to expose any sensitive student information to the viewers of those tutorials."
  }
]