[
  {
    "objectID": "posts/how-i-teach-git/index.html",
    "href": "posts/how-i-teach-git/index.html",
    "title": "How I Teach Git",
    "section": "",
    "text": "When I first learned to use Git, it supercharged my development and I was quickly sold on its value, but I had no real understanding of how it worked. I just memorized and/or frequently Googled the right commands and hoped I never encountered any issue I couldn’t solve on my own or find a solution for in Oh Shit, Git!?!\nEven since spending the time to understand it better, I’ve had a fewer and fewer encounters where a major malfunction like someone pushing production security keys to a public repo has resulted in nuking the repo and starting fresh. Luckily that’s only happened on very nascent projects where that really was the simplest solution.\nThat being said, I wouldn’t wish that learning curve on anyone with half a brain and any less persistence than me. I’ve been known to call myself the wrecking ball because when all else fails I keep throwing myself at the wall until it crumbles. I’m stubborn to a fault, but I get things done."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#how-git-tracks-changes",
    "href": "posts/how-i-teach-git/index.html#how-git-tracks-changes",
    "title": "How I Teach Git",
    "section": "How Git Tracks Changes",
    "text": "How Git Tracks Changes\nWhile searching for better ways to teach about the change tracking concepts in Git, I came across this interactive visualization which I thought was incredibly helpful for wrapping your head around what’s happening when you type in those magical commands.\n\n\n\nImage Git Change Tracking Visualized\n\n\n\nWorking Tree/Directory\nThis context encapsulates the untracked changes to files in an initialized (git init) repository.\n\n\nStaging Area\nThis context encapsulated the tracked changes to files and allows you to review atomic changes as well as control the granularity of commits. Files are added here to be tracked.\n\n\nLocal Repository\nThis is like a small database in which you can make save points or commits in your file history. It lives in a special hidden folder in the directory (.git/)\n\n\nRemote Repository\nThis is a centralized version/copy of the git repository that lives on a different computer or hosted service like Github, Gitlab, Bitbucket, or even a self hosted server. Changes are pushed to the remote repository."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#git-collaboration-with-feature-branch-workflow",
    "href": "posts/how-i-teach-git/index.html#git-collaboration-with-feature-branch-workflow",
    "title": "How I Teach Git",
    "section": "Git Collaboration with Feature Branch Workflow",
    "text": "Git Collaboration with Feature Branch Workflow\nI generally find the simplest way to collaborate with others in a codebase is the feature branch workflow approach. Each dev works on a separate and limited in scope feature branch, code reviews happen in pull requests, and then code is frequently merged into the main not master branch (see here for a great reason to start using main instead of master).\n\n\n\nImage Feature Branch Workflow\n\n\n\nGit Workflow in 10 Steps\nOk, so I know I said don’t just memorize commands, but these are the 10 commands to commit (see what I did there?) to memory.\n\n1. Clone new repo or pull changes to main branch for existing\n$ git clone https://github.com/the-repos-url.git\nOR\n$ git pull origin main\n\n\n2. Checkout a local feature branch to make changes\n$ git checkout -b my_new_branch\n\n\n3. Commit after making your code changes (writing good commits)\n$ git add -A\n$ git commit -m \"Add some new functionality\"\n\n\n4. Push changes to a remote feature branch\n$ git push origin my_new_branch\n\n\n5. Open a Pull Request (and request reviewers)\n\n\n\nImage Github PR button\n\n\n\n\n6. Code Review: Responding to change requests\n$ git add -A\n$ git commit -m \"Fix issue raised in code review\"\n\n\n7. Code Review: Approved\n\n\n\nImage Github PR approval\n\n\n\n\n8. Merge (or squash merge) into main branch\n\n\n\nImage Github merge approved changes dialog\n\n\n\n\n9. Checkout main (locally)\n$ git checkout main\n\n\n10. Pull changes (back to step 1)\n$ git pull origin main\nRinse and repeat for the next feature!"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#common-git-commands-and-what-they-do",
    "href": "posts/how-i-teach-git/index.html#common-git-commands-and-what-they-do",
    "title": "How I Teach Git",
    "section": "Common Git Commands and What They Do",
    "text": "Common Git Commands and What They Do\n$ git clone # copy remote repos to local\n$ git checkout -b branch_name # create and checkout a new branch\n$ git add -A # add all changed files to staging\n$ git status # view status of the working directory and staging area\n$ git diff # view changes made on file(s)\n$ git commit -m “Your message goes here” #  save changes to local repo\n$ git pull origin main #  get changes to main branch from remote repo\n$ git fetch #  get references to all remote branches, so they can be checked out\n$ git push origin branch_name #  push local changes to remote repo\n$ git merge main #  merge changes to main branch into another branch\n$ git log --oneline #  view list of commit messages\n\nA Few More and When to Use Them\n$ git stash #  temporarily store changes in order to switch contexts\n$ git stash pop #  restore temporarily stored changes\n$ git reset #  reset tracked file(s) to untracked (can also be used for dangerous reverts)\n$ git revert #  safer way to undo changes (fail forward)"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#creating-branches-when-why-how",
    "href": "posts/how-i-teach-git/index.html#creating-branches-when-why-how",
    "title": "How I Teach Git",
    "section": "Creating Branches (when, why, how)",
    "text": "Creating Branches (when, why, how)\n\n\n\nImage Checkout Branch Diagram\n\n\n\nWhen\nAs a rule you should use branches liberally. Anytime you need to make a change or set of changes to the code base, you should create a “feature” branch to isolate those changes.\n\n\nWhy\nIsolating your changes to a branch prevents conflicts with other collaborators and helps you experiment without fear of making “breaking” changes to the main codebase. Great for experimenting!\n\n\nHow\n$ git branch #  list all local branches\n$ git branch new_branch_name #  create a new branch\n$ git checkout new_branch_name #  switch to that new branch\n$ git checkout -b new_branch_name #  create and checkout in one step\n$ git branch -d new_branch_name #  delete a branch safely (error if unmerged)\n$ git branch -D new_branch_name #  force delete (can delete unmerged)"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#pull-requests",
    "href": "posts/how-i-teach-git/index.html#pull-requests",
    "title": "How I Teach Git",
    "section": "Pull Requests",
    "text": "Pull Requests\nPull requests let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch\nCode Review (Approve, Request Changes, Comment) → Merge → Deploy"
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#pulling-changes-from-main-to-a-local-branch-aka-merging-locally",
    "href": "posts/how-i-teach-git/index.html#pulling-changes-from-main-to-a-local-branch-aka-merging-locally",
    "title": "How I Teach Git",
    "section": "Pulling Changes from main to a Local Branch AKA Merging Locally",
    "text": "Pulling Changes from main to a Local Branch AKA Merging Locally\n\n\n\nImage Local Merge Diagram\n\n\n\nWhen\nA change on main affects your local branch and you need to integrate them into your local changes. This is the same process Github runs when a PR is approved and merged into the main branch.\n\n\nWhy\nResolving merge conflicts locally before creating PR allows for simpler code review and prevents complications in getting your changes integrated into the main branch.\n\n\nHow\nFirst commit or stash any changes you are currently working on. Then checkout main and pull changes from the remote repo. Checkout your feature branch again and run git merge main to begin the merge process. If there are any merge conflicts, resolve them by choosing which segment of code to keep and then make a new commit."
  },
  {
    "objectID": "posts/how-i-teach-git/index.html#links-to-additional-resources",
    "href": "posts/how-i-teach-git/index.html#links-to-additional-resources",
    "title": "How I Teach Git",
    "section": "Links to Additional Resources",
    "text": "Links to Additional Resources\n\nGuides\n\nHow to Use Git with DataGrip\nGithub Hello World Guide\nGit Feature Branch Workflow\nHow to Write a Git Commit Message\nGit Reset\nGit Revert\nGit Alias\ngit config\nOh Shit, Git!?!\nWhy secrets inside git are such a problem\nGit Training\n\n\n\nVisualizations\n\nGit Init Tutorial\nGitflow: Animated\n\n\n\nTools\n\nGithub Plugin for DataGrip\nTerminal Plugin for DataGrip\nGit History Plugin for VS Code\nGitLens Plugin for VS Code"
  },
  {
    "objectID": "posts/quarto-blog/index.html",
    "href": "posts/quarto-blog/index.html",
    "title": "Goodbye Pelican, Hello Quarto",
    "section": "",
    "text": "Where it All Started\nSeven years ago, I first published a Jekyll blog hosted in Github pages and let it lie largely dormant for two years. Then, as a New Year’s resolution, I rebuilt the entire thing using Pelican so I could stop using Ruby and, finally, fully transition all my personal code projects into Python.\nIt’s been five years since that blog post, and in that time I’ve published only 13 post on this blog. That’s less than three posts a year.\nThat’s kind of sad.\n\n\nStarting Over\nIn the years since I began on that resolution to write more and educate on Python, I found myself struggling to generate new content. Not for a lack of ideas, but from a combination of excuse making (classic procrastination) and technical barriers that made publishing content less than streamlined.\nThe way the Pelican based blog was setup required some complexity with a multi-repo setup. One for the source markdown files and one for the html output.\nThis required the use of git submodules and memorization of Pelican commands to generate the content. I forgot them often enough that I ended up using the README of the source repo to add notes for myself so I wouldn’t forget how to publish.\nSoon, I was looking to write more content that included not just blocks of example code but also their output and Python notebooks seemed like a good way to go about that. However, integrating them into Pelican became a journey all of it’s own. I stumbled across the pelican-jupyter library and first things were great. Suddenly I could mix markdown and python code and publish to my blog, but the fun stopped soon enough. The package depended on nbconvert to convert the notebooks into the html needed for publishing but a version upgrade broke the dependencies and soon even pinning the older version no longer solved all the conflicts; as other packages quickly required newer versions while the package was unable to keep up.\nI started looking for a better way to publish python notebooks as a blog and stumbled across Quarto. After reading through the docs, I was blown away. How long had this been around? This was everything I needed and more.\n\n\nBetter Than Ever\nBefore long I had a proof of concept stood up in parallel and was able to not only publish without any archaic commands but I could render previews locally in VS Code, eliminate the need for a second repo, and the themes and layouts were based on simple bootstrap. The current look and feel is a bit more basic, but the focus is more on the content and there is even a built in search tool.\nEven better, they had explicit directions for [publishing to Github Pages using Github Actions](https://quarto.org/docs/publishing/github-pages.html. This made it easier than ever to write content and publish withough worrying about the static site generation. In fact, I could even write markdown directly in Github (in the browser) and commit to the repo and the publish action would immediately deploy to my site! So cool.\nSuddenly, Github was my personal blog CMS without any need for a database or admin site. Just simple git commit.\nNow I have one less excuse to start writing more and much more functionality to play around with. I’m very excited about exploring more interactive data viz using Observable and the ability to switch between R and Python as I collaborate with some of my colleague who prefer to code in R, like Chris Haid."
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html",
    "href": "posts/json-normalize/json-normalize.html",
    "title": "Normalize JSON with Pandas",
    "section": "",
    "text": "When processing nested JSON data into a flat structure for importing into a relational database, it can be tricky to structure the data into the right shape. Pandas has a great tool for doing this called pandas.json_normalize() but the documentation doesn’t make it obvious how to leverage its capabilities for handling nested data structures.\nI thought I could provide a brief example using some randomly generated survey response data (using the Faker library) to illustrate it’s advantages."
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#setup",
    "href": "posts/json-normalize/json-normalize.html#setup",
    "title": "Normalize JSON with Pandas",
    "section": "Setup",
    "text": "Setup\nTo start, I’m going to be using pandas and Faker so we’ll import those. I’m also going to need to easily display the parsed json as well as the returned dataframes, so I’m importing the json module from the standard lib as well as some IPython notebook helpers for displaying dataframes as HTML tables.\n\nimport json\nfrom IPython.display import display, HTML\nfrom faker import Faker\nfrom faker.providers import BaseProvider, date_time, internet\nimport pandas as pd"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#fake-data",
    "href": "posts/json-normalize/json-normalize.html#fake-data",
    "title": "Normalize JSON with Pandas",
    "section": "Fake Data",
    "text": "Fake Data\nFaker doesn’t have a built in provider for survey questions, so let’s go ahead and add a simple one that creates non-sensical questions with a simple hack to the sentence provider.\n\nclass MyProvider(BaseProvider):\n    def question(self):\n        stems = ('Does', 'How does', 'Which', 'Why does')\n        stem = Faker().random_choices(elements=stems, length=1)[0]\n        sentence = Faker().sentence()\n        sentence = sentence[0].lower() + sentence[1:]\n        question = sentence.replace(\".\", \"?\")\n        question = f\"{stem} {question}\"\n        return question\n\nUsing this new question provider, we’ll construct a few records of fake survey response data with some respondent level data like a respondent_id, survey_date, and respondent email. Within that we’ll nest a list of responses which will in turn have it’s own dictionary of data at the question level: id, question text, and choices. The choices list will be singular here, but assume it has that structure because the API this comes from has to also account for multi-select options and we’ll need to parse it as a list regardless. Depending on our analysis needs, this might also be a place where we’d want to keep these in a comma separated string, but for our purposes here we’ll ignore that use case.\n\nfake = Faker()\nfake.add_provider(MyProvider)\nchoices = ('Strongly Agree', 'Agree', 'Neutral', 'Disagree', 'Strongly Disagree')\n\n\nsample_data = [\n    {\n        \"respondent_id\": fake.bothify(text=\"#?##??###?#\"),\n        \"survey_date\": fake.date(),\n        \"email\": fake.email(domain=\"example.com\"),\n        \"responses\": [\n            {\n                \"question_id\": fake.bothify(text=\"#??#??###?#\"),\n                \"question_text\": fake.question(),\n                \"choices\": [\n                    {\n                        \"choice\": fake.random_choices(elements=choices, length=1)[0],\n                        \"number\": fake.random_digit(),\n                    }\n                ]\n            } for _ in range(5)\n        ]\n    } for _ in range(2) \n]\n\nLet’s print out a single record to see the resulting data structure that has been generated randomly.\n\nprint(json.dumps(sample_data[0], indent=2))\n\n{\n  \"respondent_id\": \"1W38Sn628N9\",\n  \"survey_date\": \"1985-09-30\",\n  \"email\": \"ofrench@example.com\",\n  \"responses\": [\n    {\n      \"question_id\": \"9lQ8OH810H2\",\n      \"question_text\": \"How does mean action onto south usually prepare that?\",\n      \"choices\": [\n        {\n          \"choice\": \"Strongly Disagree\",\n          \"number\": 9\n        }\n      ]\n    },\n    {\n      \"question_id\": \"8YZ3Ri208p1\",\n      \"question_text\": \"Does million mean tax foot statement?\",\n      \"choices\": [\n        {\n          \"choice\": \"Strongly Disagree\",\n          \"number\": 4\n        }\n      ]\n    },\n    {\n      \"question_id\": \"2Ys2yn819r0\",\n      \"question_text\": \"How does decade what air scientist defense allow entire?\",\n      \"choices\": [\n        {\n          \"choice\": \"Disagree\",\n          \"number\": 2\n        }\n      ]\n    },\n    {\n      \"question_id\": \"3ii1tu719B3\",\n      \"question_text\": \"Does focus statement peace forward do relate?\",\n      \"choices\": [\n        {\n          \"choice\": \"Neutral\",\n          \"number\": 6\n        }\n      ]\n    },\n    {\n      \"question_id\": \"9xv2JX456C7\",\n      \"question_text\": \"Does explain ability plant?\",\n      \"choices\": [\n        {\n          \"choice\": \"Strongly Disagree\",\n          \"number\": 5\n        }\n      ]\n    }\n  ]\n}"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#reading-with-pandas",
    "href": "posts/json-normalize/json-normalize.html#reading-with-pandas",
    "title": "Normalize JSON with Pandas",
    "section": "Reading with Pandas",
    "text": "Reading with Pandas\nAs you can see below, simply reading this directly into a dataframe only parses the top level respondent data, but then keeps the responses data as a json array. Which isn’t great for simple analysis. Could you load that “as is” into a jsonb field in PostgreSQL? Sure. If you like parsing json with SQL. Yuck!\n\ndf = pd.DataFrame(sample_data)\ndf\n\n\n\n\n\n  \n    \n      \n      respondent_id\n      survey_date\n      email\n      responses\n    \n  \n  \n    \n      0\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      [{'question_id': '9lQ8OH810H2', 'question_text...\n    \n    \n      1\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      [{'question_id': '3sG5am762C6', 'question_text..."
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#a-messy-custom-parser",
    "href": "posts/json-normalize/json-normalize.html#a-messy-custom-parser",
    "title": "Normalize JSON with Pandas",
    "section": "A messy custom parser",
    "text": "A messy custom parser\nWe could attempt to reshape this by writing some custom functions to handle extracting the responses and merging that data with the top-level meta data about the respondent with some dictionary unpacking, but this gets messy and would fall apart quickly as the structure changed. While this approach works, it’s not ideal.\nSure this code could be further refactored to simplify the logic, but it’s not worth it since the pandas.json_normalize() can do this for us easily.\n\ndef record_format(responses):\n    data = []\n    for response in responses:\n        record = {\n            \"question_id\": response.get(\"question_id\"),\n            \"question_text\": response.get(\"question_text\"),\n            \"choice\": response.get(\"choices\")[0].get(\"choice\"),\n            \"number\": response.get(\"choices\")[0].get(\"number\"),\n        }\n        data.append(record)\n    return data\n\ndef parse_json(records):\n    data = []\n    for record in sample_data:\n        meta = {\n            \"respondent_id\": record.get(\"respondent_id\"),\n            \"survey_date\": record.get(\"survey_date\"),\n            \"email\": record.get(\"email\"),\n        }\n        responses = record.get(\"responses\")\n        formatted_responses = record_format(responses)\n        for response in formatted_responses:\n            combined = {**meta, **response}\n            data.append(combined)\n    return data\n\nclean_data = parse_json(sample_data)\ndf = pd.DataFrame(clean_data)\ndf\n\n\n\n\n\n  \n    \n      \n      respondent_id\n      survey_date\n      email\n      question_id\n      question_text\n      choice\n      number\n    \n  \n  \n    \n      0\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9lQ8OH810H2\n      How does mean action onto south usually prepar...\n      Strongly Disagree\n      9\n    \n    \n      1\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      8YZ3Ri208p1\n      Does million mean tax foot statement?\n      Strongly Disagree\n      4\n    \n    \n      2\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      2Ys2yn819r0\n      How does decade what air scientist defense all...\n      Disagree\n      2\n    \n    \n      3\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      3ii1tu719B3\n      Does focus statement peace forward do relate?\n      Neutral\n      6\n    \n    \n      4\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9xv2JX456C7\n      Does explain ability plant?\n      Strongly Disagree\n      5\n    \n    \n      5\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3sG5am762C6\n      Why does know writer ball bad whole?\n      Agree\n      3\n    \n    \n      6\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      5uM6LP013p7\n      Why does break chance boy enjoy call paper yet?\n      Strongly Disagree\n      9\n    \n    \n      7\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3NE9pu846z1\n      Does note spring newspaper and that thing?\n      Neutral\n      7\n    \n    \n      8\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      6ge7Zt058d3\n      Why does message next eat the stay?\n      Neutral\n      0\n    \n    \n      9\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      2EB3vo658l2\n      How does no later then inside fill discover?\n      Agree\n      4"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#json-normalize",
    "href": "posts/json-normalize/json-normalize.html#json-normalize",
    "title": "Normalize JSON with Pandas",
    "section": "JSON Normalize",
    "text": "JSON Normalize\nThankfully there is the json_normalize() function, but it requires a little understanding to get it to satisfactorily parse flat. Simply passing it the sample data without any parameters results in a very familiar result that gets us no further than we started in the first attempt.\n\ndf = pd.json_normalize(sample_data)\ndf\n\n\n\n\n\n  \n    \n      \n      respondent_id\n      survey_date\n      email\n      responses\n    \n  \n  \n    \n      0\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      [{'question_id': '9lQ8OH810H2', 'question_text...\n    \n    \n      1\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      [{'question_id': '3sG5am762C6', 'question_text...\n    \n  \n\n\n\n\nA few optional parameters can be used here to parse the first nested array called responses. We can direct the pandas json parser to a specific key as the source of records. The record_path parameter takes either a string or list of strings to construct that path. The name of this parameter is a hint about how to think of this when passed as a list as we’ll see later.\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=\"responses\", \n)\ndf\n\n\n\n\n\n  \n    \n      \n      question_id\n      question_text\n      choices\n    \n  \n  \n    \n      0\n      9lQ8OH810H2\n      How does mean action onto south usually prepar...\n      [{'choice': 'Strongly Disagree', 'number': 9}]\n    \n    \n      1\n      8YZ3Ri208p1\n      Does million mean tax foot statement?\n      [{'choice': 'Strongly Disagree', 'number': 4}]\n    \n    \n      2\n      2Ys2yn819r0\n      How does decade what air scientist defense all...\n      [{'choice': 'Disagree', 'number': 2}]\n    \n    \n      3\n      3ii1tu719B3\n      Does focus statement peace forward do relate?\n      [{'choice': 'Neutral', 'number': 6}]\n    \n    \n      4\n      9xv2JX456C7\n      Does explain ability plant?\n      [{'choice': 'Strongly Disagree', 'number': 5}]\n    \n    \n      5\n      3sG5am762C6\n      Why does know writer ball bad whole?\n      [{'choice': 'Agree', 'number': 3}]\n    \n    \n      6\n      5uM6LP013p7\n      Why does break chance boy enjoy call paper yet?\n      [{'choice': 'Strongly Disagree', 'number': 9}]\n    \n    \n      7\n      3NE9pu846z1\n      Does note spring newspaper and that thing?\n      [{'choice': 'Neutral', 'number': 7}]\n    \n    \n      8\n      6ge7Zt058d3\n      Why does message next eat the stay?\n      [{'choice': 'Neutral', 'number': 0}]\n    \n    \n      9\n      2EB3vo658l2\n      How does no later then inside fill discover?\n      [{'choice': 'Agree', 'number': 4}]\n    \n  \n\n\n\n\nBut when we direct the parser to just unpack the reponses array, we lose our data from the level above. Pandas can be instructed to keep this by giving it a list of metadata to repeat for each record it unpacks from the level above. We use the meta parameter and pass it a list of the fields to include.\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=\"responses\", \n     meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n    ],\n)\ndf\n   \n\n\n\n\n\n  \n    \n      \n      question_id\n      question_text\n      choices\n      respondent_id\n      survey_date\n      email\n    \n  \n  \n    \n      0\n      9lQ8OH810H2\n      How does mean action onto south usually prepar...\n      [{'choice': 'Strongly Disagree', 'number': 9}]\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      1\n      8YZ3Ri208p1\n      Does million mean tax foot statement?\n      [{'choice': 'Strongly Disagree', 'number': 4}]\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      2\n      2Ys2yn819r0\n      How does decade what air scientist defense all...\n      [{'choice': 'Disagree', 'number': 2}]\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      3\n      3ii1tu719B3\n      Does focus statement peace forward do relate?\n      [{'choice': 'Neutral', 'number': 6}]\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      4\n      9xv2JX456C7\n      Does explain ability plant?\n      [{'choice': 'Strongly Disagree', 'number': 5}]\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      5\n      3sG5am762C6\n      Why does know writer ball bad whole?\n      [{'choice': 'Agree', 'number': 3}]\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      6\n      5uM6LP013p7\n      Why does break chance boy enjoy call paper yet?\n      [{'choice': 'Strongly Disagree', 'number': 9}]\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      7\n      3NE9pu846z1\n      Does note spring newspaper and that thing?\n      [{'choice': 'Neutral', 'number': 7}]\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      8\n      6ge7Zt058d3\n      Why does message next eat the stay?\n      [{'choice': 'Neutral', 'number': 0}]\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      9\n      2EB3vo658l2\n      How does no later then inside fill discover?\n      [{'choice': 'Agree', 'number': 4}]\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#we-must-go-deeper",
    "href": "posts/json-normalize/json-normalize.html#we-must-go-deeper",
    "title": "Normalize JSON with Pandas",
    "section": "We must go deeper!",
    "text": "We must go deeper!\nThat works for the most part, but we still have that annoying choices json array that would be nice to split out into columns.\ndf5 = pd.json_normalize(\n    sample_data, \n    record_path=\"choices\", \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n    ],\n)\nSimply passing the choices field to the record_path param results in a KeyError though. This is because the choices field is actually nested in the responses field. So pandas need us to construct a path to reach it. We can get to it by passing each key as a record in the list to construct a path. Here that looks like [\"responses\", \"choices\"].\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=[\"responses\", \"choices\"], \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n    ],\n)\ndf\n\n\n\n\n\n  \n    \n      \n      choice\n      number\n      respondent_id\n      survey_date\n      email\n    \n  \n  \n    \n      0\n      Strongly Disagree\n      9\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      1\n      Strongly Disagree\n      4\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      2\n      Disagree\n      2\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      3\n      Neutral\n      6\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      4\n      Strongly Disagree\n      5\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n    \n    \n      5\n      Agree\n      3\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      6\n      Strongly Disagree\n      9\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      7\n      Neutral\n      7\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      8\n      Neutral\n      0\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n    \n      9\n      Agree\n      4\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n    \n  \n\n\n\n\nBut when we do that, we lose our question_id and question_text fields. That’s because we need to add them in the meta list and pass their paths like the record path param. See below.\n\ndf = pd.json_normalize(\n    sample_data, \n    record_path=[\"responses\", \"choices\"], \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n        [\"responses\", \"question_id\"],\n        [\"responses\", \"question_text\"],\n    ],\n)\ndf\n\n\n\n\n\n  \n    \n      \n      choice\n      number\n      respondent_id\n      survey_date\n      email\n      responses.question_id\n      responses.question_text\n    \n  \n  \n    \n      0\n      Strongly Disagree\n      9\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9lQ8OH810H2\n      How does mean action onto south usually prepar...\n    \n    \n      1\n      Strongly Disagree\n      4\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      8YZ3Ri208p1\n      Does million mean tax foot statement?\n    \n    \n      2\n      Disagree\n      2\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      2Ys2yn819r0\n      How does decade what air scientist defense all...\n    \n    \n      3\n      Neutral\n      6\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      3ii1tu719B3\n      Does focus statement peace forward do relate?\n    \n    \n      4\n      Strongly Disagree\n      5\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9xv2JX456C7\n      Does explain ability plant?\n    \n    \n      5\n      Agree\n      3\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3sG5am762C6\n      Why does know writer ball bad whole?\n    \n    \n      6\n      Strongly Disagree\n      9\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      5uM6LP013p7\n      Why does break chance boy enjoy call paper yet?\n    \n    \n      7\n      Neutral\n      7\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3NE9pu846z1\n      Does note spring newspaper and that thing?\n    \n    \n      8\n      Neutral\n      0\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      6ge7Zt058d3\n      Why does message next eat the stay?\n    \n    \n      9\n      Agree\n      4\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      2EB3vo658l2\n      How does no later then inside fill discover?\n    \n  \n\n\n\n\nOne last tweak: some databases (like MS SQL) don’t like naming columns with that period in the name. As a work around you can give the json_normalize function a custom separator such as an underscore instead.\n\ndf = pd.json_normalize(\n    sample_data, \n    sep=\"_\",\n    record_path=[\"responses\", \"choices\"], \n    meta=[\n        \"respondent_id\", \n        \"survey_date\",\n        \"email\", \n        [\"responses\", \"question_id\"],\n        [\"responses\", \"question_text\"]\n    ],\n)\ndf\n\n\n\n\n\n  \n    \n      \n      choice\n      number\n      respondent_id\n      survey_date\n      email\n      responses_question_id\n      responses_question_text\n    \n  \n  \n    \n      0\n      Strongly Disagree\n      9\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9lQ8OH810H2\n      How does mean action onto south usually prepar...\n    \n    \n      1\n      Strongly Disagree\n      4\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      8YZ3Ri208p1\n      Does million mean tax foot statement?\n    \n    \n      2\n      Disagree\n      2\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      2Ys2yn819r0\n      How does decade what air scientist defense all...\n    \n    \n      3\n      Neutral\n      6\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      3ii1tu719B3\n      Does focus statement peace forward do relate?\n    \n    \n      4\n      Strongly Disagree\n      5\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9xv2JX456C7\n      Does explain ability plant?\n    \n    \n      5\n      Agree\n      3\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3sG5am762C6\n      Why does know writer ball bad whole?\n    \n    \n      6\n      Strongly Disagree\n      9\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      5uM6LP013p7\n      Why does break chance boy enjoy call paper yet?\n    \n    \n      7\n      Neutral\n      7\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3NE9pu846z1\n      Does note spring newspaper and that thing?\n    \n    \n      8\n      Neutral\n      0\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      6ge7Zt058d3\n      Why does message next eat the stay?\n    \n    \n      9\n      Agree\n      4\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      2EB3vo658l2\n      How does no later then inside fill discover?\n    \n  \n\n\n\n\nFinally, we’ll order the columns from the least nested level on the left all the way to the most nested on the right for easier readability.\n\ncolumn_order = [\"respondent_id\", \"survey_date\", \"email\", \"responses_question_id\", \"responses_question_text\", \"choice\", \"number\"]\ndf = df[column_order]\ndf\n\n\n\n\n\n  \n    \n      \n      respondent_id\n      survey_date\n      email\n      responses_question_id\n      responses_question_text\n      choice\n      number\n    \n  \n  \n    \n      0\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9lQ8OH810H2\n      How does mean action onto south usually prepar...\n      Strongly Disagree\n      9\n    \n    \n      1\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      8YZ3Ri208p1\n      Does million mean tax foot statement?\n      Strongly Disagree\n      4\n    \n    \n      2\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      2Ys2yn819r0\n      How does decade what air scientist defense all...\n      Disagree\n      2\n    \n    \n      3\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      3ii1tu719B3\n      Does focus statement peace forward do relate?\n      Neutral\n      6\n    \n    \n      4\n      1W38Sn628N9\n      1985-09-30\n      ofrench@example.com\n      9xv2JX456C7\n      Does explain ability plant?\n      Strongly Disagree\n      5\n    \n    \n      5\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3sG5am762C6\n      Why does know writer ball bad whole?\n      Agree\n      3\n    \n    \n      6\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      5uM6LP013p7\n      Why does break chance boy enjoy call paper yet?\n      Strongly Disagree\n      9\n    \n    \n      7\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      3NE9pu846z1\n      Does note spring newspaper and that thing?\n      Neutral\n      7\n    \n    \n      8\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      6ge7Zt058d3\n      Why does message next eat the stay?\n      Neutral\n      0\n    \n    \n      9\n      8J14yp561A1\n      1970-07-08\n      halvarado@example.com\n      2EB3vo658l2\n      How does no later then inside fill discover?\n      Agree\n      4"
  },
  {
    "objectID": "posts/json-normalize/json-normalize.html#parsed-and-ready-to-import",
    "href": "posts/json-normalize/json-normalize.html#parsed-and-ready-to-import",
    "title": "Normalize JSON with Pandas",
    "section": "Parsed and ready to import",
    "text": "Parsed and ready to import\nAt this point our data is in a simple tabular format and ready to import into a database table with something like pandas.to_sql() function, but we’ll save that for another post."
  },
  {
    "objectID": "posts/google-classroom-connector/index.html",
    "href": "posts/google-classroom-connector/index.html",
    "title": "InnovateEDU Webinar - 19 November 2020",
    "section": "",
    "text": "In the wake of COVID19, we needed to provide our schools with visibility into student engagement with distance learning. The majority of our schools were using Google Classroom as their LMS (Learning Management System) and we knew that pulling this data down into our data warehouse ASAP would be a vital piece of that puzzle.\nWe initially built a data pipeline or “connector” using the Google Classroom API and published it to a public repo for others to use. I partnered closely with Zach Kagin who had volunteered his time pro-bono to the project to support us in this vital time. Shout out to his incredible work on batching the API calls which substantially improved the performance of the codebase.\nSoon after, Erin Mote from InnovateEDU reached out about a possible partnership to fork the repo and produce a version that could load the data to BigQuery rather than SQL Server so it could be run on a free instance of Google Cloud Platform with minimal effort. I partnered with Marcos Alcozer to translate our inital work into something that would be easily deployable on Google Cloud Shell. His team then put together beautiful and simple documentation to make it even more effortless to spin up this solution for Districts and CMOs around the country."
  },
  {
    "objectID": "posts/google-classroom-connector/index.html#webinar",
    "href": "posts/google-classroom-connector/index.html#webinar",
    "title": "InnovateEDU Webinar - 19 November 2020",
    "section": "Webinar",
    "text": "Webinar\nYou can see Erin and I presenting a tutorial on how to deploy this solution below.\n\n\n\n\nI’m incredibly proud of this work and the impact it was able to have in such a critical time. It was instrumental in my team’s ability to produce meaningful insights on distance learning engagement and even helped support tracking asynchronous attendance with a level of fidelity other orgs struggled to produce without massive manual overhead."
  },
  {
    "objectID": "posts/student-support-spotlight/index.html",
    "href": "posts/student-support-spotlight/index.html",
    "title": "Virtual K-12 Tableau User Group - 14 January 2021",
    "section": "",
    "text": "In the wake of COVID19, we needed to provide our schools with a way to identify students who would most benefit from additional supports to access distance learning.\nMy colleague, Tvisi Ravi and I presented our approach to developing this tool in partnership with our school teams and provided a demo of how the tool can be used. Lastly, we gave an overview of the impact it has had in our schools so far.\nI’ve preset the video below to begin when we started presenting, but the prior demo from Boulder Valley School District is also worth a watch!"
  },
  {
    "objectID": "posts/student-support-spotlight/index.html#using-demo-data",
    "href": "posts/student-support-spotlight/index.html#using-demo-data",
    "title": "Virtual K-12 Tableau User Group - 14 January 2021",
    "section": "Using Demo Data",
    "text": "Using Demo Data\nOne technique we used in order to easily prepare our student data for a public audience was to obfuscate our student names and alias our school names to quickly make this data anonymous without having to build a tool in parallel or have to duplicate the underlying data source with completely fake data.\nTo accomplish this we used two approaches, in our underlying student data in Schoolzilla there is a ScrambledName field we swapped our actual student names with. Secondly, we used Tableau’s aliasing feature to rename our schools and published the alternate report to our Tableau server as a “Demo Version”.\nThis has also been useful internally for developing screencast tutorials without needing to expose any sensitive student information to the viewers of those tutorials."
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html",
    "href": "posts/word-cloud/word-cloud.html",
    "title": "Creating WordClouds",
    "section": "",
    "text": "Text analysis is hard, but one simple visual tool for extracting insights from dense text are word clouds. Word clouds help identify patterns in the preponderance of certain words in a body of text or within many bodies of text. The more frequently a word appears, the larger the word appears in the word cloud.\nBut manually stripping words from paragraphs of text one by one is time consuming and mind-numbing. In the immortal words of Raymond Hettinger: “There must be a better way!”"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#generating-some-text",
    "href": "posts/word-cloud/word-cloud.html#generating-some-text",
    "title": "Creating WordClouds",
    "section": "Generating Some Text",
    "text": "Generating Some Text\nTo start, let’s create a bunch of text to generate a word cloud from. Using the Faker package, we can create a single column dataframe where each row contains a paragraph of fake/non-sensical text.\nFrom there, we’ll explore a simple way to parse out the frequency of individual words.\n\nfrom faker import Faker\nimport pandas as pd\n\nfake = Faker()\n\ndata = []\n\n# Loop over a range of 5000 records and generate a random paragraph in a key:value pair.\nfor _ in range(5000):\n    data.append(dict(text=fake.paragraph(variable_nb_sentences=True)))\n\n# Convert that list of dictionaries into a pandas dataframe\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n  \n    \n      \n      text\n    \n  \n  \n    \n      0\n      Forget trip paper and Mr around lot.\n    \n    \n      1\n      His include painting success amount child. Who...\n    \n    \n      2\n      Every keep them against build ask. Practice fo...\n    \n    \n      3\n      Worker run particular fish officer son. Reason...\n    \n    \n      4\n      Rest environment must word. Allow brother bene...\n    \n    \n      ...\n      ...\n    \n    \n      4995\n      Product activity paper power factor. Challenge...\n    \n    \n      4996\n      Choice consider deep quality right wrong. Rece...\n    \n    \n      4997\n      Kid car hotel field. So employee degree air de...\n    \n    \n      4998\n      Station authority benefit despite whether. Mov...\n    \n    \n      4999\n      Daughter ability player.\n    \n  \n\n5000 rows × 1 columns\n\n\n\nThis gives us a dataframe of 5000 paragraphs to work with. Not something I’d want to do manually!"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#splitting-paragraphs-into-words",
    "href": "posts/word-cloud/word-cloud.html#splitting-paragraphs-into-words",
    "title": "Creating WordClouds",
    "section": "Splitting Paragraphs into Words",
    "text": "Splitting Paragraphs into Words\nAdmittedly, there may be a more performant way to do this in pandas using vectorization instead of iterating row by row (sort of a no-no in dataframes), but I’m going to keep this simple and concise.\n\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided (using) a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing.\n\nThe general approach here is to create a list to hold our extracted words (word_list), then loop over each paragraph in the dataframe, splitting each word from the paragraph into an item in a list of its own. Then before looping to the next row, extending the original list*.\n* Python extend vs append\n\nword_list = []\nfor index, row in df.iterrows():\n    # Strip out periods and make all words lowercase\n    words = row.text.lower().replace(\".\",\"\").split(\" \")\n    word_list.extend(words)\n\n# Show the first 10 records as example\nprint(word_list[:10]) \n\n['forget', 'trip', 'paper', 'and', 'mr', 'around', 'lot', 'his', 'include', 'painting']\n\n\nThen we’ll convert that list into a dataframe and name the column word for easier reference later.\n\ndf2 = pd.DataFrame({\"word\": word_list})\ndf2\n\n\n\n\n\n  \n    \n      \n      word\n    \n  \n  \n    \n      0\n      forget\n    \n    \n      1\n      trip\n    \n    \n      2\n      paper\n    \n    \n      3\n      and\n    \n    \n      4\n      mr\n    \n    \n      ...\n      ...\n    \n    \n      69266\n      care\n    \n    \n      69267\n      talk\n    \n    \n      69268\n      daughter\n    \n    \n      69269\n      ability\n    \n    \n      69270\n      player\n    \n  \n\n69271 rows × 1 columns"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#data-source",
    "href": "posts/word-cloud/word-cloud.html#data-source",
    "title": "Creating WordClouds",
    "section": "Data Source",
    "text": "Data Source\nAt this point we could load this dataframe into a database (perhaps using SQLSorcery or similar tool) and move on to creating a word cloud in a data viz tool like Tableau.\nBut it would be neat to also do this in python while we’re at it!"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#word-cloud",
    "href": "posts/word-cloud/word-cloud.html#word-cloud",
    "title": "Creating WordClouds",
    "section": "Word cloud",
    "text": "Word cloud\nAfter installing the wordcloud package, we can load our dataframe in and begin visualizing. We’ll want to remove some common words that won’t give us much insight (such as pronouns, conjunctions, and similar filler). For a full list of what we’ll filter out, see below.\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nprint(STOPWORDS)\n\n{'over', \"he'd\", 'we', 'some', 'k', 'further', 'yours', 'has', \"she's\", 'otherwise', \"she'll\", \"he's\", \"i'll\", 'so', 'being', 'to', 'are', \"weren't\", 'each', 'by', 'why', 'that', 'were', 'all', 'through', 'than', 'such', 'the', 'about', 'their', 'between', 'most', 'ours', 'then', 'same', 'or', 'like', 'above', \"couldn't\", \"won't\", 'would', 'who', 'they', 'was', 'where', \"hasn't\", 'since', 'other', 'them', 'when', \"wouldn't\", 'herself', 'itself', \"shouldn't\", 'else', 'myself', 'under', 'only', \"they're\", 'if', \"it's\", \"when's\", 'she', 'there', 'here', 'r', 'how', 'these', 'against', \"here's\", 'no', 'theirs', 'doing', 'ever', 'at', 'her', 'more', \"you're\", 'until', \"what's\", 'me', 'shall', \"we'd\", 'cannot', \"i'd\", 'www', 'he', 'can', 'very', 'it', \"they've\", 'once', 'been', 'and', \"isn't\", 'therefore', 'i', 'could', 'is', 'him', 'hence', \"mustn't\", \"we'll\", \"he'll\", \"i'm\", 'up', 'with', 'be', 'too', 'while', 'whom', 'your', 'again', 'com', \"let's\", 'have', 'in', 'few', 'own', \"we're\", 'below', \"don't\", \"you'll\", 'his', \"they'll\", 'however', \"shan't\", \"we've\", \"can't\", 'during', 'having', 'not', 'yourself', 'you', 'yourselves', \"how's\", 'ought', 'any', \"that's\", \"you've\", 'ourselves', 'on', 'because', 'should', 'also', 'for', \"she'd\", 'into', 'this', \"where's\", \"i've\", \"why's\", 'out', 'himself', 'am', 'do', \"aren't\", 'down', 'what', 'its', 'those', \"doesn't\", \"wasn't\", 'of', 'an', \"haven't\", \"there's\", 'get', \"didn't\", 'themselves', 'but', 'my', 'both', \"they'd\", 'did', 'our', \"hadn't\", 'from', \"who's\", 'off', 'nor', 'before', 'had', 'a', 'after', 'which', 'as', \"you'd\", 'just', 'hers', 'does', 'http'}\n\n\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n\n# Create a word cloud with resolution of 1200 x 800 with a black background\ntext = \" \".join(df2.word.values)\nwordcloud = WordCloud(\n    width=1200,\n    height=800,\n    background_color='black',\n    stopwords=STOPWORDS).generate(text)\n\nAt this point we could spit this out to an image for presentation.\nwordcloud.to_file(\"wordcloud.jpg\")\nBut we could also use a plotting library like matplotlib to build a visualization with our wordcloud.\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=(8, 6), facecolor='k', edgecolor='k')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()"
  },
  {
    "objectID": "posts/word-cloud/word-cloud.html#conclusion",
    "href": "posts/word-cloud/word-cloud.html#conclusion",
    "title": "Creating WordClouds",
    "section": "Conclusion",
    "text": "Conclusion\nAnd there we have it. A simple, repeatable method for extracting word frequency from paragraph text for use in generating word clouds.\nIt really only required about 5 lines of code to extract the data:\n\nword_list = []\nfor index, row in df.iterrows():\n    words = row.text.lower().replace(\".\",\"\").split(\" \")\n    word_list.extend(words)\n\ndf = pd.DataFrame({\"word\": word_list})\n\nAnd only another 10 or so to actually create a word cloud visualization as well!"
  },
  {
    "objectID": "posts/dev-env-setup/index.html",
    "href": "posts/dev-env-setup/index.html",
    "title": "Automating Developer Environment Setup",
    "section": "",
    "text": "I develop on Linux. Ubuntu to be specific. Most of the web apps I build I end up deploying to Heroku. Since their stack is based on Ubuntu and Postgres, I try to make my local environment as close to production as possible.\nThat means I don’t use simpler development databases like SQLite in my local setup and I don’t build on Windows or Mac and then hope my code will deploy properly in production. I don’t want any suprises!\nTo that effect, I try to automate my setup as much as possible.\nFor work, I develop within a Linux virtual machine (using Virtualbox). At home, I’m already using linux as my main os. But I want my setup, tools, and configurations to be identical no matter where I am working. I also want to be able to spin up a new VM at a moment’s notice and be ready to go in no time.\nTo help facilitate that need, I have a github repo where I store my dotfiles and a few shell scripts to easily setup my development environment.\nI prefer to work in the terminal as much as possible and so I use Vim as my text editor with a few custom configurations to emulate some IDE like features.\nI also prefer to handle version control with git in the terminal and have a few custom aliases I like to make sure are available wherever I’m working.\nBut the most valuable script I have is my database setup script. Installing Postgres can be daunting, particularly at the start of a new project. I don’t want that to be a hindrance for myself or other developers on my team. While I have a generic postgres setup script, I also have a more custom one for use in Django web apps that I store in the source code of each app I deploy. That can be a huge help when handing off to another developer on the team, so they have exactly the same database configuration as every other developer working on the project, with single button deployment.\n\nVIM Config\nMy Vim config is pretty simple compared to many I’ve seen, but there are a few things I can’t live without. I was big on Sublime before I moved to Vim and so I really appreciate having a fuzzy finder and file tree available. So I use NERDTree and CtrlP for that. I also like being able to easily comment blocks of code and use NERDCommenter for that.\nAs I’ve shifted to using Python more, I found this tutorial from Real Python really helpful for thinking about how I wanted Vim configured to better support Python specifically. The main call outs are white space config (both indents and trailing whitespace), code-completion, and PEP8 compliance. Using static analysis tools like Flake8 has greatly cut down on the need for those arguments during code review.\nBeyond those Vundle plugins, I keep a copy of my current .vimrc file in my repo for easy syncing. If I make a change locally, I push it to the repo so I can pull it down in any other environment I’m working in. I also like a certain color scheme and run a support script during setup to ensure both my terminal and Vim use the same colors. It cuts down on context switching when I have to hop in and out of my editor to run commands.\n\n\nGit Config\nMy git settings mostly consist of setting up my editor to be Vim by default and a series of helpful aliases. I drew a lot of inspiration from this article from You’ve Been Haacked.\nI use the quick commit and pretty formatted git log constantly. Likewise the grep functionality for the commit history is incredibly useful.\n\n\nBash Config\nMy .bashrc is pretty standard, with one key exception. I really like being able to see the git branch name in my prompt. I incorporated this tip from Coderwall, which couldn’t be any easier. Other than that I mostly have aliases to quickly jump to current project directories I’m working on. Although lately, I’ve been experimenting with some aliases for Django. Typing out python manage.py runserver or even ./manage.py makemigrations all the time can be a little tedious. I know I could use django-shortcuts but it really seems like a bash function ought to be able to handle my needs.\n\n\nPostgres Installation\nWhen I first was working in Rails and trying to set up PostgreSQL, I had a lot of hiccups initially. I’d install with the wrong account permissions, or I was missing some dependency. I’d get it configured, then port to a new system and have to spend time trying to get it all back to working order. It was a serious headache.\nThen, I found this great resource for installing Ruby on Rails on Ubuntu from Go Rails which had a small aside about Postgres. I was inspired by the simplicity of their directions and translated it into a shell script to automate it.\nWhile I have since largely put Rails behind me (I still have a few legacy apps that I maintain), I am absolutely indebted to the advice about databse setup.\nJust to show how simple this is, here is my postgre setup script. It’s 7 lines.\n#!/bin/bash\n\nuser=$(whoami)\n\nsudo sh -c \"echo 'deb http://apt.postgresql.org/pub/repos/apt/ xenial-pgdg main' > /etc/apt/sources.list.d/pgdg.list\"\nwget --quiet -O - http://apt.postgresql.org/pub/repos/apt/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install -y postgresql-common\nsudo apt-get install -y postgresql libpq-dev\n\nsudo -u postgres createuser $user -s\nIt does all the heavy lifting. Figures out what account I am installing with, gets the necessary repository info, installs postgres, and even sets up a user based on my account to access it with.\nI’ve taken this idea a little further for Django and now run something a little more complicated.\n#!/bin/bash\n\n# Exit if command fails\nset -e\n# Treat unset variables as errors\nset -u\n\n# Set user as current account\nuser=$(whoami)\n\n# Install Postgres 10\nsudo sh -c \"echo 'deb http://apt.postgresql.org/pub/repos/apt/ xenial-pgdg main' >> /etc/apt/sources.list.d/pgdg.list\"\nwget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install -y postgresql-common\nsudo apt-get install -y postgresql-10 postgresql-contrib libpq-dev\n\n# Create superuser account as self for local management\nsudo -u postgres createuser $user -s\n\n# Set env vars for colors\nYELLOW='\\033[1;33m'\nNC='\\033[0m'\n\n# Collect arguments from user\n# Project specific values\nprintf \"${YELLOW}Database name:\\n${NC}\"\nread database\nprintf \"${YELLOW}Username:\\n${NC}\"\nread username\nprintf \"${YELLOW}Password:\\n${NC}\"\nread password\n\n# Create database and user\nRUN_ON_PSQL=\"psql -X -U $user --set ON_ERROR_STOP=on --set AUTOCOMMIT=off postgres\"\n$RUN_ON_PSQL <<SQL\nCREATE DATABASE $database;\nCREATE USER $username WITH PASSWORD '$password';\nALTER ROLE $username SET client_encoding TO 'utf8';\nALTER ROLE $username SET default_transaction_isolation TO 'read committed';\nALTER ROLE $username SET timezone TO 'UTC';\nGRANT ALL PRIVILEGES ON DATABASE $database TO $username; \nALTER USER $username CREATEDB;\ncommit;\nSQL\n\nexit 0\nNow my team can clone my repo, run this app and be good to go in seconds. It could not be any easier to hand off my code!\n\n\nConclusion\nThere are always ways to automate your workflow. I am constantly on the look out for ways I can take the guess work out of my development configuration.\nThere are other things I rely on that I haven’t mentioned here in detail (but you can find them in my repo) like installing Heroku CLI or ngrok.\nI keep adding more automation to my setup as I go. In fact, I’m pretty sure that’s my most active public repo!"
  },
  {
    "objectID": "posts/how-memes-inform-my-work/index.html",
    "href": "posts/how-memes-inform-my-work/index.html",
    "title": "How Memes Inform My Work",
    "section": "",
    "text": "I remember fondly how excited I was the first time I understood an obscure coding joke on r/programmerhumor. It felt like finally being in “the club” and not feeling like such an outsider.\nWhile I’ve been programming in some capacity or another for over 20 years, I didn’t get here through the regular channels. I didn’t get a degree in Computer Science. I’ve never worked at a startup or one of the Big 4 (or is it Frightful 5 now?). And as a result, I (like many programmers) often succumb to bouts of Imposter Syndrome. So finally feeling like I was part of the in-crowd was a victory in and of itself back then.\nPart of that victory is not only understanding those in-jokes and memes, but also internalizing many of the lessons they have to teach. I’ve definitely purloined many a meme, joke, or comic along the way and made it my own.\n\nOk. Not like that. I don’t mean, I’ve claimed ownership in the conventional sense, but I mean I have taken quite a few to heart in ways that make them a part of who I am professionally and philosophically.\nWhat follows is my attempt to call out some of those that have impacted me the most, as well as explain why so many of these are pinned to the walls of my cubicle.\n\nPick Two\n\nThis is probably one of the first memes I actually felt spoke to me and my frustrations with the limitations of project work in the software world. I remember just going, “Yeah. That.” And then feeling a deep need to print it out, stick it to the wall of my cubicle for reference the next time somebody asked for the impossible. Also known as all three.\nAll too often, we as developers, are asked to build quality products while still meeting deadlines and budgets with limited resources. Because so much of what we do is invisible to stakeholders and end-users, it is often hand-waved away as “magic” while somehow also simultaneously being dismissed as the type of work you can just hand off to the interchangable cogs. And if they can’t meet the feature requests then it’s likely just their competency that’s at issue.\nWe’ve all been spoiled by the free high-tech apps we consume on a daily basis and assume similar quality can be built by tiny teams. I am often comforted in those moments by the fundamental laws of production this meme espouses.\n\n\nThe Two States of Every Programmer\n\nWhile we’re on the topic of imposter syndrome and our own limitations, this meme is one I identify with all the time. It doesn’t matter how good you get at programming. The field of what we know and what is possible is constantly shifting and growing. And there isn’t a day that goes by while debugging something or trying to get a new feature implemented that I don’t find myself osciallating between these states.\nIt’s important to remember we all feel this way and to remind ourselves of the opposite state when we’re stuck in the other.\nI think what truly separates those that drown in the imposter syndrome river and those that keep paddling on is a healthy level of the growth mindset. I’m not afraid of failure and feeling like that confused dog, if every now and then I crack some puzzling code and feel lke a damn genius. That dopamine rush, however temporary or illusory, is a bit of an addiction. One I am more than happy to indulge at the cost of a little ego-death now and then.\n\n\nIs it Worth the Time?\n\nThere are plenty of XKCD comics with jokes and references that fly right over my head, but this one not only speaks to me, it is damn well a guidepost in my work. This one is likewise hanging above my desk as a constant reminder and reference.\nI spend a lot of time automating tasks. My own, my teams, and more often than not, those of other departments in the org. The automation of boring stuff is one of the most powerful impacts of code and one that I think gets at the heart of why we do what we do.\nThat being said, it’s incredibly easy to fall down your own rabbit hole trying to build the perfect solution in search of time savings. At the start of every new project request, I consult this chart and think deeply about the time being invested in the solution versus the time it’s actually saving. Return on investment is an important concept to grasp early on as a developer, because we’re inquisitive types who are more than willing to live in that damn rabbit hole if it means catching a white rabbit. Even if he is just a figment of our imagination.\n\n\nYou Don’t Have to Solve Everything Today\n\nI’m pretty sure this meme’s original context was in regards to depression, but one that nonetheless serves as a daily reminder to walk away sometimes. It’s easy to get so wrapped up in a problem to completely lose track of time. I can be hacking away while the office slowly clears out. It’s easy to think “I’ll go home once I solve this problem, or make this commit” but that can spiral out of control fast.\nThe older I get and the more my priorities focus on family and sustaining a healthy work/life balance, the more I look to this meme as a reminder that it really can “wait until tomorrow”.\nRemember, burn out is real and no one says on their death bed that they wish they had worked more. In the work/life balance equation, you’ve got to work to live not live to work. i\nIt’s corny. I know. But that doesn’t make it any less true.\n\n\nExploits of a Mom (AKA Little Bobby Tables)\n\nI started my professional career as a programmer largely working with SQL. And it’s still a daily part of my workload. Security has been everyone’s minds in the last few years with all the data breaches and hacks. Education is not immune either, and this comic about SQL injection is a good reminder that it’s important to tighten up those holes in your apps lest someone exploit them for fun or profit!\nEven in 2017, SQL injection was still the number one risk in the OWASP Top Ten Most Critical Web Application Security Risks.\n\n\nOffice Space Lumberg\n\nI don’t think I truly appreciated the brilliance of Mike Judge’s Office Space until I started to work in an office job. Although, even as a teacher I identified with beating the crap out of a copier/fax machine. I laughed at all the jokes and obvious exaggerations, but now I find myself trying to figure out which of the characters I am and which ones I never want to be.\nThis is particularly true now that I’ve moved into a role as a manager. Lumberg is the quintessential terrible boss and every time I find myself leaning on one of my teammate’s cubicles with coffee cup in hand, I can’t help feeling like I might accidentally utter, “Yeah if you could just… that’d be great.”\nDon’t be that guy. Don’t be that guy. Don’t be that guy.\nI joke about it practically every time, but I really have internalized the idea that being someone’s boss isn’t about assigning them work and cracking the whip. That never helped me as an employee and I would never want to be that to someone else.\nI’m still trying to find my way as I transition into this role, but so far, I’m finding that leading from the front is much more effective than shouting orders from the back. “Come with me” is so much more potent than “Go that way”.\nWhat memes have you taken to heart in your work or personal life? Which comics or jokes do you comfort yourself with in your moments of desparation or exhilaration?"
  },
  {
    "objectID": "posts/powerschool-api.md/index.html#project-summary",
    "href": "posts/powerschool-api.md/index.html#project-summary",
    "title": "Student Email Sync",
    "section": "Project Summary:",
    "text": "Project Summary:\n\nBusiness Objective:\nTo pull automatically generated student email addresses from our MS SQL data warehouse and push into production PowerSchool instances in multiple states.\n\n\nWhat I Built:\nA ruby script that connects to our data warehouse using an ActiveRecord adapter for SQL server, dynamically queries student records, and converts them into the expected format with a custom JSON serializer. Then it uses Faraday HTTP client to make a call to the PowerSchool API (one per instance in CA and TN) to request a token for authentication. Once authenticated, it sends the JSON request body to the API and returns a response set of success/error messages which are logged locally. The program is integrated into our SSIS integration job server using a nice little PowerShell script to dynamically call the ruby script for each state’s data.\nTL;DR: Wrote a ruby script that syncs 15,000+ student emails between a data warehouse and production PowerSchool servers in mutliple states on a daily basis.\n\n\nMade With:\n\nRuby\n\nFaraday\nRails-sqlserver adapter\nLogger\n\nPowerSchool API\nMS SQL\n\nStored procedure\nCustom views\n\nPowerShell\nSSIS\n\n\n\nCode Samples:\nSelect methods from the API class demonstrating how the program connects to the PowerSchool API and updates student emails.\n# @oauth is a class variable that stores the configuration file secrets\n# not exposed here for obvious reasons\n\nprivate\n  def connect\n    # open HTTP connection\n    Faraday.new(:url => @oauth['url'],:ssl => {:verify => false}) do |faraday|\n      faraday.request  :url_encoded               # form-encode POST params\n      faraday.adapter  Faraday.default_adapter    # make requests with Net::HTTP\n    end\n  end\n\nprivate\n  def access_token\n    result = connect.post do |request|\n      request.url \"/oauth/access_token\"\n      request.params['client_id'] = @oauth['id']\n      request.params['client_secret'] = @oauth['secret']\n      request.params['grant_type'] = 'client_credentials'\n    end\n    \n    # return token from JSON result\n    JSON.parse(result.body)['access_token']\n  end\n\npublic\n  def update_student_emails(body) # takes JSON as the body param\n    # POST request body of student emails to PowerSchool API\n    result = connect.post do |request|\n      request.url \"/ws/v1/student\"\n      request.options.timeout = 3000 # open/read timeout in seconds\n      request.options.open_timeout = 10 # connection open timeout in seconds\n      request.headers['Accept'] = 'application/json'\n      request.headers['Content-Type'] = 'application/json'\n      request.headers['Authorization'] = \"Bearer #{access_token}\"\n      request.body = body\n    end\n\n    # Return human readable JSON result\n    parsed = JSON.parse(result.body)\n    pretty = JSON.pretty_generate(parsed)\n  end\nRuby method for seralizing student emails from SQL to JSON. The PowerSchool Student API expects JSON in a non standard JSON format. That is why custom serialization is needed.\nSee PowerSchool API Documentation under “Update Students > Request Body Example”\ndef self.serialize_student_emails(state)\n  # client_uid is the PowerSchool ID and id is the DCID\n  studentArray = []\n  connect('production')\n  query = sanitize_sql_array(\n    [\"SELECT * FROM dbo.StudentEmails WHERE State = ?\", state]\n  )\n  connection.select_all(query).each do |record|\n    studentArray << {\n      'client_uid' => record['client_uid'], \n      'action' => 'UPDATE', \n      'id' => record['id'], \n      'contact_info' => {'email': record['email']}\n    }\n  end\n\n  studentHash = {'students' => {'student' => studentArray}}\n  \n  # return as JSON body\n  studentHash.to_json\nend\nPowerShell script: ssms_job.ps1\n# This is run from an SSMS job task step\n$erroractionpreference = \"Stop\"\n\ntry {\n  Set-Location 'C:\\powerSchoolApi\\app'\n  \n  # Ruby executable location called from Rails Installer\n  $RUBY = 'C:\\RailsInstaller\\Ruby2.2.0\\bin\\ruby.exe'\n  & $RUBY 'application.rb'\n}\ncatch {\n  throw 'The application encountered an error. See logs for further explanation.'\n  exit 1\n}"
  },
  {
    "objectID": "posts/python-training/index.html",
    "href": "posts/python-training/index.html",
    "title": "I Can Python, So Can You",
    "section": "",
    "text": "Before I left Aspire, I had started an internal training on Python for those in the org who were interested in learning to do a bit of coding. Inspired by the tag line from an old favorite cooking show from the 1980s, Yan Can Cook, I titled the course “I Can Python, So Can You”.\nThe scope of that original course was heaviliy inspired by Al Swiegart’s Automate The Boring Stuff With Python and was focused on building skills around office task automation. My colleague, and good friend, Jason Baek helped me design the course and content and we co-taught it during our time there.\nWhen I left Aspire and began work at KIPP Northern California, there was once again an interest from colleagues to learn a little python. So I updated the course, added some additional homework and projects, and taught it once again. I learned a lot that time around and got some great feedback from my data team about the skills they were most interested in (such as learning pandas, APIs, and data visualization).\nIn the summer of 2019, I was asked by the KIPP Foundation to design a single day version of the course for a data convening in Chicago. I recruited my data engineering colleague, Charlie Bini, to help me build out a streamlined version of the content and to co-teach with me at the convening. We reworked the content to incorporate a more data-focused curriculum and taught it over the course of a six-hour session at the convening. We repeated the course with further updates the following year, although we did it remotely as we were in the midst of the pandemic.\nOne of the core ideas in all of the incarnations was that the environment setup and IDE learning curve would be a roadblock for many newbies and we didn’t want that to stand in the way of jumping in and learning to code in python. So we reached for Google Colab, which had the ease of Jupyter Notebooks but with an in-browser, cloud hosted environment, and was shareable like any other Google Doc.\nI’m hoping to write a new version of these lessons soon, but in the meantime I wanted to get them up on my blog as a reference for folks who ask about learning resources and to hopefully cultivate some additional feedback that I can incorporate in the next iteration.\nWithout further ado, here are the links to the colab notebooks. I hope they are useful to anyone who finds this post.\n\nI Can Python, So Can You\n\nIntroduction\nFundamentals\nControl Flow\nLoops & Lists\nRecap & APIs\nPandas\nCourse Capstone: Star Wars"
  },
  {
    "objectID": "posts/manager-readme/index.html",
    "href": "posts/manager-readme/index.html",
    "title": "My Manager README",
    "section": "",
    "text": "When I transitioned from being primarily a software engineer who was an individual contributor on a team to managing a team of engineers, I didn’t have a clear sense of who I was going to be as a manager. I knew the sort of manager I wanted to be, but the shift from peer to leader isn’t always clear cut.\nAfter close to two years in the role, I learned a lot about who I am and who I still want to be as a leader, but had no clear way to communicate that to my team.\nOne of the engineers on our team left the organization after five years in the role, and while hiring their replacement I took the opportunity to get a fresh start and reset.\nAbout six months prior I had come across a few posts about the practice of creating a manager readme as a way to reflect on who you are as a leader as well as provide new hires a sort of guidebook or operating manual for how to work with you as their manager.\nI wanted to start my new engineer off with not only a clear understanding of how I saw my role but also what I expected out of them.\nTo do this, I looked over many examples of great manager readmes from top tech companies to draw inspiration from.\nUltimately, I wanted this document to be both explanatory and inspirational for my team as well as aspirational for myself. A sort of contract I could hold myself to. So that my team could say, “You said you were going to do X.” And I could make sure I was holding myself up to that same standard.\nSo, I drafted a welcome letter and appended my Manager README in the true python spirit of: > explicit is better than implicit\nBeing a programmer, I also couldn’t help seeing this as yet another program to write. A set of instructions for myself to operate by. So, of course, I had to check it into source control and track changes. Like any other codebase, I knew it would inevitably evolve and grow. And I wanted to be able to git blame myself if it went off the rails. =)\nWithout further ado, here’s what I’ve come up with, so far:"
  },
  {
    "objectID": "posts/manager-readme/index.html#my-role",
    "href": "posts/manager-readme/index.html#my-role",
    "title": "My Manager README",
    "section": "My Role",
    "text": "My Role\n\nI strive to be a servant leader: I believe managers work for their teams, not the other way around\nI am an advocate for you and the team with the rest of the organization\nI will set the context for your work, not tell you how to do it\nI want to support your growth and provide you with opportunities to learn and apply your skills\nI am also an individual contributor on this team and hold myself accountable to the same expectations I set for the team"
  },
  {
    "objectID": "posts/manager-readme/index.html#my-availability",
    "href": "posts/manager-readme/index.html#my-availability",
    "title": "My Manager README",
    "section": "My Availability",
    "text": "My Availability\n\nWhile I usually have a lot on my plate, very little is more important to me than talking to you if you need to talk. If you want to talk, let’s talk.\nIf you have questions, have a roadblock in your work, or just need a thought partner I’d love to hear about it as soon as possible. You don’t have to wait until our scheduled one-on-one meetings. Come by my desk, stop me in the hall, catch me on slack, call me, or just put some time on my calendar anywhere that’s open.\nIf my schedule is blocked when you need me, let me know so I can make sure we find time to connect\nMy commute is hell. I tend to work from home frequently and work non-standard hours. My regular schedule is M-W-F in office from 9:30/10-6ish and T-TH at home 8ish-5ish. If you need to schedule something in person, early morning, let me know and I will take extras steps to be there on time."
  },
  {
    "objectID": "posts/manager-readme/index.html#my-assumptions-about-you",
    "href": "posts/manager-readme/index.html#my-assumptions-about-you",
    "title": "My Manager README",
    "section": "My Assumptions About You",
    "text": "My Assumptions About You\n\nYou are agile and adaptable. We wear many hats and our roles and work evolve as the organization’s needs change. I will expect you to change gears as needed and work on things you may have no prior experience in. Innovation means trying new things and not always knowing how to do it right away.\nYou are a directly responsible individual. I expect you to take ownership for your work and your mistakes. I will create a safe space for productive failure as well as give you credit for your successes.\nYou are good at your job. I wouldn’t have hired you if I thought otherwise. If it feels like I’m questioning you it’s because I am trying to gather context or trying to be a sounding board and rubber duck for your ideas. However, you know best how to do your work. I’ll ask questions and vet your ideas, but won’t override your decisions. I will expect you to own those decisions though.\nYou feel safe debating with me. I believe in productive disagreement. I find that ideas improve by being examined from all angles. If it sounds like I’m disagreeing I’m most likely just playing devil’s advocate and want you to articulate and refine your position. This does rely on us being able to have a safe debate. Sometimes the right answer is one neither of us started with."
  },
  {
    "objectID": "posts/manager-readme/index.html#values",
    "href": "posts/manager-readme/index.html#values",
    "title": "My Manager README",
    "section": "Values",
    "text": "Values\n\nI value your transparency. I want you to keep me in the loop and let me know about issues as they arise. Be transparent with me. I will do the same for you.\nI value your learning. I know that it takes time to learn new things and that novel projects will require additional time to experiment and iterate.\nI value your time. I don’t want you to waste your time on busy work. I want you to provide value and feel valued. I want you to have pride in your work and feel inspired and purposeful in the work you do here. That doesn’t mean we won’t ever have a slog or work we have to power through. However, if you feel like you are wasting your time on menial work, let’s find a way to automate it!\nI value your input. I put processes in place to make us more effective. I don’t believe in just going through the motions or process for the sake of process. If you think something is getting in the way of your work, let me know. I want to improve our processes so they work effectively for all of us."
  },
  {
    "objectID": "posts/manager-readme/index.html#focus",
    "href": "posts/manager-readme/index.html#focus",
    "title": "My Manager README",
    "section": "Focus",
    "text": "Focus\n\nThe quality of our development work increases with the amount of concentration we give it.\nAttention is a valuable resource. Be prepared to defend it. Deep work is vital to quality.\nWe work in an open space. Feel free to use headphones or move to a quiet space if you need to concentrate. I find I do some of my best coding when I’m not at the office.\nYou are not paid to maintain your inbox or attend meetings. Block off time on your calendar if you need to. It’s okay to say no to an immediate request for your attention or to ask to circle back later.\nTroubleshooting a problem with a teammate, though, is a valuable exercise. There are always things to learn from unplanned collaboration. Be open to help others when they need it and they will return the favor. Just be judicious with how much of your time you dole out for interruptions."
  },
  {
    "objectID": "posts/manager-readme/index.html#feedback",
    "href": "posts/manager-readme/index.html#feedback",
    "title": "My Manager README",
    "section": "Feedback",
    "text": "Feedback\n\nRadical Candor makes a lot of sense to me. I want to give you feedback because I care about you and want to help you grow.\nI prefer to offer and receive direct feedback in a timely manner. The tighter the feedback loop, the better.\nI expect people to call BS when they see it and not be afraid to speak up.\nFeedback is best when it is actionable and explicit about what improvement looks like."
  },
  {
    "objectID": "posts/manager-readme/index.html#worklife-balance",
    "href": "posts/manager-readme/index.html#worklife-balance",
    "title": "My Manager README",
    "section": "Work/Life Balance",
    "text": "Work/Life Balance\n\nI want you to work smarter, not harder. Don’t burn yourself out. There is always more work to do. You don’t have to solve everything today.\nI believe in working to live, not living to work. We are mission driven and everyone wants to give 110%, but we have to be mindful of the stress this imbalance can cause.\nUnless there’s an emergency, I don’t expect to communicate with you outside of business hours. If you get an email from me late at night or first thing in the morning, I don’t expect you to respond immediately. That’s me working when I need to. Pick it up when you get a chance during your preferred schedule. If I call your cell or text you directly, it’s likely urgent. Please respond accordingly.\nMost people work somewhere between 7am (earliest) to 7pm (latest). Figure out what works for you and let me know. I’m flexible about your schedule and know that things come up that may require coming in late or leaving early.\nI support working from home. It’s good for mental health, supports deep work and productivity, and is definitely a quality of life issue for me. I also recognize the value of in person meetings, face-to-face collaboration, and informal discussion. We are not full-time remote."
  },
  {
    "objectID": "posts/manager-readme/index.html#one-on-ones",
    "href": "posts/manager-readme/index.html#one-on-ones",
    "title": "My Manager README",
    "section": "One on Ones",
    "text": "One on Ones\n\nI think 1:1s are important and want us to check in regularly. I will strive to prioritize our 1:1s ahead of other commitments and be there on-time and committed to listening. I will give you my full attention.\nThese meetings are for you and I expect you to set the agenda. It’s not just a time for status updates (although I am happy to chat about your project status). If I have things I want to ask you, I will, but this is your time.\nI think hard discussions are frequently facilitated by taking a walk. That doesn’t mean if I ask you to go for a walk that something is wrong though. I like stretching my legs and walking meetings can get the mind moving in different directions. I am also open to taking a meeting over the ping-pong table or grabbing some coffee.\nFor remote 1:1s, I have a difficult time focusing and connecting unless I can see your face over video. If you’re doing the whole work in your pajamas thing, fine by me. You don’t have to dress up.\nI expect to check in at least once per sprint, usually at the mid way point. But the length, frequency, and medium are up to you. I will carve out at least 60 minutes for these but if you need less time or want to run over, let me know."
  },
  {
    "objectID": "posts/manager-readme/index.html#relationships",
    "href": "posts/manager-readme/index.html#relationships",
    "title": "My Manager README",
    "section": "Relationships",
    "text": "Relationships\n\nI think strong teams come from great working relationships. I encourage you to connect with your peers as well as other teammates across Aspire.\nA former Aspire teammate once coined the term frolleague(friend-colleague) and I find it very fitting. I consider a lot of current and former teammates to be friends. I hang out with folks at Aspire on my personal time and am usually up for happy hour. There is no expectation that anyone here is your new best friend, but I do think that the presence of someone you work with you consider a friend to be a strong indicator of healthy working relationships.\nI’m happy to make introductions for you or provide networking suggestions to help you do that.\nI think it is important to get out to school sites and see the work of the organization in action. I will make efforts to provide you opportunities to do so as soon as possible."
  },
  {
    "objectID": "posts/new-years-resolutions/index.html",
    "href": "posts/new-years-resolutions/index.html",
    "title": "Resolutions for the New Year",
    "section": "",
    "text": "Well, it’s 2018 and it’s been two years since I last made an update to my blog. I originally published it to test out Jekyll and to put something basic on my personal domain. I was working with Ruby on Rails at the time and Jekyll seemed the natural choice.\nIn the intervening time, my work has shifted and we are more and more aligning on standardizing our entire stack to be Python based (the advantages of which, I will expound upon in another post). With that has come a desire to teach more of my colleagues (particularly those not directly involved in programming) to take advantage of Python in their work. Data analysts can leverage it for data science and predictive analytics work and back-office folks like our accountants and financial analysts can leverage it for task automation.\nI’ve also been getting requests from some friends and family to teach them a bit of programming and Python seems like a natural choice for beginners. With that in mind, I’ve made it my New Year’s resolution to teach as many people as possible this year to program.\nI’m rebooting my blog to post helpful resources and connect with folks interested in learning and that means giving my site a much needed boost of new energy. I’ve rebuilt it from the ground up in Pelican so that it’ll be in Python as well.\nHere’s to 2018, the year of Python!"
  },
  {
    "objectID": "posts/simple-api/index.html",
    "href": "posts/simple-api/index.html",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "",
    "text": "Suppose you have a database with tables and views that you want to expose as JSON through a REST API but you don’t want to couple the database tables with object models in your API but rather have it simply expose the data for querying (read-only).\nHow could this be done easily and with minimal code?"
  },
  {
    "objectID": "posts/simple-api/index.html#the-approach",
    "href": "posts/simple-api/index.html#the-approach",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "The Approach",
    "text": "The Approach\nFlask has to be the simplest web app microframework I’ve ever dealt with (though Node’s Express and Ruby’s Sinatra certainly come close). It has a great function jsonify that makes it dead simple to serve a JSON response to a client request.\n# app.py\nfrom flask import Flask, jsonify\n\napp = Flask(__name__)\n\n@app.route(\"/api/\")\ndef index():\n    data = {\n        \"name\": \"dchess\",\n        \"text\": \"Hello, World\",\n    }\n    return jsonify(data)\nSpin it up on localhost:5000/api/ and voila, data!"
  },
  {
    "objectID": "posts/simple-api/index.html#here-comes-the-sorcery",
    "href": "posts/simple-api/index.html#here-comes-the-sorcery",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Here comes the Sorcery",
    "text": "Here comes the Sorcery\nThat’s simple enough but how do we substitute the hard coded dictionary for a database query? This is where SQLAlchemy and Pandas come in handy.\nI almost always want to use these two packages together and so a while back I created a pypi package to provide a simple facade with some syntactic sugar to make that even simpler called SQLSorcery. It’s built on top of both of them and has a simple way to optionally install database adapter packages like pyodbc.\nThat allows me to easily pass in my database connection credentials with environment variables and make sql queries using the Pandas .read_sql_query() method."
  },
  {
    "objectID": "posts/simple-api/index.html#environment",
    "href": "posts/simple-api/index.html#environment",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Environment",
    "text": "Environment\nI’m a fan of Pipenv. It’s so simple to install python package dependencies into a local virtual environment and also handle environment variables from a .env file. Effectively combining all the functionality of pip, venv, and python-dotenv.\n$ pipenv install Flask, sqlsorcery[mssql]\nAnd just like that we’re ready to develop.\nLet’s add some environment variables to a .env file to start:\nDB_SERVER=\nDB=\nDB_SCHEMA=\nDB_USER=\nDB_PWD=\n\nFLASK_APP=app.py"
  },
  {
    "objectID": "posts/simple-api/index.html#querying-our-data",
    "href": "posts/simple-api/index.html#querying-our-data",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Querying our Data",
    "text": "Querying our Data\nThen we can return data from a table as simple as:\nfrom sqlsorcery import MSSQL\nimport pandas as pd\n\ndb = MSSQL()\n\ndata = pd.read_sql_table(\"your_table_name\", con=db.engine, schema=db.schema)\nIt’s that simple!\nTo quickly convert that dataframe into a list of dictionaries (to return as JSON), we can use the Pandas to_dict() method.\ndata = data.to_dict(orient=\"records\")\nWhat if we want to list all the tables in our database schema? Simple!\ntables = db.engine.table_names(schema=db.schema)\nWith those two approaches and Flask’s jsonify we have everything we need to make a quick, easy, and minimal API on top of any tables in our database schema."
  },
  {
    "objectID": "posts/simple-api/index.html#a-user-interface",
    "href": "posts/simple-api/index.html#a-user-interface",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "A User Interface",
    "text": "A User Interface\nWhile we can create an API for machines to read from, it’d be nice to have at least an index of tables that a human can read, navigate, and learn what data exists before pointing tools like curl, postman, or Requests at it.\nNo worries! A tiny jinja HTML template should suffice for a quick list of table names linked to their api endpoint will be just minimal enough to work!\n# templates/index.html\n<ul>\n    {% for table in tables %}\n    <li><a href=\"/api/{{ table}}\">{{ table }}</a></li>\n    {% endfor %}\n</ul>"
  },
  {
    "objectID": "posts/simple-api/index.html#putting-it-all-together",
    "href": "posts/simple-api/index.html#putting-it-all-together",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Putting it all together",
    "text": "Putting it all together\nAt this point we should have a file directory that looks like this:\n.\n├── Pipfile\n├── .env\n├── app.py\n└── templates\n    └── index.html\nLet’s finish off our app.py and give it a test run:\n# app.py\n\nfrom flask import Flask, jsonify, render_template\nfrom sqlsorcery import MSSQL\nimport pandas as pd\n\napp = Flask(__name__)\ndb = MSSQL()\n\n\n@app.route(\"/api\")\ndef index():\n    tables = db.engine.table_names(schema=db.schema)\n    return render_template(\"index.html\", tables=tables)\n\n\n@app.route(\"/api/<table>\", methods=[\"GET\"])\ndef endpoint(table):\n    data = pd.read_sql_table(table, con=db.engine, schema=db.schema)\n    data = data.to_dict(orient=\"records\")\n    return jsonify(data)\nSpin it up on localhost:5000/api/ and explore your data!\n$ pipenv run flask run"
  },
  {
    "objectID": "posts/simple-api/index.html#conclusion",
    "href": "posts/simple-api/index.html#conclusion",
    "title": "Auto-generated REST API for MS SQL database",
    "section": "Conclusion",
    "text": "Conclusion\nI wouldn’t take this and deploy it to production anywhere without some serious security decisions, but it makes for a nice proof-of-concept. And certainly could be expanded to include user authentication, a proper production server like gunicorn and a nicer user interface. But at that point you might be better off with Flask-API or Django REST Framework, both of which I’ve used with success.\nBut still, not bad for less than 20 lines of code.\nAll the code for this blog can be found on my github. Feel free to fork and use it however you like.\nSQLSorcery is also MIT licensed and free to use. It’s in active development and contributors are welcome."
  },
  {
    "objectID": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html",
    "href": "posts/parsing-texas-assessment-data/parsing-texas-assessment-data.html",
    "title": "Parsing Texas Assessment Data",
    "section": "",
    "text": "Data files available here on the TEA website.\nThe STAAR data files contain results for 60+ student demographic groups and each variable is repeated in a separate column for both demographic and test subject. As you will see below, this results in data files that are several thousand columns wide and are a different schema for every grade.\nThese files are designed for use in SAS and SPSS, and the website itself indicates that the number of variables in these files is too great to import into Microsoft Access or some versions of Microsoft Excel without significant truncation.\nHowever, my goal was to pivot this data into a standardized format and load into BigQuery for cross analysis with other state assessment data. What follows is my approach in Python, but I am sure there are other potentially simpler methods than mine, I hope this demonstration helps other approach this dataset for analysis and possibly encourages others to improve upon this method.\nThe original files are linked below for easy access.\nCampus level data for Grades 3-8\n\nGrade 3: test data | variable list\nGrade 4: test data | variable list\nGrade 5: test data | variable list\nGrade 6: test data | variable list\nGrade 7: test data | variable list\nGrade 8: test data | variable list\n\n\n\nBefore determining a solution and final schema, I wanted to understand the structure of these files. This can be accomplished by looping over the linked dat files and getting their shapes.\n\nimport pandas as pd\nfrom pathlib import Path\nimport xlrd\nimport os\n\n# Display options for pandas data frames\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\npd.set_option('max_seq_item', None)\n\n\nbase_url = \"https://tea.texas.gov/sites/default/files/\"\ndat_files = [\"cfy21e3.dat\", \"cfy21e4.dat\", \"cfy21e5.dat\", \"cfy21e6.dat\", \"cfy21e7.dat\", \"cfy21e8.dat\"]\n\nfor dat_file in dat_files:\n    url = f\"{base_url}{dat_file}\"\n    content = Path(url).stem\n    df = pd.read_csv(url)\n    print(content, df.shape)\n\ncfy21e3 (4643, 2093)\ncfy21e4 (4622, 3075)\ncfy21e5 (4415, 3197)\ncfy21e6 (2700, 2093)\ncfy21e7 (2337, 3075)\ncfy21e8 (2370, 4301)\n\n\nEach file is named with this convention (as far as I can discern): - c: Campus level data - fy21: Fiscal year 2021 - e#: English grade # (English, because grades 3-5 also include Spanish results)\nThe numbers in the accompanying parenthesis represent the count of rows and columns, respectively.\n\n\n\nWe can see from this quick file inspection that combining these datasets into a single unified structure will require more than just concatenation. The number of columns is jagged/inconsistent. Reviewing the variable files shows that each grade has differences in the variables reported. Some grades have Reading and Mathematics, while other include a Writing test as well. There are additional differences, but that is one of the most common.\nThey are so wide, because each variable such as # Tested is repeated for each subject and each student group (60+ in total). We can also see that the naming of these columns is encoded/abbreviated, such as r_all_d for # Tested -- Reading -- All Students or r_eth2_d for # Tested -- Reading -- Two or More Races Students.\n\n\n\nThere is something of a pattern to this naming convention you may have noticed: - r: reading - all: all students - d: # tested\nThis becomes more obvious with subsequent examples such as r_all_unsatgl_nm for # Did Not Meet Grade Level Performance -- Reading -- All Students.\n\n\n\nMy initial thought was to split these variable names using the the _ as a delimiter, but this was met with limited success, again due to inconsistent usage.\nNote: You will see the following warning in places WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero. This is due to an encoding issue with the TEA provided files and can be ignored as it does not impact our ability to parse the files.\n\ngrade3_vars = 'https://tea.texas.gov/sites/default/files/fy21_varlist_g03.xls'\n\ndf = pd.read_excel(grade3_vars)\ndf.head(20)\n\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n    \n  \n  \n    \n      0\n      campus\n      Character\n      Campus Number\n    \n    \n      1\n      year\n      Character\n      Test Administration Year\n    \n    \n      2\n      region\n      Character\n      Education Service Center (Region) Number\n    \n    \n      3\n      district\n      Character\n      District Number\n    \n    \n      4\n      dname\n      Character\n      District Name\n    \n    \n      5\n      cname\n      Character\n      Campus Name\n    \n    \n      6\n      grade\n      Character\n      Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n    \n    \n      7\n      r_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Reading -- All Students\n    \n    \n      8\n      r_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Reading -- All Students\n    \n    \n      9\n      r_all_oth_n\n      Numeric\n      # Other - Not Tested -- Reading -- All Students\n    \n    \n      10\n      m_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Mathematics -- All Students\n    \n    \n      11\n      m_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Mathematics -- All Students\n    \n    \n      12\n      m_all_oth_n\n      Numeric\n      # Other - Not Tested -- Mathematics -- All Students\n    \n    \n      13\n      r_all_d\n      Numeric\n      # Tested -- Reading -- All Students\n    \n    \n      14\n      r_sexm_d\n      Numeric\n      # Tested -- Reading -- Male Students\n    \n    \n      15\n      r_sexf_d\n      Numeric\n      # Tested -- Reading -- Female Students\n    \n    \n      16\n      r_sexv_d\n      Numeric\n      # Tested -- Reading -- No Sex Info Students\n    \n    \n      17\n      r_ethh_d\n      Numeric\n      # Tested -- Reading -- Hispanic/Latino Students\n    \n    \n      18\n      r_ethi_d\n      Numeric\n      # Tested -- Reading -- American Indian or Alaska Native Students\n    \n    \n      19\n      r_etha_d\n      Numeric\n      # Tested -- Reading -- Asian Students\n    \n  \n\n\n\n\nIt appears at first glance, that the pattern is subject, student demographic group, variable type. However, we can see this doesn’t remain consistent.\n\ndf.tail(15)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n    \n  \n  \n    \n      2078\n      m_spen_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Special Ed Students\n    \n    \n      2079\n      m_spev_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Special Ed Students\n    \n    \n      2080\n      m_spev_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Special Ed Students\n    \n    \n      2081\n      m_gify_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- Gifted/Talented Students\n    \n    \n      2082\n      m_gify_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- Gifted/Talented Students\n    \n    \n      2083\n      m_gifn_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Gifted/Talented Students\n    \n    \n      2084\n      m_gifn_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- Not Gifted/Talented Students\n    \n    \n      2085\n      m_gifv_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Gifted/Talented Students\n    \n    \n      2086\n      m_gifv_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info Gifted/Talented Students\n    \n    \n      2087\n      m_atry_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n    \n    \n      2088\n      m_atry_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n    \n    \n      2089\n      m_atrn_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n    \n    \n      2090\n      m_atrn_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n    \n    \n      2091\n      m_atrv_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n    \n    \n      2092\n      m_atrv_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n    \n  \n\n\n\n\nIn these latter cases, there is a 4th variable reporting category that comes before the subject and student group.\n\ndf[195:215]\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n    \n  \n  \n    \n      195\n      r_atrv_unsatgl_nm\n      Numeric\n      # Did Not Meet Grade Level Performance -- Reading -- No Info At-Risk Students\n    \n    \n      196\n      r_all_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- All Students\n    \n    \n      197\n      r_sexm_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Male Students\n    \n    \n      198\n      r_sexf_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Female Students\n    \n    \n      199\n      r_sexv_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- No Sex Info Students\n    \n    \n      200\n      r_ethh_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Hispanic/Latino Students\n    \n    \n      201\n      r_ethi_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- American Indian or Alaska Native Students\n    \n    \n      202\n      r_etha_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Asian Students\n    \n    \n      203\n      r_ethb_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Black or African American Students\n    \n    \n      204\n      r_ethp_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Native Hawaiian or Other Pacific Islander Students\n    \n    \n      205\n      r_ethw_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- White Students\n    \n    \n      206\n      r_eth2_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Two or More Races Students\n    \n    \n      207\n      r_ethv_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- No Ethnicity Info Students\n    \n    \n      208\n      r_ecoy_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Econ Disadv Students Codes: 1 2 9\n    \n    \n      209\n      r_econ_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Not Econ Disadv Students\n    \n    \n      210\n      r_eco1_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Free Meals Students\n    \n    \n      211\n      r_eco2_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Reduced-Price Meals Students\n    \n    \n      212\n      r_eco9_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Other Econ Disadvantaged Students\n    \n    \n      213\n      r_ecov_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- No Info Econ Students\n    \n    \n      214\n      r_ti1y_approgl_nm\n      Numeric\n      # Approaches Grade Level Performance -- Reading -- Title-I Participant Students Codes: 6 7 9\n    \n  \n\n\n\n\nThere is another pattern with 4 parts for the proficiency levels that is subject, student group, and then a two part string for the variable type.\nThese differences would make it challenging to simply split on the _ character. Even distinguishing between the three part and four part variables isn’t sufficient, because there are two types of the four part variables.\n\n\n\nHowever, we can simply ignore the variables. We have a description column that also has it’s own delimiter: --.\n\ndescriptions = df.Description.values.tolist()\n\nfor description in descriptions[:20]:\n    parts = description.split('--')\n    print(parts)\n\nprint('\\n...\\n')\n\nfor description in descriptions[-5:]:\n    parts = description.split('--')\n    print(parts)\n\n['Campus Number']\n['Test Administration Year']\n['Education Service Center (Region) Number']\n['District Number']\n['District Name']\n['Campus Name']\n['Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)']\n['# Answer Documents Submitted ', ' Reading ', ' All Students']\n['# Absent - Not Tested ', ' Reading ', ' All Students']\n['# Other - Not Tested ', ' Reading ', ' All Students']\n['# Answer Documents Submitted ', ' Mathematics ', ' All Students']\n['# Absent - Not Tested ', ' Mathematics ', ' All Students']\n['# Other - Not Tested ', ' Mathematics ', ' All Students']\n['# Tested ', ' Reading ', ' All Students']\n['# Tested ', ' Reading ', ' Male Students']\n['# Tested ', ' Reading ', ' Female Students']\n['# Tested ', ' Reading ', ' No Sex Info Students']\n['# Tested ', ' Reading ', ' Hispanic/Latino Students']\n['# Tested ', ' Reading ', ' American Indian or Alaska Native Students']\n['# Tested ', ' Reading ', ' Asian Students']\n\n...\n\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' At-Risk Students']\n['# Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' Not At-Risk Students']\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' Not At-Risk Students']\n['# Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' No Info At-Risk Students']\n['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' No Info At-Risk Students']\n\n\nThis mostly works. We have three different types of descriptions/variables: - 1 part variables like ['Campus Number'] - 3 part variables like ['# Tested ', ' Reading ', ' All Students'] - 4 part variables like ['% Avg Items Correct', 'Reporting Category 4 ', ' Mathematics ', ' At-Risk Students']\nThat means we can use the list length to determine the order of the variable values: variable name, test subject, and student group. The fourth part reporting category is basically a modifier for the variable name so we can combine those.\nBy examining a single variable name, we can see that # Tested, for example, isn’t actually 122 different variables but rather a single variable with 122 permutations of test subject and student group: 61 student groups * 2 test subjects\n\ndf[df.Description.str.contains('# Tested')]\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n    \n  \n  \n    \n      13\n      r_all_d\n      Numeric\n      # Tested -- Reading -- All Students\n    \n    \n      14\n      r_sexm_d\n      Numeric\n      # Tested -- Reading -- Male Students\n    \n    \n      15\n      r_sexf_d\n      Numeric\n      # Tested -- Reading -- Female Students\n    \n    \n      16\n      r_sexv_d\n      Numeric\n      # Tested -- Reading -- No Sex Info Students\n    \n    \n      17\n      r_ethh_d\n      Numeric\n      # Tested -- Reading -- Hispanic/Latino Students\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      130\n      m_gifn_d\n      Numeric\n      # Tested -- Mathematics -- Not Gifted/Talented Students\n    \n    \n      131\n      m_gifv_d\n      Numeric\n      # Tested -- Mathematics -- No Info Gifted/Talented Students\n    \n    \n      132\n      m_atry_d\n      Numeric\n      # Tested -- Mathematics -- At-Risk Students\n    \n    \n      133\n      m_atrn_d\n      Numeric\n      # Tested -- Mathematics -- Not At-Risk Students\n    \n    \n      134\n      m_atrv_d\n      Numeric\n      # Tested -- Mathematics -- No Info At-Risk Students\n    \n  \n\n122 rows × 3 columns\n\n\n\n\n\n\nIn addition to pulling out the test subject and student group from the variable description, it will be useful to rename the columns into a common naming convention that is more human readable than the current encodings.\nExample:\nIf we take the initial variable example r_all_d for # Tested -- Reading -- All Students and remove the subject and student group we would be left with simply d as the variable/column name. But a better description would be # Tested or even better (to eliminate special characters that would be problematic in the database) n_tested.\n\n\nTo start let’s add a new column to the dataframe that has the array of descriptors. We can also remove any leading or trailing whitespace from both the variable and description to prevent mismatch issues later.\n\ndf[\"Variable\"] = df[\"Variable\"].str.strip()\ndf[\"Description\"] = df[\"Description\"].str.strip()\ndf[\"desc_list\"] = df[\"Description\"].apply(lambda x: x.split(\"--\"))\ndf\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n      desc_list\n    \n  \n  \n    \n      0\n      campus\n      Character\n      Campus Number\n      [Campus Number]\n    \n    \n      1\n      year\n      Character\n      Test Administration Year\n      [Test Administration Year]\n    \n    \n      2\n      region\n      Character\n      Education Service Center (Region) Number\n      [Education Service Center (Region) Number]\n    \n    \n      3\n      district\n      Character\n      District Number\n      [District Number]\n    \n    \n      4\n      dname\n      Character\n      District Name\n      [District Name]\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2088\n      m_atry_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- At-Risk Students\n      [% Avg Items Correct, Reporting Category 4 ,  Mathematics ,  At-Risk Students]\n    \n    \n      2089\n      m_atrn_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n      [# Avg Items Correct, Reporting Category 4 ,  Mathematics ,  Not At-Risk Students]\n    \n    \n      2090\n      m_atrn_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- Not At-Risk Students\n      [% Avg Items Correct, Reporting Category 4 ,  Mathematics ,  Not At-Risk Students]\n    \n    \n      2091\n      m_atrv_avg_cat4\n      Numeric\n      # Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n      [# Avg Items Correct, Reporting Category 4 ,  Mathematics ,  No Info At-Risk Students]\n    \n    \n      2092\n      m_atrv_pct_cat4\n      Numeric\n      % Avg Items Correct--Reporting Category 4 -- Mathematics -- No Info At-Risk Students\n      [% Avg Items Correct, Reporting Category 4 ,  Mathematics ,  No Info At-Risk Students]\n    \n  \n\n2093 rows × 4 columns\n\n\n\n\n\n\nIn most cases the test subject is the 2nd part of the variable description with two exceptions: if the variable is one of the campus/grade variables that only has a single part or for the four part variables that include reporting category.\n\ndef get_subject(desc_list):\n    \"\"\"\n    Return the 2nd part of 3 part descriptors, the 3rd part of 4 part descriptors, or None.\n\n    Note: Else None is not necessary here, since functions return None by default in Python\n    \"\"\"\n    length = len(desc_list)\n    if length == 4:\n        return desc_list[2]\n    elif length > 1:\n        return desc_list[1]\n\n\ndf[\"subject\"] = df[\"desc_list\"].apply(lambda x: get_subject(x))\ndf.head(20)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n      desc_list\n      subject\n    \n  \n  \n    \n      0\n      campus\n      Character\n      Campus Number\n      [Campus Number]\n      None\n    \n    \n      1\n      year\n      Character\n      Test Administration Year\n      [Test Administration Year]\n      None\n    \n    \n      2\n      region\n      Character\n      Education Service Center (Region) Number\n      [Education Service Center (Region) Number]\n      None\n    \n    \n      3\n      district\n      Character\n      District Number\n      [District Number]\n      None\n    \n    \n      4\n      dname\n      Character\n      District Name\n      [District Name]\n      None\n    \n    \n      5\n      cname\n      Character\n      Campus Name\n      [Campus Name]\n      None\n    \n    \n      6\n      grade\n      Character\n      Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n      [Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\n      None\n    \n    \n      7\n      r_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Reading -- All Students\n      [# Answer Documents Submitted ,  Reading ,  All Students]\n      Reading\n    \n    \n      8\n      r_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Reading -- All Students\n      [# Absent - Not Tested ,  Reading ,  All Students]\n      Reading\n    \n    \n      9\n      r_all_oth_n\n      Numeric\n      # Other - Not Tested -- Reading -- All Students\n      [# Other - Not Tested ,  Reading ,  All Students]\n      Reading\n    \n    \n      10\n      m_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Mathematics -- All Students\n      [# Answer Documents Submitted ,  Mathematics ,  All Students]\n      Mathematics\n    \n    \n      11\n      m_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Mathematics -- All Students\n      [# Absent - Not Tested ,  Mathematics ,  All Students]\n      Mathematics\n    \n    \n      12\n      m_all_oth_n\n      Numeric\n      # Other - Not Tested -- Mathematics -- All Students\n      [# Other - Not Tested ,  Mathematics ,  All Students]\n      Mathematics\n    \n    \n      13\n      r_all_d\n      Numeric\n      # Tested -- Reading -- All Students\n      [# Tested ,  Reading ,  All Students]\n      Reading\n    \n    \n      14\n      r_sexm_d\n      Numeric\n      # Tested -- Reading -- Male Students\n      [# Tested ,  Reading ,  Male Students]\n      Reading\n    \n    \n      15\n      r_sexf_d\n      Numeric\n      # Tested -- Reading -- Female Students\n      [# Tested ,  Reading ,  Female Students]\n      Reading\n    \n    \n      16\n      r_sexv_d\n      Numeric\n      # Tested -- Reading -- No Sex Info Students\n      [# Tested ,  Reading ,  No Sex Info Students]\n      Reading\n    \n    \n      17\n      r_ethh_d\n      Numeric\n      # Tested -- Reading -- Hispanic/Latino Students\n      [# Tested ,  Reading ,  Hispanic/Latino Students]\n      Reading\n    \n    \n      18\n      r_ethi_d\n      Numeric\n      # Tested -- Reading -- American Indian or Alaska Native Students\n      [# Tested ,  Reading ,  American Indian or Alaska Native Students]\n      Reading\n    \n    \n      19\n      r_etha_d\n      Numeric\n      # Tested -- Reading -- Asian Students\n      [# Tested ,  Reading ,  Asian Students]\n      Reading\n    \n  \n\n\n\n\n\n\n\n\nIn all cases (except the single descriptor variables) the student group is included at the end of the description.\n\ndef get_student_group(desc_list):\n    \"\"\"\n    Unless the description has only one part, return the final part.\n    \"\"\"\n    if len(desc_list) > 1:\n        return desc_list[-1]\n\n\ndf[\"student_group\"] = df[\"desc_list\"].apply(lambda x: get_student_group(x))\ndf.head(20)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n      desc_list\n      subject\n      student_group\n    \n  \n  \n    \n      0\n      campus\n      Character\n      Campus Number\n      [Campus Number]\n      None\n      None\n    \n    \n      1\n      year\n      Character\n      Test Administration Year\n      [Test Administration Year]\n      None\n      None\n    \n    \n      2\n      region\n      Character\n      Education Service Center (Region) Number\n      [Education Service Center (Region) Number]\n      None\n      None\n    \n    \n      3\n      district\n      Character\n      District Number\n      [District Number]\n      None\n      None\n    \n    \n      4\n      dname\n      Character\n      District Name\n      [District Name]\n      None\n      None\n    \n    \n      5\n      cname\n      Character\n      Campus Name\n      [Campus Name]\n      None\n      None\n    \n    \n      6\n      grade\n      Character\n      Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n      [Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\n      None\n      None\n    \n    \n      7\n      r_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Reading -- All Students\n      [# Answer Documents Submitted ,  Reading ,  All Students]\n      Reading\n      All Students\n    \n    \n      8\n      r_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Reading -- All Students\n      [# Absent - Not Tested ,  Reading ,  All Students]\n      Reading\n      All Students\n    \n    \n      9\n      r_all_oth_n\n      Numeric\n      # Other - Not Tested -- Reading -- All Students\n      [# Other - Not Tested ,  Reading ,  All Students]\n      Reading\n      All Students\n    \n    \n      10\n      m_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Mathematics -- All Students\n      [# Answer Documents Submitted ,  Mathematics ,  All Students]\n      Mathematics\n      All Students\n    \n    \n      11\n      m_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Mathematics -- All Students\n      [# Absent - Not Tested ,  Mathematics ,  All Students]\n      Mathematics\n      All Students\n    \n    \n      12\n      m_all_oth_n\n      Numeric\n      # Other - Not Tested -- Mathematics -- All Students\n      [# Other - Not Tested ,  Mathematics ,  All Students]\n      Mathematics\n      All Students\n    \n    \n      13\n      r_all_d\n      Numeric\n      # Tested -- Reading -- All Students\n      [# Tested ,  Reading ,  All Students]\n      Reading\n      All Students\n    \n    \n      14\n      r_sexm_d\n      Numeric\n      # Tested -- Reading -- Male Students\n      [# Tested ,  Reading ,  Male Students]\n      Reading\n      Male Students\n    \n    \n      15\n      r_sexf_d\n      Numeric\n      # Tested -- Reading -- Female Students\n      [# Tested ,  Reading ,  Female Students]\n      Reading\n      Female Students\n    \n    \n      16\n      r_sexv_d\n      Numeric\n      # Tested -- Reading -- No Sex Info Students\n      [# Tested ,  Reading ,  No Sex Info Students]\n      Reading\n      No Sex Info Students\n    \n    \n      17\n      r_ethh_d\n      Numeric\n      # Tested -- Reading -- Hispanic/Latino Students\n      [# Tested ,  Reading ,  Hispanic/Latino Students]\n      Reading\n      Hispanic/Latino Students\n    \n    \n      18\n      r_ethi_d\n      Numeric\n      # Tested -- Reading -- American Indian or Alaska Native Students\n      [# Tested ,  Reading ,  American Indian or Alaska Native Students]\n      Reading\n      American Indian or Alaska Native Students\n    \n    \n      19\n      r_etha_d\n      Numeric\n      # Tested -- Reading -- Asian Students\n      [# Tested ,  Reading ,  Asian Students]\n      Reading\n      Asian Students\n    \n  \n\n\n\n\n\n\n\nThere are a few problematic characters like # and % in the names of the variables that should be replaced as well as replacing spaces and abbreviating some longer words in the names. This also uses some regex parsing to remove any text contained in parenthesis.\n\nimport re\n\ndef rename_variable(desc_list):\n    \"\"\"\n    Replace special characters and append reporting category when present.\n    \"\"\"\n    variable_name = desc_list[0].lower().strip()\n    if len(desc_list) == 4:\n        variable_name += desc_list[1].lower().strip()\n    replacements = {\n        '-': '',\n        '#': 'n',\n        '%': 'pct',\n        'average': 'avg',\n        'reporting category': 'rpt cat',\n        ' ': '_',\n        '__': '_',\n    }\n    for old, new in replacements.items():\n        variable_name = re.sub(r\"\\([^()]*\\)\", \"\", variable_name).strip()\n        variable_name = variable_name.replace(old, new)\n    return variable_name\n\n\ndf[\"name\"] = df[\"desc_list\"].apply(lambda x: rename_variable(x))\ndf.head(20)\n\n\n\n\n\n  \n    \n      \n      Variable\n      Format_Type\n      Description\n      desc_list\n      subject\n      student_group\n      name\n    \n  \n  \n    \n      0\n      campus\n      Character\n      Campus Number\n      [Campus Number]\n      None\n      None\n      campus_number\n    \n    \n      1\n      year\n      Character\n      Test Administration Year\n      [Test Administration Year]\n      None\n      None\n      test_administration_year\n    \n    \n      2\n      region\n      Character\n      Education Service Center (Region) Number\n      [Education Service Center (Region) Number]\n      None\n      None\n      education_service_center_number\n    \n    \n      3\n      district\n      Character\n      District Number\n      [District Number]\n      None\n      None\n      district_number\n    \n    \n      4\n      dname\n      Character\n      District Name\n      [District Name]\n      None\n      None\n      district_name\n    \n    \n      5\n      cname\n      Character\n      Campus Name\n      [Campus Name]\n      None\n      None\n      campus_name\n    \n    \n      6\n      grade\n      Character\n      Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)\n      [Tested Grade (Usually the Enrolled Grade Unless Student Tested Above Grade)]\n      None\n      None\n      tested_grade\n    \n    \n      7\n      r_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Reading -- All Students\n      [# Answer Documents Submitted ,  Reading ,  All Students]\n      Reading\n      All Students\n      n_answer_documents_submitted\n    \n    \n      8\n      r_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Reading -- All Students\n      [# Absent - Not Tested ,  Reading ,  All Students]\n      Reading\n      All Students\n      n_absent_not_tested\n    \n    \n      9\n      r_all_oth_n\n      Numeric\n      # Other - Not Tested -- Reading -- All Students\n      [# Other - Not Tested ,  Reading ,  All Students]\n      Reading\n      All Students\n      n_other_not_tested\n    \n    \n      10\n      m_all_docs_n\n      Numeric\n      # Answer Documents Submitted -- Mathematics -- All Students\n      [# Answer Documents Submitted ,  Mathematics ,  All Students]\n      Mathematics\n      All Students\n      n_answer_documents_submitted\n    \n    \n      11\n      m_all_abs_n\n      Numeric\n      # Absent - Not Tested -- Mathematics -- All Students\n      [# Absent - Not Tested ,  Mathematics ,  All Students]\n      Mathematics\n      All Students\n      n_absent_not_tested\n    \n    \n      12\n      m_all_oth_n\n      Numeric\n      # Other - Not Tested -- Mathematics -- All Students\n      [# Other - Not Tested ,  Mathematics ,  All Students]\n      Mathematics\n      All Students\n      n_other_not_tested\n    \n    \n      13\n      r_all_d\n      Numeric\n      # Tested -- Reading -- All Students\n      [# Tested ,  Reading ,  All Students]\n      Reading\n      All Students\n      n_tested\n    \n    \n      14\n      r_sexm_d\n      Numeric\n      # Tested -- Reading -- Male Students\n      [# Tested ,  Reading ,  Male Students]\n      Reading\n      Male Students\n      n_tested\n    \n    \n      15\n      r_sexf_d\n      Numeric\n      # Tested -- Reading -- Female Students\n      [# Tested ,  Reading ,  Female Students]\n      Reading\n      Female Students\n      n_tested\n    \n    \n      16\n      r_sexv_d\n      Numeric\n      # Tested -- Reading -- No Sex Info Students\n      [# Tested ,  Reading ,  No Sex Info Students]\n      Reading\n      No Sex Info Students\n      n_tested\n    \n    \n      17\n      r_ethh_d\n      Numeric\n      # Tested -- Reading -- Hispanic/Latino Students\n      [# Tested ,  Reading ,  Hispanic/Latino Students]\n      Reading\n      Hispanic/Latino Students\n      n_tested\n    \n    \n      18\n      r_ethi_d\n      Numeric\n      # Tested -- Reading -- American Indian or Alaska Native Students\n      [# Tested ,  Reading ,  American Indian or Alaska Native Students]\n      Reading\n      American Indian or Alaska Native Students\n      n_tested\n    \n    \n      19\n      r_etha_d\n      Numeric\n      # Tested -- Reading -- Asian Students\n      [# Tested ,  Reading ,  Asian Students]\n      Reading\n      Asian Students\n      n_tested\n    \n  \n\n\n\n\n\n\n\nBecause we’re going to iterate over each variable list (there are six files: one for each grade between 3 and 8), we can simplify this by running all these change operations at once. We also want to remove unneeded columns and rename the original columns to standardize the naming conventions and make for an easy join condition later.\n\ndef add_descriptors(df):\n    df[\"Variable\"] = df[\"Variable\"].str.strip()\n    df[\"Description\"] = df[\"Description\"].str.strip()\n    df[\"desc_list\"] = df[\"Description\"].apply(lambda x: x.split('--'))\n    df[\"name\"] = df[\"desc_list\"].apply(lambda x: rename_variable(x))\n    df[\"subject\"] = df[\"desc_list\"].apply(lambda x: get_subject(x))\n    df[\"student_group\"] = df[\"desc_list\"].apply(lambda x: get_student_group(x))\n    df.drop(columns=[\"Description\", \"desc_list\"], inplace=True)\n    df.rename(columns={'Variable': 'variable', 'Format_Type': 'format_type'}, inplace=True)\n    return df\n\n\n\n\nThat means it would be possible to pivot this data long for each distinct variable after extracting the subject and student group from the variable description. This information, though, is split across two different files: the test data in the .dat files and the variable descriptions in the .xls files.\nMy plan then is to: 1. Match the corresponding dat file to its variable list 2. Pivot the test data long so that each variable and value is a single row 3. Extract the test subject and student group data points from the variable description 4. Join the two datasets together using the variable name as a unique identifier 5. Once more pivot the combined data such that each distinct variable becomes its own column 6. Combine those standardized data structures for each grade into a single dataset 7. Write that new dataset out to a compressed csv file for importing into BigQuery\n\nbase_url = \"https://tea.texas.gov/sites/default/files/\"\ndat_files = [\"cfy21e3.dat\", \"cfy21e4.dat\", \"cfy21e5.dat\", \"cfy21e6.dat\", \"cfy21e7.dat\", \"cfy21e8.dat\"]\nvar_files = [\"fy21_varlist_g03.xls\", \"fy21_varlist_g04.xls\", \"fy21_varlist_g05.xls\", \"fy21_varlist_g06.xls\", \"fy21_varlist_g07.xls\", \"fy21_varlist_g08.xls\"]\n\n# 1. Match the dat file with its corresponding var file\nall_files = zip(dat_files, var_files)\n\n# Create a list to concatenate the resulting grade level data frames later\nall_grades = []\n\nfor dat_file, var_file in all_files:\n    test_data = pd.read_csv(f\"{base_url}{dat_file}\", dtype=\"string\")\n    variables = pd.read_excel(f\"{base_url}{var_file}\", dtype=\"string\")\n\n    print(\"Parsing:\", dat_file, \"rows/cols:\", test_data.shape)\n\n\n    # 2. melt the dataframe (pivot wide to long) but keep all the site and grade identifiers out of the pivot\n    identifiers = variables[variables.Format_Type == 'Character'].Variable.tolist()\n    id_vars = [identifier.upper() for identifier in identifiers]\n    test_data = pd.melt(test_data, id_vars=id_vars)\n    print(\"Pivoted:\", dat_file, \"rows/cols:\", test_data.shape)\n\n    # 3. extract the subject and student groups\n    variables = add_descriptors(variables)\n\n    # 4. join the two dataset together using the variable name as the unique identifier\n    combined = test_data.merge(variables, how='left', on='variable')\n    print(\"Joined:\", dat_file, \"+\", var_file, \"rows/cols:\", combined.shape)\n\n    # 5. pivot back wide but with only the distinct variables\n    cols = combined.columns.tolist()\n    combined[cols] = combined[cols].apply(lambda x: x.str.strip())\n    combined.columns = combined.columns.str.lower()\n    columns = ['campus','year','region','district','dname','cname','grade','format_type','subject','student_group','name']\n    combined = combined.set_index(columns)['value'].unstack().reset_index()\n    print(\"Combined:\", dat_file, \"+\", var_file, \"rows/cols:\", combined.shape)\n\n    # 6. append data for each grade into a single data structure and concatenate\n    all_grades.append(combined)\n\ndf = pd.concat(all_grades)\nprint(\"Final rows/cols:\", df.shape)\n\n# 7. write to compressed csv\ndf.to_csv(\"TX_STAAR_3_8_2021.csv.gz\", index=False, compression=\"gzip\")\n\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e3.dat rows/cols: (4643, 2093)\nPivoted: cfy21e3.dat rows/cols: (9685298, 9)\nJoined: cfy21e3.dat + fy21_varlist_g03.xls rows/cols: (9685298, 13)\nCombined: cfy21e3.dat + fy21_varlist_g03.xls rows/cols: (571089, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e4.dat rows/cols: (4622, 3075)\nPivoted: cfy21e4.dat rows/cols: (14180296, 9)\nJoined: cfy21e4.dat + fy21_varlist_g04.xls rows/cols: (14180296, 13)\nCombined: cfy21e4.dat + fy21_varlist_g04.xls rows/cols: (850448, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e5.dat rows/cols: (4415, 3197)\nPivoted: cfy21e5.dat rows/cols: (14083850, 9)\nJoined: cfy21e5.dat + fy21_varlist_g05.xls rows/cols: (14083850, 13)\nCombined: cfy21e5.dat + fy21_varlist_g05.xls rows/cols: (812360, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e6.dat rows/cols: (2700, 2093)\nPivoted: cfy21e6.dat rows/cols: (5632200, 9)\nJoined: cfy21e6.dat + fy21_varlist_g06.xls rows/cols: (5632200, 13)\nCombined: cfy21e6.dat + fy21_varlist_g06.xls rows/cols: (332100, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e7.dat rows/cols: (2337, 3075)\nPivoted: cfy21e7.dat rows/cols: (7169916, 9)\nJoined: cfy21e7.dat + fy21_varlist_g07.xls rows/cols: (7169916, 13)\nCombined: cfy21e7.dat + fy21_varlist_g07.xls rows/cols: (430008, 35)\nWARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\nParsing: cfy21e8.dat rows/cols: (2370, 4301)\nPivoted: cfy21e8.dat rows/cols: (10176780, 9)\nJoined: cfy21e8.dat + fy21_varlist_g08.xls rows/cols: (10176780, 13)\nCombined: cfy21e8.dat + fy21_varlist_g08.xls rows/cols: (580650, 35)\nFinal rows/cols: (3576655, 35)\n\n\n\ndf\n\n\n\n\n\n  \n    \n      name\n      campus\n      year\n      region\n      district\n      dname\n      cname\n      grade\n      format_type\n      subject\n      student_group\n      NaN\n      avg_scale_score\n      n_absent_not_tested\n      n_answer_documents_submitted\n      n_approaches_grade_level_performance\n      n_avg_items_correctrpt_cat_1\n      n_avg_items_correctrpt_cat_2\n      n_avg_items_correctrpt_cat_3\n      n_avg_items_correctrpt_cat_4\n      n_did_not_meet_grade_level_performance\n      n_masters_grade_level_performance\n      n_meets_grade_level_performance\n      n_other_not_tested\n      n_tested\n      pct_absent_not_tested\n      pct_answer_documents_submitted\n      pct_approaches_grade_level_performance\n      pct_avg_items_correctrpt_cat_1\n      pct_avg_items_correctrpt_cat_2\n      pct_avg_items_correctrpt_cat_3\n      pct_avg_items_correctrpt_cat_4\n      pct_did_not_meet_grade_level_performance\n      pct_masters_grade_level_performance\n      pct_meets_grade_level_performance\n      pct_other_not_tested\n    \n  \n  \n    \n      0\n      001902103\n      21\n      07\n      001902\n      CAYUGA ISD\n      CAYUGA ELEM.\n      03\n      <NA>\n      NaN\n      NaN\n      \n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      001902103\n      21\n      07\n      001902\n      CAYUGA ISD\n      CAYUGA ELEM.\n      03\n      Numeric\n      Mathematics\n      All Students\n      NaN\n      1607\n      0\n      26\n      22\n      6.3\n      10.7\n      5\n      3.3\n      4\n      16\n      19\n      0\n      26\n      0\n      100\n      85\n      79\n      83\n      71\n      84\n      15\n      62\n      73\n      0\n    \n    \n      2\n      001902103\n      21\n      07\n      001902\n      CAYUGA ISD\n      CAYUGA ELEM.\n      03\n      Numeric\n      Mathematics\n      American Indian or Alaska Native Students\n      NaN\n      <NA>\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n      0\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n    \n    \n      3\n      001902103\n      21\n      07\n      001902\n      CAYUGA ISD\n      CAYUGA ELEM.\n      03\n      Numeric\n      Mathematics\n      Asian Students\n      NaN\n      <NA>\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n      0\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n    \n    \n      4\n      001902103\n      21\n      07\n      001902\n      CAYUGA ISD\n      CAYUGA ELEM.\n      03\n      Numeric\n      Mathematics\n      At-Risk Students\n      NaN\n      1465\n      NaN\n      NaN\n      5\n      5.1\n      9.3\n      3.3\n      2.7\n      2\n      2\n      3\n      NaN\n      7\n      NaN\n      NaN\n      71\n      64\n      71\n      47\n      68\n      29\n      29\n      43\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      580645\n      254902001\n      21\n      20\n      254902\n      LA PRYOR ISD\n      LA PRYOR H.S.\n      08\n      Numeric\n      Social Studies\n      Title-I Participant Students Codes: 6 7 9\n      NaN\n      3425\n      NaN\n      NaN\n      12\n      6.9\n      5.4\n      4.2\n      3.2\n      23\n      0\n      4\n      NaN\n      35\n      NaN\n      NaN\n      34\n      41\n      54\n      42\n      46\n      66\n      0\n      11\n      NaN\n    \n    \n      580646\n      254902001\n      21\n      20\n      254902\n      LA PRYOR ISD\n      LA PRYOR H.S.\n      08\n      Numeric\n      Social Studies\n      Transitional Bilingual/Early Exit Students\n      NaN\n      <NA>\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n      0\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n    \n    \n      580647\n      254902001\n      21\n      20\n      254902\n      LA PRYOR ISD\n      LA PRYOR H.S.\n      08\n      Numeric\n      Social Studies\n      Transitional Bilingual/Late Exit Students\n      NaN\n      <NA>\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n      0\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n    \n    \n      580648\n      254902001\n      21\n      20\n      254902\n      LA PRYOR ISD\n      LA PRYOR H.S.\n      08\n      Numeric\n      Social Studies\n      Two or More Races Students\n      NaN\n      <NA>\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n      0\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n    \n    \n      580649\n      254902001\n      21\n      20\n      254902\n      LA PRYOR ISD\n      LA PRYOR H.S.\n      08\n      Numeric\n      Social Studies\n      White Students\n      NaN\n      <NA>\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n      1\n      NaN\n      NaN\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      <NA>\n      NaN\n    \n  \n\n3576655 rows × 35 columns\n\n\n\n\n\n\nThe resulting dataset/file ends up being ~3.5 million rows in length and a consistent 35 columns wide. You can download the reshaped dataset here: TX STAAR 2021 - Grades 3-8\nWe can now run simpler queries on the reshaped data such as:\nSELECT *\nFROM TX_STAAR_2021\nWHERE subject = 'Reading'\nAND student_group = 'All Students'\nAND grade IN ('6', '7', '8')\nA similar query using the original datasets would’ve required unioning three different datasets and a much more complicated set of column matching to filter down to the right subject and student group.\nIn our production environment, the approach is similar but we initially load the raw dat and var files into Google Cloud Storage and as well as the resulting csv. That is then converted to avro for better query performance and an external table is generated in BigQuery using their API."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I got started, as a young teen, programming in BASIC on a used Commodore64 in the early 90s. I quickly found my way to HTML4 and built some of my first websites on GeoCities.\n\nIn college I tried my hand at Computer Science, but was more and more attracted to the Social Sciences and eventually ended up teaching high school Social Studies.\nI soon needed better tools to support my students and started building out some simple web apps in PHP on the backend of a Wordpress blog to help me teach in the days before blended learning and EdTech were ubiquitous in the education sector.\nAfter a few years of building my skills, I eventually found myself working for Aspire Public Schools on the Godzilla team alongside the folks who went on to start Schoolzilla. I’ve been hacking away ever since building data integrations and web applications to support our students, teachers, and back-office staff.\nI’m a polyglot programmer with a deep understanding of data integrations, data warehousing, and back-end APIs. I’m a decent full stack developer when it comes to enterprise web apps and work primarily in python these days.\n\n\n\nFront End: HTML5/CSS3/ES6, SASS, JQuery, Bootstrap, Material Design\nBack End: Python, Django, Flask, Ruby on Rails, NodeJS, C#, MS SQL, PostgreSQL, BigQuery, dbt\nScripting: Bash, PowerShell, Python, Ruby\nDevOps: VirtualBox, Heroku, AWS, S3, Docker, DigitalOcean, Google Cloud Platform\n\n\n\n\nI am an avid homebrewer and will wax philosophical on the merits of late boil hop additions if given the chance. But I’m also happy to enjoy some lawn-mower beer while sitting on the patio.\nI love making music and recently started publishing some of my work on Spotify under the pseudonym Mister Yeti.\nI’m also a husband and father and love nothing better than sitting around playing with my son. He’s still young, but I’m really looking forward to sharing the joy of programming with him when he’s a little older."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dchess.org",
    "section": "",
    "text": "Goodbye Pelican, Hello Quarto\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nI Can Python, So Can You\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nParsing Texas Assessment Data\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nJan 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nCreating WordClouds\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\nAuto-generated REST API for MS SQL database\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\nVirtual K-12 Tableau User Group - 14 January 2021\n\n\n\n\n\n\n\nprojects\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHow I Teach Git\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nNormalize JSON with Pandas\n\n\n\n\n\n\n\ntutorial\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\nInnovateEDU Webinar - 19 November 2020\n\n\n\n\n\n\n\nprojects\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMy Manager README\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2018\n\n\n\n\n\n\n  \n\n\n\n\nAutomating Developer Environment Setup\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2018\n\n\n\n\n\n\n  \n\n\n\n\nHow Memes Inform My Work\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2018\n\n\n\n\n\n\n  \n\n\n\n\nResolutions for the New Year\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2018\n\n\n\n\n\n\n  \n\n\n\n\nStudent Email Sync\n\n\n\n\n\n\n\nprojects\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2016\n\n\n\n\n\n\nNo matching items"
  }
]